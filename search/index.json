[{"content":"A couple of days ago we had to migrate a large data set from a MongoDB to a PostgreSQL database. The customer did not want to maintain a MongoDB cluster and wanted (yesterday as usual) to move all its data to a new PostgreSQL database.\nSince we did not have the available time to build a new relational schema, we had to use the new PostgreSQL JSON capabilities in order to store the existing MongoDB database documents (entries) into a relational table.\nTherefore, let us say that our data in MongoDB vehicle collection look like the following :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 [ { \u0026#34;name\u0026#34;: \u0026#34;Toyota Yaris\u0026#34;, \u0026#34;creation\u0026#34;: 2007, \u0026#34;owners\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Christos\u0026#34;, \u0026#34;surname\u0026#34;: \u0026#34;Manios\u0026#34;, \u0026#34;percentage\u0026#34;: 50 }, { \u0026#34;name\u0026#34;: \u0026#34;Telis\u0026#34;, \u0026#34;surname\u0026#34;: \u0026#34;Skarpelis\u0026#34;, \u0026#34;percentage\u0026#34;: 50 } ] }, { \u0026#34;name\u0026#34;: \u0026#34;Toyota RAV4\u0026#34;, \u0026#34;creation\u0026#34;: 2017, \u0026#34;owners\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Christos\u0026#34;, \u0026#34;surname\u0026#34;: \u0026#34;Manios\u0026#34;, \u0026#34;percentage\u0026#34;: 70 }, { \u0026#34;name\u0026#34;: \u0026#34;Soulis\u0026#34;, \u0026#34;surname\u0026#34;: \u0026#34;Delapongos\u0026#34;, \u0026#34;percentage\u0026#34;: 30 } ] }, { \u0026#34;name\u0026#34;: \u0026#34;Ferarri F40\u0026#34;, \u0026#34;creation\u0026#34;: 1999, \u0026#34;owners\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Bobos\u0026#34;, \u0026#34;surname\u0026#34;: \u0026#34;Peskardelis\u0026#34;, \u0026#34;percentage\u0026#34;: 100 } ] } ] We can quickly test it in PostgreSQL by creating the following table and inserting some data:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 -- create a test table CREATE TABLE vehicle ( id serial NOT NULL PRIMARY KEY, info jsonb NOT NULL ); -- add an index to assist jsonb queries CREATE INDEX vehicle_info_gin_idx ON vehicle USING gin (info); -- insert three vehicles insert into vehicle (info) values (\u0026#39; {\u0026#34;name\u0026#34;:\u0026#34;Toyota Yaris\u0026#34;,\u0026#34;creation\u0026#34;:2007,\u0026#34;owners\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;Christos\u0026#34;,\u0026#34;surname\u0026#34;:\u0026#34;Manios\u0026#34;,\u0026#34;percentage\u0026#34;:50},{\u0026#34;name\u0026#34;:\u0026#34;Telis\u0026#34;,\u0026#34;surname\u0026#34;:\u0026#34;Skarpelis\u0026#34;,\u0026#34;percentage\u0026#34;:50}]}\u0026#39;); insert into vehicle (info) values (\u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;Toyota RAV4\u0026#34;,\u0026#34;creation\u0026#34;:2017,\u0026#34;owners\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;Christos\u0026#34;,\u0026#34;surname\u0026#34;:\u0026#34;Manios\u0026#34;,\u0026#34;percentage\u0026#34;:70},{\u0026#34;name\u0026#34;:\u0026#34;Soulis\u0026#34;,\u0026#34;surname\u0026#34;:\u0026#34;Delapongos\u0026#34;,\u0026#34;percentage\u0026#34;:30}]}\u0026#39;); insert into vehicle (info) values (\u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;Ferarri F40\u0026#34;,\u0026#34;creation\u0026#34;:1999,\u0026#34;owners\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;Bobos\u0026#34;,\u0026#34;surname\u0026#34;:\u0026#34;Peskardelis\u0026#34;,\u0026#34;percentage\u0026#34;:100}]}\u0026#39;); Then let us construct the query which checks if there is a vehicle which belongs to Christos Manios and is the owner with percentage more than 40% .\n1 2 3 4 5 6 7 SELECT v.info -\u0026gt;\u0026gt; \u0026#39;name\u0026#39; AS vehicleType , obj -\u0026gt;\u0026gt; \u0026#39;name\u0026#39; as ownerName, obj -\u0026gt;\u0026gt; \u0026#39;surname\u0026#39; as ownerName, obj -\u0026gt;\u0026gt; \u0026#39;percentage\u0026#39; as ownerPercentage from vehicle v, jsonb_array_elements(v.info#\u0026gt;\u0026#39;{owners}\u0026#39;) obj where obj -\u0026gt;\u0026gt; \u0026#39;surname\u0026#39; = \u0026#39;Manios\u0026#39; and obj -\u0026gt;\u0026gt; \u0026#39;percentage\u0026#39; \u0026gt; \u0026#39;40\u0026#39; ; The expected result is the following:\n1 2 3 4 | vehicletype | ownername | ownername | ownerpercentage | |--------------|-----------|-----------|-----------------| | Toyota Yaris | Christos | Manios | 50 | | Toyota RAV4 | Christos | Manios | 70 | I hope that you got a little taste of how you can query JSON data in PostgreSQL with SQL!\nGreetings from a rainy Thessaloniki!\n","date":"2025-11-22T12:43:27+02:00","permalink":"https://manios.org/2025/11/22/postgre-sql-perform-queries-in-json-array-sub-documents/","title":"PostgreSQL perform queries in JSON array sub-documents"},{"content":"The Problem A few days ago, while we were watching a film in VLC player in my laptop which runs Linux Mint 20.1 MATE, the screen went into screensaver mode. This happened multiple times and we had to press a button to make the screen come back. We have searched the web and we have seen that others had the same issue as seen here:\nScreensaver activates when watching movies - Linux Mint Forums Firefox activates screensaver when watching videos - Linux Mint Forums After spending some time reading the experiences of other users in forums, we ended up disabling the screensaver and display sleep until our film was finished.\nHowever we wanted to be able to do that by command line in bash, in order to be able to quickly toggle the setting. When watching films disable the screensaver and when using the computer for work to enable it.\nOur preferred settings would be when working with the PC:\nScreensaver settings: Activate screensaver when computer is idle: true Regard the computer as idle after (minutes) Powermanagement settings: Put display to sleep when inactive for: 10 minutes After some searching in the internet, we found out a related article for Ubuntu: 20.04 - How to configure ScreenSaver by Command Line? - Ask Ubuntu\ngsettings command While playing with the gsettings command, we found the following:\nTo list all settings and write them to a file you can execute this command:\n1 2 # List all settings and write them to allsettings.txt file gsettings list-recursively \u0026gt; allsettings.txt To read a single setting you can use the gsettings get command:\n1 2 chris@laptop:~$ gsettings get org.mate.power-manager sleep-display-ac 600 To set a single setting you can use the gsettings set command:\n1 chris@laptop:~$ gsettings get org.mate.power-manager sleep-display-ac 300 After playing with the values of Mate desktop \u0026ldquo;Screensaver\u0026rdquo; settings window we found the following settings:\nScreensaver settings: Activate screensaver when computer is idle (true/false): org.mate.screensaver idle-activation-enabled \u0026lt;true/false\u0026gt; Regard the computer as idle after 5 minutes: org.mate.session idle-delay 5 Regard the computer as idle after Never (disabled): org.mate.session idle-delay 0 Power management settings: Put display to sleep when inactive for (seconds): org.mate.power-manager sleep-display-ac 600 Put display to sleep when inactive for Never (disabled): org.mate.power-manager sleep-display-ac 0 In configuration this is described as:\n1 2 3 4 5 6 7 8 9 # Activate screensaver when computer is idle org.mate.screensaver idle-activation-enabled true # Regard the computer as idle after (minutes) org.mate.session idle-delay 5 # Put display to sleep when inactive for: 10 minutes org.mate.power-manager sleep-display-ac 600 # Put display to sleep when inactive for: Never org.mate.power-manager sleep-display-ac 600 The script We quickly drafted a bash script which toggles (enables or dsables) the screensaver and screen sleep you:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 #!/bin/bash currentSetting=`gsettings get org.mate.screensaver idle-activation-enabled` if [ \u0026#34;$currentSetting\u0026#34; == \u0026#34;true\u0026#34; ]; then # Activate screensaver when computer is idle gsettings set org.mate.screensaver idle-activation-enabled false # Regard the computer as idle after 0 minutes (Never) gsettings set org.mate.session idle-delay 0 # Put display to sleep when inactive for: 0 seconds (Never) gsettings set org.mate.power-manager sleep-display-ac 0 echo \u0026#34;Screensaver deactivated\u0026#34; else # Activate screensaver when computer is idle gsettings set org.mate.screensaver idle-activation-enabled true # Regard the computer as idle after 5 minutes gsettings set org.mate.session idle-delay 5 # Put display to sleep when inactive for: 10 minutes gsettings set org.mate.power-manager sleep-display-ac 600 echo \u0026#34;Screensaver activated\u0026#34; fi How does it work? The script is pretty simple:\nIt reads the org.mate.screensaver idle-activation-enabled configuration to check if the screensaver is currently enabled. If the screensaver is enabled: it disables it. It also disables display sleep. If the screensaver is disabled: it enables it with idle time of 5 minutes. It also enables the display sleep after 10 minutes. It prints a message in stdout to inform us about the action it took. Example usage Put the script into a file, let us say myscreensaver.sh, make it runnable with chmod 744 myscreensaver.sh and test it:\n1 2 3 4 5 6 7 chris@laptop:~$ # Calling it for the first time chris@laptop:~$ ./myscreensaver.sh Screensaver activated chris@laptop:~$ chris@laptop:~$ # Calling it for the second time chris@laptop:~$ ./myscreensaver.sh Screensaver deactivated I hope this helps. Have a wonderful day!\n","date":"2024-02-11T14:17:00+02:00","permalink":"https://manios.org/2024/02/11/linux-mint-mate-screensaver-command-line-configure-enable-disable/","title":"Linux Mint Mate configure screensaver and display sleep via command line"},{"content":"A few days ago we stumbled upon a very strange error while migrating a legacy application from Spring Boot 2.7.0 with Spring Data JPA (Hibernate Core 5.6.9-Final) to Spring Boot 3.1.0 (Hibernate Core 6.2.2-Final)\nThe code was executing an Oracle database function, called pl_functions.persist_bonus_points which was persisting some loyalty customer points for a bank and returned a transaction ID. The function call used to work before the upgrade. After upgrading to Hibernate 6 it started failing with an error:\n1 Could not extract column [1] from JDBC ResultSet [ORA-17026: Numeric overflow] [n/a] with root cause After quite some investigation, we found out that when pl_functions.persist_bonus_points() function is called, it returns a value which looks like this: 4807214298310656894. Hibernate 6 is trying to convert it to number and it fails with an overflow. In our case we could not rewrite the code because we could not understand exactly what it was doing. After searching in the web, we found in StackOverflow question #36073450 that the least intrusive way to patch the existing code was by using .addScalar() method and convert the returned value to String.\nAfter the fix the code is the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public String getTransactionIdForUpdatingBonusPoints(Long branchId, String customerId, String points) { Query query = entityManager.createNativeQuery(\u0026#34;select pl_functions.persist_bonus_points (:branchId, :customerId, :points) as transactionId from dual\u0026#34;); query.setParameter(\u0026#34;branchId\u0026#34;, branchId); query.setParameter(\u0026#34;customerId\u0026#34;, customerId); query.setParameter(\u0026#34;points\u0026#34;, points); // FIX FOR HIBERNATE 6 query.unwrap(NativeQuery.class) .addScalar(\u0026#34;transactionId\u0026#34;, StandardBasicTypes.STRING); // THIS IS WHERE THE ERROR OCCURED var res = query.getResultList(); return getTransactionIdFromResult(res); } I hope that this helps in your Hibernate migration endeavours! Cheers from Thessaloniki, Greece!\n","date":"2023-08-30T22:43:27+03:00","permalink":"https://manios.org/2023/08/30/java-spring-hibernate-error-ora-17026-numeric-overflow-oracle-function-call/","title":"ORA-17026: Numeric overflow when calling Oracle SQL function using Hibernate 6"},{"content":"A few days ago we were debugging a Java Spring Boot web application (called engine-export-service) in Kubernetes and we wanted to tail the Pod logs. Since we have a lot of replicas for the engine-export-service Deployment of this application, we decided to use Stern, The Multi pod and container log tailing for Kubernetes!\nTherefore we set up stern, opened up a shell and run:\n1 stern --context ctx-jetengine-factory --namespace=jetengine-factory-prod engine-export* The output we got was looking like the following:\n1 2 3 4 5 6 7 8 9 10 11 + engine-export-service-7f6c797f5b-q7cm8 › engine-export engine-export-service-7f6c797f5b-q7cm8 engine-export {\u0026#34;@timestamp\u0026#34;:\u0026#34;2023-06-15T02:09:23.240Z\u0026#34;,\u0026#34;@version\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Started sending of 1000 messages\u0026#34;,\u0026#34;logger_name\u0026#34;:\u0026#34;org.manios.airplanefactory.engineexport.rabbit.RabbitService\u0026#34;,\u0026#34;thread_name\u0026#34;:\u0026#34;ForkJoinPool.commonPool-worker-3\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;level_value\u0026#34;:20000} engine-export-service-7f6c797f5b-q7cm8 engine-export {\u0026#34;@timestamp\u0026#34;:\u0026#34;2023-06-15T02:09:25.710Z\u0026#34;,\u0026#34;@version\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Finished sending of 1000 messages\u0026#34;,\u0026#34;logger_name\u0026#34;:\u0026#34;org.manios.airplanefactory.engineexport.rabbit.RabbitService\u0026#34;,\u0026#34;thread_name\u0026#34;:\u0026#34;ForkJoinPool.commonPool-worker-3\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;level_value\u0026#34;:20000} engine-export-service-7f6c797f5b-q7cm8 engine-export {\u0026#34;@timestamp\u0026#34;:\u0026#34;2023-06-15T02:09:26.990Z\u0026#34;,\u0026#34;@version\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Finished retrieving engine information chunk 51/51 for 2023-06-18 after 9 minute/s and 26 seconds\u0026#34;,\u0026#34;logger_name\u0026#34;:\u0026#34;org.manios.airplanefactory.engineexport.EngineExportService\u0026#34;,\u0026#34;thread_name\u0026#34;:\u0026#34;ForkJoinPool.commonPool-worker-3\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;level_value\u0026#34;:20000} engine-export-service-7f6c797f5b-q7cm8 engine-export {\u0026#34;@timestamp\u0026#34;:\u0026#34;2023-06-15T02:09:27.088Z\u0026#34;,\u0026#34;@version\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Loaded 1000 normal prices in 0 seconds\u0026#34;,\u0026#34;logger_name\u0026#34;:\u0026#34;org.manios.airplanefactory.engineexport.engine.dao.EngineRepositoryImpl\u0026#34;,\u0026#34;thread_name\u0026#34;:\u0026#34;ForkJoinPool.commonPool-worker-3\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;level_value\u0026#34;:20000} engine-export-service-7f6c797f5b-q7cm8 engine-export {\u0026#34;@timestamp\u0026#34;:\u0026#34;2023-06-15T02:09:27.139Z\u0026#34;,\u0026#34;@version\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Loaded 0 special prices in 0 seconds\u0026#34;,\u0026#34;logger_name\u0026#34;:\u0026#34;org.manios.airplanefactory.engineexport.engine.dao.EngineRepositoryImpl\u0026#34;,\u0026#34;thread_name\u0026#34;:\u0026#34;ForkJoinPool.commonPool-worker-3\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;level_value\u0026#34;:20000} engine-export-service-7f6c797f5b-q7cm8 engine-export {\u0026#34;@timestamp\u0026#34;:\u0026#34;2023-06-15T02:09:27.225Z\u0026#34;,\u0026#34;@version\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Started sending of 1000 messages\u0026#34;,\u0026#34;logger_name\u0026#34;:\u0026#34;org.manios.airplanefactory.engineexport.rabbit.RabbitService\u0026#34;,\u0026#34;thread_name\u0026#34;:\u0026#34;ForkJoinPool.commonPool-worker-3\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;level_value\u0026#34;:20000} engine-export-service-7f6c797f5b-q7cm8 engine-export {\u0026#34;@timestamp\u0026#34;:\u0026#34;2023-06-15T02:09:29.645Z\u0026#34;,\u0026#34;@version\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Finished sending of 1000 messages\u0026#34;,\u0026#34;logger_name\u0026#34;:\u0026#34;org.manios.airplanefactory.engineexport.rabbit.RabbitService\u0026#34;,\u0026#34;thread_name\u0026#34;:\u0026#34;ForkJoinPool.commonPool-worker-3\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;level_value\u0026#34;:20000} engine-export-service-7f6c797f5b-q7cm8 engine-export {\u0026#34;@timestamp\u0026#34;:\u0026#34;2023-06-15T02:09:29.645Z\u0026#34;,\u0026#34;@version\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Finished export for 2023-06-18 after 9 minute/s and 29 seconds\u0026#34;,\u0026#34;logger_name\u0026#34;:\u0026#34;org.manios.airplanefactory.engineexport.EngineExportService\u0026#34;,\u0026#34;thread_name\u0026#34;:\u0026#34;scheduling-1\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;level_value\u0026#34;:20000} engine-export-service-7f6c797f5b-q7cm8 engine-export {\u0026#34;@timestamp\u0026#34;:\u0026#34;2023-06-15T02:09:29.645Z\u0026#34;,\u0026#34;@version\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Engine export successfully finished for date period: [2023-06-15,2023-06-18] after: 9.494 min\u0026#34;,\u0026#34;logger_name\u0026#34;:\u0026#34;org.manios.airplanefactory.engineexport.EngineExportService\u0026#34;,\u0026#34;thread_name\u0026#34;:\u0026#34;scheduling-1\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;level_value\u0026#34;:20000} engine-export-service-7f6c797f5b-q7cm8 engine-export {\u0026#34;@timestamp\u0026#34;:\u0026#34;2023-06-15T02:09:29.664Z\u0026#34;,\u0026#34;@version\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Open links after closing: 0\u0026#34;,\u0026#34;logger_name\u0026#34;:\u0026#34;org.manios.airplanefactory.engineexport.database.OracleLinkService\u0026#34;,\u0026#34;thread_name\u0026#34;:\u0026#34;scheduling-1\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;level_value\u0026#34;:20000} This is quite normal, since we have configured Logback in Spring to print the logs in JSON format which is easily parsed by FluentD in order to push the logs to Elasticsearch and view them in Kibana. However, although JSON is nice and machine readable, the parsing and propagation of logs to Elasticsearch can take some time. Therefire we wanted to view the logs quickly and in a human friendly way!\nFortunately, Stern allows to format the logs using a Golang text template! We opened up an editor and created one! This is it (stern-spring-template.tpl):\nWe saved the template into a file, called stern-spring-template.tpl and we run again the command with the addition of the --template-file parameter!\n1 stern --context ctx-jetengine-factory --namespace=jetengine-factory-prod --template-file=\u0026#34;~/stern-spring-template.tpl\u0026#34; engine-export* Now the output looks like (almost) the same as the normal we get when we develop the Spring Boot application in our IDE!\n1 2 3 4 5 6 7 8 9 10 11 + engine-export-service-7f6c797f5b-q7cm8 › engine-export engine-export-service-7f6c797f5b-q7cm8 2023-06-15T02:09:23.240Z 20000 INFO --- [ForkJoinPool.co] org.manios.airplanefactory.engineexport.rabbit.RabbitService : Started sending of 1000 messages engine-export-service-7f6c797f5b-q7cm8 2023-06-15T02:09:25.710Z 20000 INFO --- [ForkJoinPool.co] org.manios.airplanefactory.engineexport.rabbit.RabbitService : Finished sending of 1000 messages engine-export-service-7f6c797f5b-q7cm8 2023-06-15T02:09:26.990Z 20000 INFO --- [ForkJoinPool.co] org.manios.airplanefactory.engineexport.EngineExportService : Finished retrieving engine information chunk 51/51 for 2023-06-18 after 9 minute/s and 26 seconds engine-export-service-7f6c797f5b-q7cm8 2023-06-15T02:09:27.088Z 20000 INFO --- [ForkJoinPool.co] org.manios.airplanefactory.engineexport.engine.dao.EngineRepository : Loaded 1000 normal prices in 0 seconds engine-export-service-7f6c797f5b-q7cm8 2023-06-15T02:09:27.139Z 20000 INFO --- [ForkJoinPool.co] org.manios.airplanefactory.engineexport.engine.dao.EngineRepository : Loaded 0 special prices in 0 seconds engine-export-service-7f6c797f5b-q7cm8 2023-06-15T02:09:27.225Z 20000 INFO --- [ForkJoinPool.co] org.manios.airplanefactory.engineexport.rabbit.RabbitService : Started sending of 1000 messages engine-export-service-7f6c797f5b-q7cm8 2023-06-15T02:09:29.645Z 20000 INFO --- [ForkJoinPool.co] org.manios.airplanefactory.engineexport.rabbit.RabbitService : Finished sending of 1000 messages engine-export-service-7f6c797f5b-q7cm8 2023-06-15T02:09:29.645Z 20000 INFO --- [ scheduling-1] org.manios.airplanefactory.engineexport.EngineExportService : Finished export for 2023-06-18 after 9 minute/s and 29 seconds engine-export-service-7f6c797f5b-q7cm8 2023-06-15T02:09:29.645Z 20000 INFO --- [ scheduling-1] org.manios.airplanefactory.engineexport.EngineExportService : Engine export successfully finished for date period: [2023-06-15,2023-06-18] after: 9.494 min engine-export-service-7f6c797f5b-q7cm8 2023-06-15T02:09:29.664Z 20000 INFO --- [ scheduling-1] org.manios.airplanefactory.engineexport.database.OracleLinkService : Open links after closing: 0 I hope this helped you a little bit with your next Kubernetes endeavour!\n","date":"2023-06-18T16:50:00+03:00","permalink":"https://manios.org/2023/06/18/kubernetes-view-pod-json-log-in-plain-text-with-stern-template/","title":"Kubernetes tail Spring Boot json logs in plain text Log4j (Logback) format using Stern"},{"content":"Yesterday, a client which uses Microsoft SQL Server 2008 R2 (SP2) wanted to test if his database backup can work in Microsoft SQL Server 2019 Developer version. He currently runs his SQL Server instance in Windows Server. He provided us with a .bak file, let us call it for this post WinCarFactorySQL.bak.\nRun a Microsoft SQL Server Docker container In order to test quickly, we are going to use the official, Ubuntu based, Microsoft SQL Server 2019 docker container. Therefore we download it:\n1 docker pull mcr.microsoft.com/mssql/server:2019-CU18-ubuntu-20.04 and run it in Developer mode:\n1 2 3 4 5 6 7 8 docker run -d \\ --name sqlserver2019 \\ -e \u0026#34;ACCEPT_EULA=Y\u0026#34; \\ -e \u0026#39;MSSQL_SA_PASSWORD=mysuperduperpassworD1234!56\u0026#39; \\ -e \u0026#34;MSSQL_PID=Developer\u0026#34; \\ -p 1433:1433 \\ -v \u0026#34;/home/mylinux/wincarfactorybak:/opt/wincarfactorybak\u0026#34; \\ mcr.microsoft.com/mssql/server:2019-CU18-ubuntu-20.04 As you can see we mount /home/mylinux/wincarfactorybak directory which contains the WinCarFactorySQL.bak in order to use it inside the container in /opt/wincarfactorybak directory.\nFor more information you can read: Quickstart: Run SQL Server Linux container images with Docker\nExamine .bak file contents In order to restore the database properly, we have to examine the .bak file contents. This can be achieved by executing this command on the running container:\n1 2 3 4 docker exec -it sqlserver2019 /opt/mssql-tools/bin/sqlcmd -S localhost \\ -U SA -P \u0026#39;mysuperduperpassworD1234!56\u0026#39; \\ -Q \u0026#39;RESTORE FILELISTONLY FROM DISK = \u0026#34;/opt/wincarfactorybak/WinCarFactorySQL.bak\u0026#34;\u0026#39; \\ | tr -s \u0026#39; \u0026#39; | cut -d \u0026#39; \u0026#39; -f 1-2 The output looks like this:\n1 2 3 4 5 6 LogicalName PhysicalName ----------- ------------ WinCarFactorySQL c:\\Program Files\\Microsoft SQL Server\\MSSQL10_50.SQLEXPRESS\\MSSQL\\DATA\\WinCarFactorySQL.mdf WinCarFactorySQL_log c:\\Program Files\\Microsoft SQL Server\\MSSQL10_50.SQLEXPRESS\\MSSQL\\DATA\\WinCarFactorySQL_log.ldf (2 rows) For more information you can read: Restore a SQL Server database in a Linux Docker container\nRestore database Now we have to import and relocate these two files into the directory SQL Server uses to store these files in this container. The docker container uses a non-root user called mssql. Therefore in order to find the directory we searched for *.mdf files:\n1 2 3 4 5 6 7 8 9 10 11 12 bob@bobis:/home/mylinux$ docker exec -it sqlserver2019 /bin/bash mssql@7b208ca8f759:/$ f^C (failed reverse-i-search)`fin\u0026#39;: ^C mssql@7b208ca8f759:/$ find -iname \u0026#34;*.mdf\u0026#34; ./var/opt/mssql/data/tempdb.mdf ./var/opt/mssql/data/msdbdata.mdf ./var/opt/mssql/data/model_replicatedmaster.mdf ./var/opt/mssql/data/model_msdbdata.mdf ./var/opt/mssql/data/model.mdf ./var/opt/mssql/data/master.mdf mssql@7b208ca8f759:/$ whoami mssql Finally we restore the WinCarFactorySQL.bak file using the following command:\n1 2 3 docker exec -it sqlserver2019 /opt/mssql-tools/bin/sqlcmd -S localhost \\ -U SA -P \u0026#39;mysuperduperpassworD1234!56\u0026#39; \\ -Q \u0026#39;RESTORE DATABASE WinCarFactorySQL FROM DISK = \u0026#34;/opt/wincarfactorybak/WinCarFactorySQL.bak\u0026#34; WITH MOVE \u0026#34;WinCarFactorySQL\u0026#34; TO \u0026#34;/var/opt/mssql/data/WinCarFactorySQL.mdf\u0026#34;, MOVE \u0026#34;WinCarFactorySQL_log\u0026#34; TO \u0026#34;/var/opt/mssql/data/WinCarFactorySQL_log.ldf\u0026#34;\u0026#39; If you get an output similar to the following, then your database restoration was successful!\n1 2 3 4 5 6 7 8 9 10 Processed 130576 pages for database \u0026#39;WinCarFactorySQL\u0026#39;, file \u0026#39;WinCarFactorySQL\u0026#39; on file 1. Processed 1 pages for database \u0026#39;WinCarFactorySQL\u0026#39;, file \u0026#39;WinCarFactorySQL_log\u0026#39; on file 1. Converting database \u0026#39;WinCarFactorySQL\u0026#39; from version 661 to the current version 904. Database \u0026#39;WinCarFactorySQL\u0026#39; running the upgrade step from version 661 to version 668. Database \u0026#39;WinCarFactorySQL\u0026#39; running the upgrade step from version 668 to version 669. -- some lines truncated -- Database \u0026#39;WinCarFactorySQL\u0026#39; running the upgrade step from version 901 to version 902. Database \u0026#39;WinCarFactorySQL\u0026#39; running the upgrade step from version 902 to version 903. Database \u0026#39;WinCarFactorySQL\u0026#39; running the upgrade step from version 903 to version 904. RESTORE DATABASE successfully processed 130577 pages in 10.623 seconds (96.030 MB/sec). For more information about these commands, you can read:\nRestore Files to a New Location (SQL Server) Change default database file and backup paths in SQL Server on Linux Note 1: Regarding directory permissions If you decide to use another directory to MOVE your .mdf and .ldf files into any other random directory, you might stumble uppon permission issues. For instance if instead of using /var/opt/mssql/data you use /opt/wincarfactorybak which is not writable by mssql user you might get errors like this:\n1 2 3 4 5 6 7 8 9 10 11 The operating system returned the error \u0026#39;5(Access is denied.)\u0026#39; while attempting \u0026#39;RestoreContainer::ValidateTargetForCreation\u0026#39; on \u0026#39;/opt/wincarfactorybak/WinCarFactorySQL.mdf\u0026#39;. Msg 3156, Level 16, State 5, Server 7b208ca8f759, Line 1 File \u0026#39;WinCarFactorySQL\u0026#39; cannot be restored to \u0026#39;/opt/wincarfactorybak/WinCarFactorySQL.mdf\u0026#39;. Use WITH MOVE to identify a valid location for the file. Msg 3634, Level 16, State 1, Server 7b208ca8f759, Line 1 The operating system returned the error \u0026#39;5(Access is denied.)\u0026#39; while attempting \u0026#39;RestoreContainer::ValidateTargetForCreation\u0026#39; on \u0026#39;/opt/wincarfactorybak/WinCarFactorySQL_log.ldf\u0026#39;. Msg 3156, Level 16, State 5, Server 7b208ca8f759, Line 1 File \u0026#39;WinCarFactorySQL\u0026#39; cannot be restored to \u0026#39;/opt/wincarfactorybak/WinCarFactorySQL_log.ldf\u0026#39;. Use WITH MOVE to identify a valid location for the file. Msg 3119, Level 16, State 1, Server 7b208ca8f759, Line 1 Problems were identified while planning for the RESTORE statement. Previous messages provide details. Msg 3013, Level 16, State 1, Server 7b208ca8f759, Line 1 RESTORE DATABASE is terminating abnormally. Note 2: Regarding SQL server version where backup was made If you received a database backup which fails to be restored locally getting the following error:\n1 2 3 4 Msg 3169, Level 16, State 1, Server 203c41e3a2e1, Line 1 The database was backed up on a server running version 16.00.1000. That version is incompatible with this server, which is running version 15.00.4261. Either restore the database on a server that supports the backup, or use a backup that is compatible with this server. Msg 3013, Level 16, State 1, Server 203c41e3a2e1, Line 1 RESTORE DATABASE is terminating abnormally. then it seems that the backup has been created using a different Microsoft SQL Server version than the one that you are running in your container. In this example we took help from https://sqlserverbuilds.blogspot.com/ and we found out that:\nThe backup was created using Microsoft SQL Server 2022 with build 16.0.1000. We tried to restore it in an older Microsoft SQL Server 2019 with build 15.0.4261. Therefore we need to run a container using an updated image according to this table which will be in our case: mcr.microsoft.com/mssql/server:2022-CU9-ubuntu-20.04\n","date":"2022-12-23T20:00:00+02:00","permalink":"https://manios.org/2022/12/23/oracle-sql-select-date-range-between-range/","title":"MS SQL Server Docker container restore database from .bak file created in Windows to a New Location"},{"content":"A few days ago, we had to select records from an Oracle database table which should be inside a given date range. The difficult thing with those records was that they had a date range defined in them too.\nThe table was described as:\n1 2 3 4 5 6 7 CREATE TABLE \u0026#34;DATEBOB\u0026#34; (\t\u0026#34;ID\u0026#34; NUMBER(*,0) NOT NULL ENABLE, \u0026#34;FROMDATE\u0026#34; DATE NOT NULL ENABLE, \u0026#34;TODATE\u0026#34; DATE NOT NULL ENABLE, CONSTRAINT \u0026#34;DATEBOB_PK\u0026#34; PRIMARY KEY (\u0026#34;ID\u0026#34;) ) and the records like:\nID FROMDATE TODATE 1 10-JUL-2022 10-JUL-2040 2 10-JUL-2024 10-JUL-2040 3 22-JUL-2022 23-JUL-2022 4 24-JUL-2022 25-JUL-2022 5 22-JUL-2022 22-JUL-2022 or in SQL insert statements:\n1 2 3 4 5 INSERT INTO \u0026#34;DATEBOB\u0026#34; (ID, FROMDATE, TODATE) VALUES (\u0026#39;1\u0026#39;, TO_DATE(\u0026#39;2022-07-10\u0026#39;, \u0026#39;YYYY-MM-DD\u0026#39;), TO_DATE(\u0026#39;2040-07-10\u0026#39;, \u0026#39;YYYY-MM-DD\u0026#39;)); INSERT INTO \u0026#34;DATEBOB\u0026#34; (ID, FROMDATE, TODATE) VALUES (\u0026#39;2\u0026#39;, TO_DATE(\u0026#39;2024-07-10\u0026#39;, \u0026#39;YYYY-MM-DD\u0026#39;), TO_DATE(\u0026#39;2040-07-10\u0026#39;, \u0026#39;YYYY-MM-DD\u0026#39;)); INSERT INTO \u0026#34;DATEBOB\u0026#34; (ID, FROMDATE, TODATE) VALUES (\u0026#39;3\u0026#39;, TO_DATE(\u0026#39;2022-07-22\u0026#39;, \u0026#39;YYYY-MM-DD\u0026#39;), TO_DATE(\u0026#39;2022-07-23\u0026#39;, \u0026#39;YYYY-MM-DD\u0026#39;)); INSERT INTO \u0026#34;DATEBOB\u0026#34; (ID, FROMDATE, TODATE) VALUES (\u0026#39;4\u0026#39;, TO_DATE(\u0026#39;2022-07-24\u0026#39;, \u0026#39;YYYY-MM-DD\u0026#39;), TO_DATE(\u0026#39;2022-07-25\u0026#39;, \u0026#39;YYYY-MM-DD\u0026#39;)); INSERT INTO \u0026#34;DATEBOB\u0026#34; (ID, FROMDATE, TODATE) VALUES (\u0026#39;5\u0026#39;, TO_DATE(\u0026#39;2022-07-22\u0026#39;, \u0026#39;YYYY-MM-DD\u0026#39;), TO_DATE(\u0026#39;2022-07-22\u0026#39;, \u0026#39;YYYY-MM-DD\u0026#39;)); In order to select the records which are valid from the current date up to 3 days in the future, you can do it with this query:\n1 2 3 4 5 select * from datebob where (trunc(sysdate) BETWEEN fromdate AND todate) or ((trunc(sysdate)+3) BETWEEN fromdate AND todate) ; example output when current date is 2022-07-22:\n1 2 3 4 5 6 ID FROMDATE TODATE ---- --------- --------- 1 10-JUL-22 10-JUL-40 3 22-JUL-22 23-JUL-22 4 24-JUL-22 25-JUL-22 5 22-JUL-22 22-JUL-22 In order to select the records which are valid from a specific date current date (let us say 2022-07-24) up to 3 days in the future, you can do it with this query:\n1 2 3 4 5 select * from datebob where (TO_DATE(\u0026#39;2022-07-24\u0026#39;, \u0026#39;YYYY-MM-DD\u0026#39;) BETWEEN fromdate AND todate) or ((TO_DATE(\u0026#39;2022-07-24\u0026#39;, \u0026#39;YYYY-MM-DD\u0026#39;)+3) BETWEEN fromdate AND todate) ; and the output looks like the following:\n1 2 3 4 ID FROMDATE TODATE ---- --------- --------- 1 10-JUL-22 10-JUL-40 4 24-JUL-22 25-JUL-22 Note that in order to add days in a DATE column, you just need to use the plus sign and the number of days to add. There is no specific function like DATE_ADD of MariaDB which offers different intervals, like seconds, months, etc.\n","date":"2022-07-22T21:55:00+03:00","permalink":"https://manios.org/2022/07/22/oracle-sql-select-date-range-between-range/","title":"SQL query to check if a date range is between two dates in Oracle database"},{"content":"Today we had a MongoDB 3.6 collection with the following documents:\n1 2 3 4 5 6 7 db.getCollection(\u0026#39;person\u0026#39;).insertMany([ {\u0026#34;name\u0026#34;: \u0026#34;Odyseas\u0026#34;, \u0026#34;surname\u0026#34;: \u0026#34;Androutsos\u0026#34;, \u0026#34;profilePicture\u0026#34;: null}, {\u0026#34;name\u0026#34;: \u0026#34;Theodoros\u0026#34;, \u0026#34;surname\u0026#34;: \u0026#34;Kolokotronis\u0026#34;, \u0026#34;profilePicture\u0026#34;: null}, {\u0026#34;name\u0026#34;: \u0026#34;Ioannis\u0026#34;, \u0026#34;surname\u0026#34;: \u0026#34;Makrygiannis\u0026#34;, \u0026#34;profilePicture\u0026#34;: null}, {\u0026#34;name\u0026#34;: \u0026#34;Ioannis\u0026#34;, \u0026#34;surname\u0026#34;: \u0026#34;Kapodistrias\u0026#34;, \u0026#34;profilePicture\u0026#34;: null}, {\u0026#34;name\u0026#34;: \u0026#34;Emanouil\u0026#34;, \u0026#34;surname\u0026#34;: \u0026#34;Pappas\u0026#34;, \u0026#34;profilePicture\u0026#34;: null} ]) and we wanted to update them with Go using mongodb/mongo-go-driver in order to become like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 [{ \u0026#34;name\u0026#34;: \u0026#34;Ioannis\u0026#34;, \u0026#34;surname\u0026#34;: \u0026#34;Kapodistrias\u0026#34;, \u0026#34;profilePicture\u0026#34;: { \u0026#34;image\u0026#34;:{ \u0026#34;iconUrl\u0026#34;:null, \u0026#34;retrievalStatus\u0026#34;: \u0026#34;NOT_FOUND\u0026#34;, \u0026#34;lastSuccessfulRetrieval\u0026#34;: ISODate(\u0026#34;2021-07-13T10:00:00.000Z\u0026#34;) } } }, { \u0026#34;name\u0026#34;: \u0026#34;Ioannis\u0026#34;, \u0026#34;surname\u0026#34;: \u0026#34;Makrygiannis\u0026#34;, \u0026#34;profilePicture\u0026#34;: { \u0026#34;image\u0026#34;:{ \u0026#34;iconUrl\u0026#34;:null, \u0026#34;retrievalStatus\u0026#34;: \u0026#34;NOT_FOUND\u0026#34;, \u0026#34;lastSuccessfulRetrieval\u0026#34;: ISODate(\u0026#34;2021-07-13T10:00:00.000Z\u0026#34;) } } }] . We were using the following update statement:\n1 2 3 4 5 6 7 8 9 10 db.getCollection(\u0026#39;person\u0026#39;).update( { \u0026#34;name\u0026#34;: \u0026#34;Ioannis\u0026#34; }, { \u0026#34;$set\u0026#34;: { \u0026#34;profilePicture.image.iconUrl\u0026#34;: null, \u0026#34;profilePicture.image.retrievalStatus\u0026#34;: \u0026#34;NOT_FOUND\u0026#34;, \u0026#34;profilePicture.image.lastSuccessfulRetrieval\u0026#34;: ISODate(\u0026#34;2021-07-13T10:00:00.000Z\u0026#34;), } }, {multi:true}) however the MongoDB driver was returning this error:\n1 Cannot create field \u0026#39;image\u0026#39; in element {profilePicture: null} Initially we thought that something is wrong with the single update() command and we tried in mongo shell the updateMany():\n1 2 3 4 5 6 7 8 9 db.getCollection(\u0026#39;person\u0026#39;).updateMany( { \u0026#34;name\u0026#34;: \u0026#34;Ioannis\u0026#34; }, { \u0026#34;$set\u0026#34;: { \u0026#34;profilePicture.image.iconUrl\u0026#34;: null, \u0026#34;profilePicture.image.retrievalStatus\u0026#34;: \u0026#34;NOT_FOUND\u0026#34;, \u0026#34;profilePicture.image.lastSuccessfulRetrieval\u0026#34;: ISODate(\u0026#34;2021-07-13T10:00:00.000Z\u0026#34;), } }) However it returned a quite similar error:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 Failed to execute script. Error: WriteError({ \u0026#34;index\u0026#34; : 0, \u0026#34;code\u0026#34; : 28, \u0026#34;errmsg\u0026#34; : \u0026#34;Cannot create field \u0026#39;image\u0026#39; in element {profilePicture: null}\u0026#34;, \u0026#34;op\u0026#34; : { \u0026#34;q\u0026#34; : { \u0026#34;surname\u0026#34; : \u0026#34;Manios\u0026#34; }, \u0026#34;u\u0026#34; : { \u0026#34;$set\u0026#34; : { \u0026#34;profilePicture.image.iconUrl\u0026#34; : null, \u0026#34;profilePicture.image.retrievalStatus\u0026#34; : \u0026#34;NOT_FOUND\u0026#34;, \u0026#34;profilePicture.image.lastSuccessfulRetrieval\u0026#34; : ISODate(\u0026#34;2021-07-13T10:00:00Z\u0026#34;) } }, \u0026#34;multi\u0026#34; : true, \u0026#34;upsert\u0026#34; : false } }) : WriteError({ \u0026#34;index\u0026#34; : 0, \u0026#34;code\u0026#34; : 28, \u0026#34;errmsg\u0026#34; : \u0026#34;Cannot create field \u0026#39;image\u0026#39; in element {profilePicture: null}\u0026#34;, \u0026#34;op\u0026#34; : { \u0026#34;q\u0026#34; : { \u0026#34;surname\u0026#34; : \u0026#34;Manios\u0026#34; }, \u0026#34;u\u0026#34; : { \u0026#34;$set\u0026#34; : { \u0026#34;profilePicture.image.iconUrl\u0026#34; : null, \u0026#34;profilePicture.image.retrievalStatus\u0026#34; : \u0026#34;NOT_FOUND\u0026#34;, \u0026#34;profilePicture.image.lastSuccessfulRetrieval\u0026#34; : ISODate(\u0026#34;2021-07-13T10:00:00Z\u0026#34;) } }, \u0026#34;multi\u0026#34; : true, \u0026#34;upsert\u0026#34; : false } }) WriteError@src/mongo/shell/bulk_api.js:458:48 mergeBatchResults@src/mongo/shell/bulk_api.js:855:49 executeBatch@src/mongo/shell/bulk_api.js:919:13 Bulk/this.execute@src/mongo/shell/bulk_api.js:1163:21 DBCollection.prototype.updateMany@src/mongo/shell/crud_api.js:690:17 @(shell):1:1 We searched for the error in the internet but we could not understand where we are wrong:\nGoogle: code: 28, \u0026ldquo;errmsg\u0026rdquo; : \u0026ldquo;Cannot create element in Null\u0026rdquo;, Mongodb “Cannot create field \u0026lsquo;x\u0026rsquo; in element” [duplicate] NULL data type in MongoDB After some time and numerous attempts, we realised that MongoDB was trying to tell us that it cannot create the subdocument:\n1 2 3 4 5 \u0026#34;image\u0026#34;:{ \u0026#34;iconUrl\u0026#34;:null, \u0026#34;retrievalStatus\u0026#34;: \u0026#34;NOT_FOUND\u0026#34;, \u0026#34;lastSuccessfulRetrieval\u0026#34;: ISODate(\u0026#34;2021-07-13T10:00:00.000Z\u0026#34;) } inside a field which is alread assigned to null:\n1 \u0026#34;profilePicture\u0026#34;: null So with a small change, the update statement works:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 // ------ // BEFORE // ------ // THIS DOES NOT WORK AND CONFUSES MONGODB. db.getCollection(\u0026#39;person\u0026#39;).update( { \u0026#34;name\u0026#34;: \u0026#34;Ioannis\u0026#34; }, { \u0026#34;$set\u0026#34;: { \u0026#34;profilePicture.image.iconUrl\u0026#34;: null, \u0026#34;profilePicture.image.retrievalStatus\u0026#34;: \u0026#34;NOT_FOUND\u0026#34;, \u0026#34;profilePicture.image.lastSuccessfulRetrieval\u0026#34;: ISODate(\u0026#34;2021-07-13T10:00:00.000Z\u0026#34;), } }, {multi:true} ) // ------ // AFTER // ------ // THIS WORKS db.getCollection(\u0026#39;person\u0026#39;).update( { \u0026#34;name\u0026#34;: \u0026#34;Ioannis\u0026#34; }, { \u0026#34;$set\u0026#34;: { \u0026#34;profilePicture\u0026#34;: { \u0026#34;image\u0026#34;: { \u0026#34;iconUrl\u0026#34;: null, \u0026#34;retrievalStatus\u0026#34;: \u0026#34;NOT_FOUND\u0026#34;, \u0026#34;lastSuccessfulRetrieval\u0026#34;: ISODate(\u0026#34;2021-07-13T10:00:00.000Z\u0026#34;), } } } }, {multi:true} ) We tested the example in the following versions (running db.version() in Mongo shell):\nCosmosDB for MongoDB 3.6.0 MongoDB 3.6.22 MongoDB 4.4.0 and all behave identically.\nWe hope this helped you and you gain some minutes instead of searching in the web to find a solution! Greetings from our hot Greece with 40 °C!\n","date":"2021-08-04T21:55:00+03:00","permalink":"https://manios.org/2021/08/04/go-mongodb-error-cannot-create-field-in-element-null/","title":"Go Mongodb Error Cannot create field X in element {Y: null}"},{"content":"Yesterday we had to marshal and umarshal to/from JSON some HTTP requests and responses in a Go microservice we are building. We already use ericlagergren/decimal. However we have spotted that the struct for the response were declared like this:\n1 2 3 4 type IncomeExpensesResponse struct { Income string `json:\u0026#34;income\u0026#34;` Expenses string `json:\u0026#34;expenses\u0026#34;` } and inside the source code, my colleagues were trying to manually convert from/to string to *decimal.Big for responses like:\n1 2 3 4 { \u0026#34;income\u0026#34;: \u0026#34;14.67\u0026#34;, \u0026#34;expenses\u0026#34;: \u0026#34;340.75\u0026#34; } This is actually not needed! Why should we reinvent the wheel?\nHence I created a small test to demonstrate ericlagergren/decimal JSON marshalling and unmarshalling capabilities:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 package main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;testing\u0026#34; \u0026#34;github.com/ericlagergren/decimal\u0026#34; \u0026#34;github.com/stretchr/testify/assert\u0026#34; ) type IncomeExpenses struct { Income *decimal.Big `json:\u0026#34;income\u0026#34;` Expenses *decimal.Big `json:\u0026#34;expenses\u0026#34;` } func TestMarshalIncomeExpensesToJson(t *testing.T) { pentochiliaro := decimal.New(1467, 2) drachmaEuro := decimal.New(34075, 2) testExpense := IncomeExpenses{ Income: pentochiliaro, Expenses: drachmaEuro, } expectedJson := `{\u0026#34;income\u0026#34;: \u0026#34;14.67\u0026#34;, \u0026#34;expenses\u0026#34;: \u0026#34;340.75\u0026#34;}` jsonStr, _ := json.Marshal(testExpense) assert.JSONEq(t, expectedJson, string(jsonStr)) } func TestUnMarshalIncomeExpensesFromJson(t *testing.T) { pentochiliaro := decimal.New(1467, 2) drachmaEuro := decimal.New(34075, 2) expectedExpense := IncomeExpenses{ Income: pentochiliaro, Expenses: drachmaEuro, } inputJson := `{\u0026#34;income\u0026#34;: \u0026#34;14.67\u0026#34;, \u0026#34;expenses\u0026#34;: \u0026#34;340.75\u0026#34;}` var incomeExpences IncomeExpenses _ = json.Unmarshal([]byte(inputJson), \u0026amp;incomeExpences) assert.Equal(t, expectedExpense, incomeExpences) } If you are puzzled by the words pentochiliaro and drachmaEuro, let me explain!\nPentochiliaro (πεντοχίλιαρο) is a complex word consisting of the words Pente (Πέντε), which means five in Greek and Chiliades (Χιλιάδες) which means thousands in Greek. This how it the 5000 drachma banknote was called before Euro currency came! drachmaEuro: This is the currency exchange rate while we used both Drachmas and Euros in Greece. 1 Euro was 340.75 Greek Drachmas. I hope this helps! Have a nice evening from sunny Greece!\n","date":"2021-07-24T19:27:00+03:00","permalink":"https://manios.org/2021/07/24/golang-decimal-field-in-struct-json-convert/","title":"Convert from/to JSON decimal fields in Go (golang) structs"},{"content":"A few days ago we were implementing a Spring Boot application which stored its data into a PostgreSQL database. As a requirement we needed to search into the database with LIKE operator in our SQL queries and at the same time perform case and accent insensitive searches. Since we are Greek, this is pretty typical for us because our letters have diacritics (\u0026ldquo;tonos\u0026rdquo;). So a query like the following\n1 2 3 select * from blablatable where name like \u0026#39;%χρη%\u0026#39; ; should return results for:\nΧρήστος ΧΡΗΣΤΟΣ χΡήστος The aforementioned SQL query works perfectly normal in a MySQL or MariaDB database with collation utf8_general_ci. However in PostgreSQL this is not the case. So after some searching and reading StackOverflow #11005036, we figured out how to make case and accent insensitive search SQL queries.\nFire up a PostgreSQL Docker container We chose to use PostgreSQL inside a Docker container, so we fire up one:\n1 2 3 4 5 6 docker run --name postgres \\ -p 5432:5432 \\ -e POSTGRES_PASSWORD=strongpassword \\ -e POSTGRES_USER=stronguser \\ -e POSTGRES_DB=airplanes \\ -d postgres:13.0 Connect to PostgreSQL using a Docker container We used a temporary container to connect to our newly created database. You can skip this step and use another client such as pgAdmin or DBeaver.\n1 2 3 docker run -it --rm postgres:13.0 psql -h host.docker.internal -d airplanes --user stronguser Password for user stronguser: \u0026lt;enter thy password\u0026gt; Mind the host.docker.internal. This host name is used only in Docker for Windows to mark the host computer where Docker runs.\nCreate an extension in PostgreSQL In order to use case and accent insensitive search SQL queries, we need to use a PostgreSQL extension. So, execute in your favourite PostgreSQL client:\n1 CREATE EXTENSION unaccent; Create a table and insert some data Then we need to create a simple table in the default schema public, which will have some text data with accents:\n1 2 3 4 5 6 7 create sequence public.BOBOS_SEQUENCE start 1 increment 1; create table public.bobos ( ID int8 not null DEFAULT nextval(\u0026#39;BOBOS_SEQUENCE\u0026#39;), name varchar(255), primary key (ID) ); After that we populate it with some test data:\n1 2 3 4 INSERT INTO public.bobos (name) VALUES(\u0026#39;Τάκης, the best handmade gyros of Triandria\u0026#39;); INSERT INTO public.bobos (name) VALUES(\u0026#39;Χρήστος\u0026#39;); INSERT INTO public.bobos (name) VALUES(\u0026#39;ΧΡΗΣΤΟΣ\u0026#39;); INSERT INTO public.bobos (name) VALUES(\u0026#39;χΡήστος\u0026#39;); SQL case and accent insensitive queries Finally in order to perform case and accent insensitive SQL queries , execute the following statement:\n1 2 3 select * from public.bobos where unaccent(LOWER(NAME)) like unaccent(LOWER(\u0026#39;%χρη%\u0026#39;)) ; The results will be like the following:\n1 2 3 4 5 id|name | --|-------| 2|Χρήστος| 3|ΧΡΗΣΤΟΣ| 4|χΡήστος| As you might have noticed, we need to use both unaccent() extension and LOWER() function in order to meet the desired result.\nI hope you found this article useful and you do not waste any more time searching in the web. Have a nice day and love thy neighbour!\n","date":"2021-02-17T21:00:00+02:00","permalink":"https://manios.org/2021/02/17/postgresql-case--accent-insentitive-sql-queries/","title":"PostgreSQL case and accent insentitive SQL queries"},{"content":"Issues After Windows 10 performed its updates in our Lenovo Thinkpad T470p laptop, we could not use the camera. The symptoms were the following: When we tried to use the laptop camera, the camera turned on (led light is green) and after 2 seconds it turned off (led light off).\nThe laptop has Windows 10. In detail:\n1 2 3 4 5 PS C:\\Users\\sbobos\u0026gt; [System.Environment]::OSVersion.Version Major Minor Build Revision ----- ----- ----- -------- 10 0 18362 0 We had searched through the web and followed multiple steps, but the camera was still not working:\n[SOLVED] Lenovo – Built-in webcam camera not working (Windows 10) https://www.techjunkie.com/lenovo-webcam-not-working-issue/ https://forums.lenovo.com/t5/ThinkPad-T400-T500-and-newer-T/T410-Webcam-turning-off-after-3-10-seconds/td-p/1184859 Google : lenovo camera switches off automatically Google: lenovo camera not working windows 10 update My computer looked like as it the solutions described above:\nThe camera permissions:\nThe Lenovo Vantage configuration was:\nCamera driver properties:\nThe actual problem: After more than 3 hours it struck us in the head: \u0026ldquo;Why don\u0026rsquo;t we have a look in Windows log?\u0026rdquo;. While inspecting it, we stumbled upon this error:\n1 2 3 4 5 6 7 8 9 10 11 Faulting application name: svchost.exe_FrameServer, version: 10.0.18362.1, time stamp: 0x32d6c210 Faulting module name: RsProvider.dll, version: 1.30.0.0, time stamp: 0x57ee1a06 Exception code: 0xc0000005 Fault offset: 0x000000000007bd6d Faulting process id: 0x24b4 Faulting application start time: 0x01d5a4ef579b77f9 Faulting application path: C:\\WINDOWS\\System32\\svchost.exe Faulting module path: C:\\Program Files\\Realtek\\RsProviders\\RsProvider.dll Report Id: c07bcfb8-dfcc-401d-b116-72ccc3417d71 Faulting package full name: Faulting package-relative application ID: The error appeared whenever we were trying to use the Camera.\nSolution 1 This solution is a contribution from user lgpms who added it as a comment to our initial post. I place it first, because it is simpler than my solution 2 and I can also confirm that it works for us. So thanks again and kudos to lgpms!\nOpen your File Explorer and browse to the location C:\\Program Files\\Realtek\\RsProviders. Rename the directory from RsProviders to something else, like RsProviders_ (note the underscore in the end). So the directory will become:\nOLD NAME: C:\\Program Files\\Realtek\\RsProviders NEW NAME: C:\\Program Files\\Realtek\\RsProviders_ It seems that for some reason the FrameServer moves on and works when it does not find the initial path.\nSolution 2 After searching this error at Google, I found this blog post where it gives the final solution using registry. So if Solution 1 did not work for you , give it a try:\nCreate a Restore Point first.\nStart Regedit.exe and go to the following key:\n1 HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows Media Foundation\\Platform Create a REG_DWORD value named EnableFrameServerMode and leave the data to 0. If you’re using Windows 64-bit, make the changes here, in addition:\n1 HKEY_LOCAL_MACHINE\\SOFTWARE\\WOW6432Node\\Microsoft\\Windows Media Foundation\\Platform The above key is for 32-bit applications running on a Windows 64-bit computer. Exit the Registry Editor.\nNo other solution had worked for us. We hope that FrameServer is not used by another program so this trick will not cause any more issues.\nI hope it helped! Happy new year 2020!\n","date":"2020-01-12T12:19:00+02:00","permalink":"https://manios.org/2020/01/12/lenovo-thinkpad-t470p-camera-problem-windows-10/","title":"Lenovo Thinkpad T470p camera problem switching on and off in Windows 10"},{"content":"Today we had to deliver a .pdf file to a customer containing documentation about a system. The documentation was written in Markdown and needed to be converted to PDF. After some thoughts we did not find some straightforward way to do it, so we needed to convert Markdown to Asciidoctor using Pandoc first and then from Asciidoctor to PDF. In addition to that, we love Docker, so we used Docker containers to achieve that. The containers used:\npandoc/core asciidoctor/docker-asciidoctor We prepared quickly a small Bash script, which is the following (Listing of convertMdToPdf.sh):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/bin/bash set -eux INPUT_MD_FILE=bob.md OUT_ADOC_FILE=tmpOut.adoc OUT_TMP_PDF_FILE=tmpOut.pdf OUT_PDF_FILE=documentation.pdf echo \u0026#34;Clear temporary files (if exist)\u0026#34; # Clear any temporary files (rm -f \u0026#34;${OUT_ADOC_FILE}\u0026#34; \u0026#34;${OUT_PDF_FILE}\u0026#34; \u0026#34;${OUT_TMP_PDF_FILE}\u0026#34; || true) # Convert Markdown to Asciidoctor docker run --rm -v \u0026#34;$(pwd)\u0026#34;:/data pandoc/core -f markdown -t asciidoc -i \u0026#34;${INPUT_MD_FILE}\u0026#34; -o \u0026#34;${OUT_ADOC_FILE}\u0026#34; # Convert Asciidoctor to PDF docker run --rm -v $(pwd):/documents/ asciidoctor/docker-asciidoctor asciidoctor-pdf \u0026#34;${OUT_ADOC_FILE}\u0026#34; # Rename the output PDF to a more proper one mv \u0026#34;${OUT_TMP_PDF_FILE}\u0026#34; \u0026#34;${OUT_PDF_FILE}\u0026#34; This is it! Very simple and straightforward! I hope this article helps you and you will not devote much time to do the same. Have a nice day from a stormy Greece!\n","date":"2020-01-08T18:01:00+02:00","permalink":"https://manios.org/2020/01/08/convert-markdown-to-pdf-using-docker-pandoc-asciidoctor/","title":"Convert Markdown to PDF using Docker, Pandoc and Asciidoctor"},{"content":"Debugging an application which runs on a remote Virtual Machine can prove to be very hard, especially if your client has \u0026ldquo;exceptional\u0026rdquo; conditions due to security, company or any other psycological measures/policies. For example, your VM can be a Windows machine (yes, Windows servers are an exception themselves and impose many issues and difficulties), it may have no internet connection or accept no traffic other than http and RDP. Typically we as developers are called to confront such installations and make them work. As a paralellism, we are asked to fix a vehicle engine through the exhaust pipe without having access to the hood!\nIf you are not bored to death yet, you may proceed!\nThe problem A few days ago we had to remotely debug a Java Enterprise application inside a VM. The interesting fact was that everything had to be offline. Thus IDE installation, Maven installation and compile a Java project using Maven with no internet connection. Everything had to go fully offline. Really. Trully. Nothing special! (for a non technical person) At first we thought that it would suffice if we just copy the local maven repository from a development machine to the VM. Unfortunately that was not the case and it was not as simple as it looked like.\nAfter investing 3 hours in searching and numerous trial and error attempts we managed to compile the application using Maven. The steps described are performed in 2 phases:\nPreparation steps performed in a development machine with internet connection. Steps performed inside the VM with no internet connection (trully offline). Inside your development machine Step 1: Create a new local repository This new local repository will be used only for the dependencies required by your application to be compiled in offline mode. It is unnecessary to copy your everyday repository with all other redundant dependencies which maybe numerous and great in size.\nIn order to create a new local repository, navigate to your Maven settings.xml file. By default you can find it in:\nC:\\Users\\\u0026lt;your-username\u0026gt;\\.m2\\settings.xml: in Windows machines /home/\u0026lt;your-username\u0026gt;/.m2/settings.xml: in Linux or *nix machines Edit the line:\n1 2 \u0026lt;!-- Point to a new empty directory --\u0026gt; \u0026lt;localRepository\u0026gt;C:/apps/m2repo\u0026lt;/localRepository\u0026gt;--\u0026gt; Step 2: Run Maven to download dependencies for offline usage In order to to download dependencies for offline usage, inside our project we execute the following:\n1 mvn dependency:go-offline Helpful link: StackOverflow #8851424.\nStep 3: Run your usual Maven goals You also have to run your usual Maven goals which you eventually you are going to execute again in the offline VM, like clean, test, package. The reason for that is that Maven did not download the .jars needed for these goals when you run mvn dependency:go-offline. In our case we run:\n1 2 3 mvn clean test mvn clean install mvn clean package You will notice that Maven downloads some extra .jars needed to execute its goals:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 $ mvn clean install [INFO] Scanning for projects... [INFO] [INFO] ------------------------------------------------------------------------ [INFO] Building jetsprinkler 2.5.0-SNAPSHOT [INFO] ------------------------------------------------------------------------ [INFO] [INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ jetsprinkler --- [INFO] [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ jetsprinkler --- [INFO] Using \u0026#39;UTF-8\u0026#39; encoding to copy filtered resources. [INFO] Copying 11 resources [INFO] [INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ jetsprinkler --- [INFO] Compiling 64 source files to C:\\javaprojects\\jetsprinkler\\target\\classes [INFO] [INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ jetsprinkler --- [INFO] Using \u0026#39;UTF-8\u0026#39; encoding to copy filtered resources. [INFO] Copying 8 resources [INFO] [INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ jetsprinkler --- [INFO] Compiling 15 source files to C:\\javaprojects\\jetsprinkler\\target\\test-classes [INFO] [INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ jetsprinkler --- [INFO] Surefire report directory: C:\\javaprojects\\jetsprinkler\\target\\surefire-reports Downloading from public: https://nexus.manios.org/nexus/repository/public/org/apache/maven/surefire/surefire-junit4/2.12.4/surefire-junit4-2.12.4.pom Downloaded from public: https://nexus.manios.org/nexus/repository/public/org/apache/maven/surefire/surefire-junit4/2.12.4/surefire-junit4-2.12.4.pom (2.4 kB at 4.2 kB/s) Downloading from public: https://nexus.manios.org/nexus/repository/public/org/apache/maven/surefire/surefire-providers/2.12.4/surefire-providers-2.12.4.pom Downloaded from public: https://nexus.manios.org/nexus/repository/public/org/apache/maven/surefire/surefire-providers/2.12.4/surefire-providers-2.12.4.pom (2.3 kB at 18 kB/s) Downloading from public: https://nexus.manios.org/nexus/repository/public/org/apache/maven/surefire/surefire-junit4/2.12.4/surefire-junit4-2.12.4.jar Downloaded from public: https://nexus.manios.org/nexus/repository/public/org/apache/maven/surefire/surefire-junit4/2.12.4/surefire-junit4-2.12.4.jar (37 kB at 187 kB/s) ------------------------------------------------------- T E S T S ------------------------------------------------------- Step 4: Copy your local repository to the remote VM We have to copy the contents of our new local repository (in our case C:/apps/m2repo) to the remote VM with no internet connection. Place the contents of the repository in the respective repository directory.\nInside the offline machine (with no internet connection) The following steps will be run inside the target machine with no internet connection. Verify that you have cloned your source code somewhere.\nStep 5: Remove dependency files that cause trouble in offline mode When Maven resolves and downloads a dependency from the internet, it stores 2-3 extra files other than the .jars that contain the library, javadoc or the sources. For example for spring-core, the files inside the local repository are:\n1 2 3 4 5 6 7 ls -l ~/.m2/repository/org/springframework/spring-core/4.1.7.RELEASE/ total 1004 -rw-r--r-- 1 bobos bobos 212 Jun 17 13:16 _remote.repositories -rw-r--r-- 1 bobos bobos 1008584 Jun 17 13:16 spring-core-4.1.7.RELEASE.jar -rw-r--r-- 1 bobos bobos 40 Jun 17 13:16 spring-core-4.1.7.RELEASE.jar.sha1 -rw-r--r-- 1 bobos bobos 2490 Jun 17 13:16 spring-core-4.1.7.RELEASE.pom -rw-r--r-- 1 bobos bobos 40 Jun 17 13:16 spring-core-4.1.7.RELEASE.pom.sha1 An here is the peculiar fact:\nAlthough we configure Maven to work offline, when it detects files with the extension *.repositories or *.sha1, it tries to connect to the internet! Consequently the Maven goal we run tries to download the dependency from the internet and it eventually fails after timeout (StackOverflow #33548395). To resolve this issue we have to recursively delete all files with the aforementioned suffixes. Hence, execute:\n1 2 3 4 5 6 7 # cd to repository directory cd ~/.m2/repository # Then delete recursively all files # with the extension `*.repositories` or `*.sha1` find -iname \u0026#34;*.repositories\u0026#34; -exec rm -f {} \\; \u0026amp;\u0026amp; \\ find -iname \u0026#34;*.sha1\u0026#34; -exec rm -f {} \\; This issue might also be resolved by tweaking the updatePolicy in Maven as mentioned in StackOverflow #33548395 but unfortunately we did not have the time to test it.\nStep 6 (optional): Fix parent pom references In our case, the project had a parent pom:\n1 2 3 4 5 \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;com.airplaneman\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;super-pom\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.1.4\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt; which could not be downloaded from an external Maven repository. We had the files offline, however this configuration could not work. To make it work we had to add the \u0026lt;relativePath\u0026gt; to the pom file. This is trully a relative path to the project.\nHence our project was placed in:\nC:/projects/jetsprinkler and the parent pom was located in\nC:/Users/cmanios/.m2/repository/com/airplanman/super-pom/1.1.4/super-pom-1.1.4.pom To reference the parent pom in a relative way, we resulted in the following configuration:\n1 2 3 4 5 6 \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;com.airplaneman\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;super-pom\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.1.4\u0026lt;/version\u0026gt; \u0026lt;relativePath\u0026gt;../../Users/cmanios/.m2/repository/com/airplanman/super-pom/1.1.4/super-pom-1.1.4.pom\u0026lt;/relativePath\u0026gt; \u0026lt;/parent\u0026gt; For more info regarding parent pom refer to:\nDZone: Maven Non Resolvable parent pom StackOverflow #7612309 Non Resolvable parent pom Google search: Offline Non Resolvable parent pom Step 7: Configure Maven to work offline Edit your Maven settings.xml file in the following lines:\n1 2 3 4 5 \u0026lt;!-- (Optional) Set the path to your local repository --\u0026gt; \u0026lt;localRepository\u0026gt;/path/to/local/repo\u0026lt;/localRepository\u0026gt; \u0026lt;!-- Run Maven in Offline mode --\u0026gt; \u0026lt;offline\u0026gt;true\u0026lt;/offline\u0026gt; Step 8: Execute your goals with -o and -nsu flag It is useful that you run your goals with -o and -nsu flags. According to documentation:\n-o: Work offline -nsu,--no-snapshot-updates: Suppress SNAPSHOT updates Thus your goal will look like this:\n1 mvn -nsu -o clean install Troubleshooting Cannot access central (https://repo.maven.apache.org/maven2) in offline mode and the artifact org.apache.maven.plugins\u0026hellip; You may step on this error:\n1 2 3 4 5 [WARNING] The POM for org.apache.maven.plugins:maven-resources-plugin:jar:2.6 is missing, no dependency information available [INFO] --------------------------------------------- [INFO] BUILD FAILURE [INFO] ------------------------------------------------------ [ERROR] Plugin org.apache.maven.plugins:maven-resources-plugin:2.6 or one of its dependencies could not be resolved: Cannot access central (https://repo.maven.apache.org/maven2) in offline mode and the artifact org.apache.maven.plugins:maven-resources-plugin:jar:2.6 has not been downloaded from it before. -\u0026gt; [Help 1] if you omit Step 3. Maven tries to download it\u0026rsquo;s plugins to run the goals but they do not exist.\nNon-resolvable parent POM [\u0026hellip;] and \u0026lsquo;parent.relativePath\u0026rsquo; points at no local POM If you see an error similar to the following:\n1 2 3 4 [INFO] Scanning for projects... [DEBUG] Verifying availability of C:\\Users\\admin_manios\\.m2\\repository\\com\\airplanman\\super-pom\\1.1.4\\super-pom-1.1.4.pom from [central (https://repo.maven.apache.org/maven2, default, releases)] [ERROR] [ERROR] Some problems were encountered while processing the POMs: [FATAL] Non-resolvable parent POM for airplanman.airsprinkler:jetengine:2.5.0-SNAPSHOT: Cannot access central (https://repo.maven.apache.org/maven2) in offline mode and the artifact com.airplaneman:super-pom:pom:1.1.4 has not been downloaded from it before. and \u0026#39;parent.relativePath\u0026#39; points at no local POM @ line 10, column 10 then it means that either you did not copy the parent pom in your local repository for offline usage, or you did not set correctly the relativePath. Please refer to Step 6.\nMaven attempts to download artifacts although we have configure it for offline mode Either you have not downloaded all your dependencies as described in Step 2 or you have not deleted the files described in Step 5.\nConclusion I hope this article helps you and you will not waste so much time to do the same. Have a nice day and a wonderful and fruitful life!\n","date":"2019-08-21T20:57:53+03:00","permalink":"https://manios.org/2019/08/21/force-maven-offline-execute-goal-dependencies/","title":"Force Maven to execute offline with local repository dependencies in a machine without Internet connection"},{"content":"Yesterday a colleague had a problem on finding a query. His documents looked like the following:\n1 2 3 { id : 1, category : \u0026#34;aaa\u0026#34;, answers : [ { gender : \u0026#34;male\u0026#34;} ] } { id : 2, category : \u0026#34;bbb\u0026#34; , answers : [ { gender : \u0026#34;female\u0026#34;} ] }, { id : 3, category : \u0026#34;aaa\u0026#34;, answers : [ { gender : \u0026#34;male\u0026#34;} ] } He needed to query them and get an output similar to the following (format did not matter):\n1 2 3 4 { \u0026#34;category\u0026#34; : \u0026#34;aaa\u0026#34;, \u0026#34;male\u0026#34; : 2 }, { \u0026#34;category\u0026#34; : \u0026#34;aaa\u0026#34;, \u0026#34;female\u0026#34; : 0 }, { \u0026#34;category\u0026#34; : \u0026#34;bbb\u0026#34;, \u0026#34;female\u0026#34; : 1 }, { \u0026#34;category\u0026#34; : \u0026#34;bbb\u0026#34;, \u0026#34;male\u0026#34; : 0 } His requirments was to fetch the data using only one query using MongoDB aggregation. So after some thought I concluded to the following query:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 db.getCollection(\u0026#39;question1034\u0026#39;) .aggregate([ { $unwind : \u0026#34;$answers\u0026#34; }, { $group: { _id : {\u0026#34;gender\u0026#34; : \u0026#34;$answers.gender\u0026#34;, \u0026#34;category\u0026#34; : \u0026#34;$category\u0026#34;}, count: { $sum: 1 }} }, { $project: { _id :0 , \u0026#34;gender\u0026#34;: \u0026#34;$_id.gender\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;$_id.category\u0026#34;, count: 1 }} ]) The query returns this result:\n1 2 3 4 5 6 7 8 9 10 { \u0026#34;count\u0026#34; : 1.0, \u0026#34;gender\u0026#34; : \u0026#34;female\u0026#34;, \u0026#34;category\u0026#34; : \u0026#34;bbb\u0026#34; }, { \u0026#34;count\u0026#34; : 2.0, \u0026#34;gender\u0026#34; : \u0026#34;male\u0026#34;, \u0026#34;category\u0026#34; : \u0026#34;aaa\u0026#34; } I hope this helped you if you stumble upon an issue like this. Greetings from sunny and hot Greece (inside an airconditioned environment)!\n","date":"2019-08-11T10:41:00+03:00","permalink":"https://manios.org/2019/08/11/mongodb-aggregate-documents-by-property/","title":"MongoDB query to aggregate documents by a specific property"},{"content":"First Incident A year ago (April 2017) while we were using a HP 630 notebook in a training, it started overheating and eventually made an emergency shutdown. The fan was spinning at maximum speed and the problem was not solved by restarting it. Temperatures could rise high up to 75 degrees Celcious when browsing pages, not to mention when running VMs for our training! At the same time a friend of ours, who owns the same notebook, informed us that due to the overheating, its WiFi module got burnt.\nThe problem After searching in the internet, we found out that the issue can be caused by dust inside the CPU fan which blocks the air ventilation grilles. Thus, CPU is suffocating.\nThe solution proposed was to disassemble the notebook and clean the CPU fan. Be advised that the notebook worked for 4 years, overheating, but neglected because it had not reached the issue of abrupt shutdown.\nRemedy We wanted to clean the notebook ourselves, so we followed the official HP 630 Product End-of-Life Disassembly Instructions (alternative link) combined with visual assistance from a Youtube video and we removed the CPU fan in order to clean it. After dissasembling the CPU fan we found out that all ventilation grilles were blocked by a \u0026ldquo;sponge\u0026rdquo; of dust formed in front of them. Under no circumstances air could could cool down the CPU!\nAfter cleaning the CPU fan from the dust, the temperatures went back to normal as seen in the screenshot:\nAnnual Service Yesterday we cleaned it again after 17 months as the notebook was overheating. We discovered that the dust had formed again a sponge\n, lesser in thickness than the previous time.\nConsequently we will monitor the temperature of our notebook and when it starts dangerously increasing, we will have to clean it again! We hope it does happen once in a year as the disassembly, cleaning and reassembly costs us two hours of work!\nCheers!\n","date":"2018-10-07T21:54:00+03:00","permalink":"https://manios.org/2018/10/07/hp-630-notebook-overheating-problem/","title":"Solved: HP 630 notebook overheating problem"},{"content":"Today we were trying to debug Metricbeat (on commit #cff3e40c) using Visual Studio Code v1.27.2 and Delve debugger but it failed with the error:\n1 Could not find __debug_frame section in binary Our environment was:\n1 2 3 4 5 6 $ go version go version go1.10.4 linux/amd64 $ dlv version Delve Debugger Version: 1.1.0 Build: $Id: 1990ba12450cab9425a2ae62e6ab988725023d5c Our Visual Studio Code debug configuration was the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Launch\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;go\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;remotePath\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;port\u0026#34;: 2345, \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${fileDirname}\u0026#34;, \u0026#34;env\u0026#34;: {}, \u0026#34;args\u0026#34;: [\u0026#34;-e\u0026#34;,\u0026#34;-d\u0026#34;,\u0026#34;\u0026#39;*\u0026#39;\u0026#34;,\u0026#34;-c\u0026#34;, \u0026#34;metricbeat.yml\u0026#34;], \u0026#34;showLog\u0026#34;: true } ] } After 1 hour of searching in the internet we found that starting from go 1.10.x the .debug_frame is missing entirely, so Delve could not be used.\nSolution In order to make debugging with Delve work again we downgraded to go 1.9.7.\nYou can find more information regarding the problem in the following links:\nElastic Discuss question #146817 olang/go #23733 derekparker/delve #860 orbs-network/orbs-network-go #245 Have a nice night!\nP.S. TAKIS R.I.P. ! We will never forget! We had eaten the best gyros in Triandria, Thessaloniki!\n","date":"2018-10-05T22:35:00+03:00","permalink":"https://manios.org/2018/10/05/metricbeat-debug-frame-section-error/","title":"Resolve \"Could not find __debug_frame section in binary\" when trying to Debug Metricbeat with Delve and Go 1.10.4"},{"content":"Today we wanted to parse some json logs which we had in a file using Rsyslog and enrich them with Geolocation information regarding the city and the ISP an IP belongs. We initially tried with Logstash (see relevant [previous blog post]({% post_url 2018-08-14-logstash-geo-maxmind-docker %})) but it was too slow. Thus we decided to try with parsing with Rsyslog. The file, let\u0026rsquo;s call it /var/log/input-geo.json had the following structure and content. (It is the same as in Logstash post):\n1 2 {\u0026#34;name\u0026#34;:\u0026#34;Christos\u0026#34;,\u0026#34;src_ip\u0026#34;:\u0026#34;63.145.248.101\u0026#34;,\u0026#34;age\u0026#34;:12} {\u0026#34;name\u0026#34;:\u0026#34;Nikos\u0026#34;,\u0026#34;src_ip\u0026#34;:\u0026#34;98.158.156.175\u0026#34;,\u0026#34;age\u0026#34;:10} Rsyslog has MaxMind/GeoIP DB lookup (mmdblookup) module which adds information about the geographical location of IP addresses, based on data from the Maxmind GeoLite2 databases. In our case we used:\nGeoLite2 City database (free) GeoIP2 ISP Database (commercial licence) We wanted to parse the JSON file enrich the src_ip field and store the geolocation information to src_geoip. Then we forward the message to Elasticsearch using omelasticsearch: Elasticsearch Output Module. For debugging purposes we also enabled file output. Thus , the configuration (rsyslog.conf) looked like the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 # /etc/rsyslog.conf\tConfiguration file for rsyslog. # #\tFor more information see #\t/usr/share/doc/rsyslog-doc/html/rsyslog_conf.html # # Default logging rules can be found in /etc/rsyslog.d/50-default.conf ################# #### MODULES #### ################# module(load=\u0026#34;imuxsock\u0026#34;) # provides support for local system logging # module(load=\u0026#34;imklog\u0026#34;) # provides kernel logging support module(load=\u0026#34;immark\u0026#34;) # provides --MARK-- message capability # $ModLoad imuxsock # provides support for local system logging # $ModLoad imklog # provides kernel logging support (previously done by rklogd) #$ModLoad immark # provides --MARK-- message capability # provides UDP syslog reception # $ModLoad imudp # $UDPServerRun 514 # provides TCP syslog reception # $ModLoad imtcp # $InputTCPServerRun 514 module(load=\u0026#34;builtin:omfile\u0026#34;) module(load=\u0026#34;mmnormalize\u0026#34;) # parser using liblognorm module(load=\u0026#34;mmjsonparse\u0026#34;) #for parsing CEE-enhanced syslog messages module(load=\u0026#34;imfile\u0026#34;) module(load=\u0026#34;mmdblookup\u0026#34; container=\u0026#34;!src_geo\u0026#34;) ########################### #### GLOBAL DIRECTIVES #### ########################### # # Use traditional timestamp format. # To enable high precision timestamps, comment out the following line. # $ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat # Filter duplicated messages $RepeatedMsgReduction on # # Set the default permissions for all log files. # $FileOwner root $FileGroup root $FileCreateMode 0640 $DirCreateMode 0755 $Umask 0022 $PrivDropToUser syslog $PrivDropToGroup syslog # General globals global(net.enableDNS=\u0026#34;off\u0026#34;) # Remove Control Chars global(parser.escapeControlCharactersOnReceive=\u0026#34;off\u0026#34; ) # # Where to place spool files # $WorkDirectory /var/spool/rsyslog global(workDirectory=\u0026#34;/var/spool/rsyslog\u0026#34;) # # Include all config files in /etc/rsyslog.d/ # # $IncludeConfig /etc/rsyslog.d/*.conf ################# #### Inputs #### ################# # provides UDP syslog reception module(load=\u0026#34;imudp\u0026#34;) input(type=\u0026#34;imudp\u0026#34; port=\u0026#34;514\u0026#34;) # provides TCP syslog reception module(load=\u0026#34;imtcp\u0026#34;) input(type=\u0026#34;imtcp\u0026#34; port=\u0026#34;514\u0026#34; ) # File 1 input(type=\u0026#34;imfile\u0026#34; File=\u0026#34;/opt/input-geo.json\u0026#34; Tag=\u0026#34;geoip\u0026#34; PersistStateInterval=\u0026#34;1\u0026#34; freshStartTail=\u0026#34;off\u0026#34;) ################# ### Templates ### ################# # this is for formatting our syslog in JSON with @timestamp for Json messages template(name=\u0026#34;geoip\u0026#34; type=\u0026#34;list\u0026#34;) { constant(value=\u0026#34;{\u0026#34;) constant(value=\u0026#34;\\\u0026#34;@version\\\u0026#34;:\\\u0026#34;1\u0026#34;) constant(value=\u0026#34;\\\u0026#34;,\\\u0026#34;@timestamp\\\u0026#34;:\\\u0026#34;\u0026#34;)\tproperty(name=\u0026#34;timegenerated\u0026#34; dateFormat=\u0026#34;rfc3339\u0026#34;) constant(value=\u0026#34;\\\u0026#34;,\\\u0026#34;host\\\u0026#34;:\\\u0026#34;\u0026#34;)\tproperty(name=\u0026#34;hostname\u0026#34;) constant(value=\u0026#34;\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;syslog\u0026#34;) constant(value=\u0026#34;\\\u0026#34;,\\\u0026#34;syslog_timestamp\\\u0026#34;:\\\u0026#34;\u0026#34;)\tproperty(name=\u0026#34;timereported\u0026#34; dateFormat=\u0026#34;rfc3164\u0026#34; format=\u0026#34;json\u0026#34;) constant(value=\u0026#34;\\\u0026#34;,\\\u0026#34;syslog_hostname\\\u0026#34;:\\\u0026#34;\u0026#34;)\tproperty(name=\u0026#34;hostname\u0026#34; format=\u0026#34;json\u0026#34;) constant(value=\u0026#34;\\\u0026#34;,\\\u0026#34;syslog_program\\\u0026#34;:\\\u0026#34;\u0026#34;)\tproperty(name=\u0026#34;programname\u0026#34; format=\u0026#34;json\u0026#34;) constant(value=\u0026#34;\\\u0026#34;,\\\u0026#34;syslog_message\\\u0026#34;:\\\u0026#34;\u0026#34;)\tproperty(name=\u0026#34;msg\u0026#34; format=\u0026#34;json\u0026#34;) constant(value=\u0026#34;\\\u0026#34;,\\\u0026#34;received_at\\\u0026#34;:\\\u0026#34;\u0026#34;)\tproperty(name=\u0026#34;timegenerated\u0026#34; dateFormat=\u0026#34;rfc3339\u0026#34;) constant(value=\u0026#34;\\\u0026#34;,\\\u0026#34;received_from\\\u0026#34;:\\\u0026#34;\u0026#34;)\tproperty(name=\u0026#34;fromhost\u0026#34; format=\u0026#34;json\u0026#34;) constant(value=\u0026#34;\\\u0026#34;,\\\u0026#34;path\\\u0026#34;:\\\u0026#34;\u0026#34;) property(name=\u0026#34;$!metadata!filename\u0026#34;) constant(value=\u0026#34;\\\u0026#34;,\\\u0026#34;name\\\u0026#34;:\\\u0026#34;\u0026#34;) property(name=\u0026#34;$!name\u0026#34; format=\u0026#34;json\u0026#34;) constant(value=\u0026#34;\\\u0026#34;,\\\u0026#34;src_ip\\\u0026#34;:\\\u0026#34;\u0026#34;) property(name=\u0026#34;$!src_ip\u0026#34; format=\u0026#34;json\u0026#34;) constant(value=\u0026#34;\\\u0026#34;,\\\u0026#34;src_geoip\\\u0026#34;:{\u0026#34;) property(name=\u0026#34;$!src_geo\u0026#34; position.from=\u0026#34;2\u0026#34;) constant(value=\u0026#34;}\\n\u0026#34;) } ################# #### Actions #### ################# ###Fix text to utf8, disabled for now ###action(type=\u0026#34;mmutf8fix\u0026#34;) #action(type=\u0026#34;omfile\u0026#34; file=\u0026#34;/tmp/logtesting\u0026#34; template=\u0026#34;RSYSLOG_DebugFormat\u0026#34;) if ($syslogtag contains \u0026#39;geoip\u0026#39;) then { action(type=\u0026#34;mmjsonparse\u0026#34; cookie=\u0026#34;\u0026#34;) if $parsesuccess == \u0026#34;OK\u0026#34; then { # https://github.com/rsyslog/rsyslog/issues/1650 # Add MaxMind/GeoIP DB lookup information action( type=\u0026#34;mmdblookup\u0026#34; mmdbfile=\u0026#34;/etc/rsyslog.d/GeoLite2-City.mmdb\u0026#34; key=\u0026#34;!src_ip\u0026#34; fields=[\u0026#34;:timezone:!location!time_zone\u0026#34;, \u0026#34;:latitude:!location!latitude\u0026#34;, \u0026#34;:longitude:!location!longitude\u0026#34;, \u0026#34;:dma_code:!location!metro_code\u0026#34;, \u0026#34;:city_name:!names!en\u0026#34;, \u0026#34;:continent_code:!continent!code\u0026#34;, \u0026#34;:country_code2:!country!iso_code\u0026#34;, \u0026#34;:country_code3:!country!iso_code\u0026#34;, \u0026#34;:country_name:!country!names!en\u0026#34;, \u0026#34;:postal_code:!postal!code\u0026#34;, \u0026#34;:region_code:!subdivisions!iso_code\u0026#34;, \u0026#34;:region_name:!subdivisions!names!en\u0026#34; ] ) # Add MaxMind/GeoIP ISP DB lookup information action( type=\u0026#34;mmdblookup\u0026#34; mmdbfile=\u0026#34;/etc/rsyslog.d/GeoIP2-ISP.mmdb\u0026#34; key=\u0026#34;!src_ip\u0026#34; fields=[\u0026#34;:asn:!autonomous_system_number\u0026#34;, \u0026#34;:as_org:!autonomous_system_organization\u0026#34;, \u0026#34;!isp\u0026#34;, \u0026#34;!organization\u0026#34; ] ) # Add IP to src_geo object set $!src_geo!ip = $!src_ip; # If geolocation was successful, add lat,lon in a special location object if $! contains \u0026#39;latitude\u0026#39; then { set $!src_geo!location!lat = $!src_geo!latitude; set $!src_geo!location!lon = $!src_geo!longitude; } # Output to a new file action(type=\u0026#34;omfile\u0026#34; File=\u0026#34;/tmp/json-output\u0026#34; template=\u0026#34;geoip\u0026#34;) # Output to Elasticsearch action(type=\u0026#34;omelasticsearch\u0026#34; server=\u0026#34;192.168.1.10\u0026#34; serverport=\u0026#34;9200\u0026#34; template=\u0026#34;geoip\u0026#34; searchIndex=\u0026#34;geoindex\u0026#34; dynSearchIndex=\u0026#34;on\u0026#34; searchType=\u0026#34;syslog\u0026#34; bulkmode=\u0026#34;on\u0026#34; # use the Bulk API queue.dequeuebatchsize=\u0026#34;5000\u0026#34; # ES bulk size queue.size=\u0026#34;100000\u0026#34; # capacity of the action queue queue.workerthreads=\u0026#34;5\u0026#34; # 5 workers for the action action.resumeretrycount=\u0026#34;-1\u0026#34; # retry indefinitely if ES is unreachable errorfile=\u0026#34;/var/log/omelasticsearch.log\u0026#34; ) } else if $parsesuccess == \u0026#34;FAIL\u0026#34; then { action(type=\u0026#34;omfile\u0026#34; File=\u0026#34;/tmp/json-parse-failure\u0026#34;) } } To run Logstash we chose the quickest way, hence run it in Docker , so we have put all required Logstash configuration, logs and Maxmind databases in a directory:\n1 2 3 4 5 linux@linux-VM:~$ ls -l -rwxrwxr-x 1 linux linux 26331174 Aug 6 19:11 GeoIP2-ISP.mmdb -rwxrwxr-x 1 linux linux 51469823 Aug 6 19:11 GeoLite2-City.mmdb -rw-rw-r-- 1 linux linux 107 Aug 18 19:13 input-geo.json -rwxrwxr-x 1 linux linux 2244 Aug 18 19:32 rsyslog.conf Running Rsyslog on Docker is not so relatively easy as the Docker images are a work in progress in Github (last checked 2018-08-18). So we created a custom image in our personal account, called manios/rsyslog. Let\u0026rsquo;s run a Docker container with Rsyslog 8.37.0:\n1 2 3 4 5 6 7 8 docker run \\ -d \\ --name myrsyslog \\ -v $(pwd)/GeoLite2-City.mmdb:/etc/rsyslog.d/GeoLite2-City.mmdb \\ -v $(pwd)/GeoIP2-ISP.mmdb:/etc/rsyslog.d/GeoIP2-ISP.mmdb \\ -v $(pwd)/input-geo.json:/opt/input-geo.json \\ -v $(pwd)/rsyslog.conf:/etc/rsyslog.conf \\ manios/rsyslog:8.37.0 Now Rsyslog is running. Let\u0026rsquo;s run a Bash shell inside the container:\n1 docker exec -it myrsyslog /bin/bash While Rsyslog is running, if you examine the contents of /tmp/json-output file, you will notice that the messages contain a lot of geolocation information and they will resemble to the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 { \u0026#34;@version\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;@timestamp\u0026#34;: \u0026#34;2018-08-18T06:59:14.640903+00:00\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;b576d0a6022b\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;syslog\u0026#34;, \u0026#34;syslog_timestamp\u0026#34;: \u0026#34;Aug 18 06:59:14\u0026#34;, \u0026#34;syslog_hostname\u0026#34;: \u0026#34;b576d0a6022b\u0026#34;, \u0026#34;syslog_program\u0026#34;: \u0026#34;geoip\u0026#34;, \u0026#34;syslog_message\u0026#34;: \u0026#34;{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;Christos\\\u0026#34;,\\\u0026#34;src_ip\\\u0026#34;:\\\u0026#34;63.145.248.101\\\u0026#34;,\\\u0026#34;age\\\u0026#34;:12}\u0026#34;, \u0026#34;received_at\u0026#34;: \u0026#34;2018-08-18T06:59:14.640903+00:00\u0026#34;, \u0026#34;received_from\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/opt/input-geo.json\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Christos\u0026#34;, \u0026#34;src_ip\u0026#34;: \u0026#34;63.145.248.101\u0026#34;, \u0026#34;src_geoip\u0026#34;: { \u0026#34;timezone\u0026#34;: \u0026#34;America\\/Los_Angeles\u0026#34;, \u0026#34;latitude\u0026#34;: 37.925500, \u0026#34;longitude\u0026#34;: -122.343700, \u0026#34;dma_code\u0026#34;: 807, \u0026#34;city_name\u0026#34;: null, \u0026#34;continent_code\u0026#34;: \u0026#34;NA\u0026#34;, \u0026#34;country_code2\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;country_code3\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;country_name\u0026#34;: \u0026#34;UnitedStates\u0026#34;, \u0026#34;postal_code\u0026#34;: \u0026#34;94804\u0026#34;, \u0026#34;region_code\u0026#34;: null, \u0026#34;region_name\u0026#34;: null, \u0026#34;asn\u0026#34;: 209, \u0026#34;as_org\u0026#34;: \u0026#34;QwestCommunicationsCompany,LLC\u0026#34;, \u0026#34;isp\u0026#34;: \u0026#34;CenturyLink\u0026#34;, \u0026#34;organization\u0026#34;: \u0026#34;CenturyLink\u0026#34;, \u0026#34;ip\u0026#34;: \u0026#34;63.145.248.101\u0026#34;, \u0026#34;location\u0026#34;: { \u0026#34;lat\u0026#34;: 37.9255, \u0026#34;lon\u0026#34;: -122.3437 } } } Unfortunately, current version of Rsyslog contains a known bug (#1650) which strips the space characters from strings. Notice that\n1 \u0026#34;country_name\u0026#34;: \u0026#34;UnitedStates\u0026#34; should be\n1 \u0026#34;country_name\u0026#34;: \u0026#34;United States\u0026#34; for example. Also, if you compare to our previous post (TODO add link here), region_code and region_name should have a value. You can easily spot the differences between Rsyslog and Logstash output in the following image:\nWe hope this article helped you get up and running with Rsyslog and the use of MaxMind/GeoIP DB lookup (mmdblookup) module!\n","date":"2018-08-18T10:13:44+03:00","permalink":"https://manios.org/2018/08/18/logstash-geoip-json-logs-maxmint-geolite-docker/","title":"Rsyslog - Parse Json and enrich IP with Geolocation using Maxmind GeoLite2 City and ISP"},{"content":"Today we wanted to parse some json logs which we had in a file using Logstash and enrich them with Geolocation information regarding the city and the ISP an IP belongs. The file, (let\u0026rsquo;s call it /var/log/input-geo.json) had the following structure:\n1 2 {\u0026#34;name\u0026#34;:\u0026#34;Christos\u0026#34;,\u0026#34;src_ip\u0026#34;:\u0026#34;63.145.248.101\u0026#34;,\u0026#34;age\u0026#34;:12} {\u0026#34;name\u0026#34;:\u0026#34;Nikos\u0026#34;,\u0026#34;src_ip\u0026#34;:\u0026#34;98.158.156.175\u0026#34;,\u0026#34;age\u0026#34;:10} Logstash has GeoIP filter which adds information about the geographical location of IP addresses, based on data from the Maxmind GeoLite2 databases. In our case we used:\nGeoLite2 City database (free) GeoIP2 ISP Database (commercial licence) We wanted to parse the JSON file enrich the src_ip and then forward it to Elasticsearch. For debugging purposes we also enabled file output. Thus , the configuration (logstash.conf) looked like the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 input { file { path =\u0026gt; [\u0026#34;/var/log/input-geo.json\u0026#34;] sincedb_path =\u0026gt; \u0026#34;/usr/share/logstash/geo-sincedb1\u0026#34; start_position =\u0026gt; \u0026#34;beginning\u0026#34; codec =\u0026gt; \u0026#34;json\u0026#34; type =\u0026gt; \u0026#34;mygeo\u0026#34; } } filter { # Enhance with City geolocation information using free GeoLite2 City database if [src_ip] { geoip { source =\u0026gt; \u0026#34;src_ip\u0026#34; target =\u0026gt; \u0026#34;src_geoip\u0026#34; database =\u0026gt; \u0026#34;/usr/share/logstash/GeoLite2-City.mmdb\u0026#34; } } # Enhance with ISP geolocation information using free GeoIP2 ISP Database if [src_ip] { geoip { source =\u0026gt; \u0026#34;src_ip\u0026#34; target =\u0026gt; \u0026#34;src_geoip\u0026#34; database =\u0026gt; \u0026#34;/usr/share/logstash/GeoIP2-ISP.mmdb\u0026#34; } } output { # stdout { codec =\u0026gt; rubydebug } # Output successfull messages to a file if \u0026#34;_grokparsefailure\u0026#34; not in [tags] { file { path =\u0026gt; \u0026#34;/usr/share/logstash/success-debug.txt\u0026#34; codec =\u0026gt; json_lines # codec =\u0026gt; rubydebug } } # Output failed messages to a file if \u0026#34;_grokparsefailure\u0026#34; in [tags] { file { path =\u0026gt; \u0026#34;/usr/share/logstash/failed-debug.txt\u0026#34; codec =\u0026gt; rubydebug } } # Output successfull messages to Elasticsearch elasticsearch { hosts =\u0026gt; [\u0026#34;192.168.1.3:9200\u0026#34;] index =\u0026gt; \u0026#34;json_logs-%{+YYYY.MM.dd}\u0026#34; index =\u0026gt; \u0026#34;192.168.1.3_login_events-%{+YYYY.MM.dd}\u0026#34; manage_template =\u0026gt; true template_name =\u0026gt; \u0026#34;login_events\u0026#34; template_overwrite =\u0026gt; true protocol =\u0026gt; http flush_size =\u0026gt; 512 workers =\u0026gt; 8 } } To run Logstash we chose the quickest way, hence run it in Docker , so we have put all required Logstash configuration, logs and Maxmind databases in a directory:\n1 2 3 4 5 linux@linux-VM:~$ ls -l -rwxrwxr-x 1 linux linux 26331174 Aug 6 19:11 GeoIP2-ISP.mmdb -rwxrwxr-x 1 linux linux 51469823 Aug 6 19:11 GeoLite2-City.mmdb -rw-rw-r-- 1 linux linux 107 Aug 13 19:13 input-geo.json -rwxrwxr-x 1 linux linux 2244 Aug 13 19:32 logstash.conf Running Logstash on Docker is relatively easy as Docker images for are available from the Elastic Docker registry. You can find more information here. Let\u0026rsquo;s run and interactive Docker container with Logstash 6.3.2:\n1 2 3 4 5 6 7 8 docker run -it \\ --rm \\ --name logstash \\ -v $(pwd)/GeoIP2-ISP.mmdb:/usr/share/logstash/GeoIP2-ISP.mmdb \\ -v $(pwd)/GeoLite2-City.mmdb:/usr/share/logstash/GeoLite2-City.mmdb \\ -v $(pwd)/input-geo.json:/var/log/input-geo.json \\ -v $(pwd)/logstash.conf:/usr/share/logstash/pipeline/logstash.conf \\ docker.elastic.co/logstash/logstash-oss:6.3.2 While Logstash is running, if you examine Elasticsearch or /usr/share/logstash/success-debug.txt, you will notice that the messages contain a lot of geolocation information and they will resemble to the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \u0026#34;src_ip\u0026#34;: \u0026#34;63.145.248.101\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/var/log/input-geo.json\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;a14a2394ba12\u0026#34;, \u0026#34;@timestamp\u0026#34;: \u0026#34;2018-08-13T17:33:05.894Z\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;mygeo\u0026#34;, \u0026#34;src_geoip\u0026#34;: { \u0026#34;postal_code\u0026#34;: \u0026#34;94804\u0026#34;, \u0026#34;country_code3\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;dma_code\u0026#34;: 807, \u0026#34;continent_code\u0026#34;: \u0026#34;NA\u0026#34;, \u0026#34;region_name\u0026#34;: \u0026#34;California\u0026#34;, \u0026#34;longitude\u0026#34;: -122.3437, \u0026#34;ip\u0026#34;: \u0026#34;63.145.248.101\u0026#34;, \u0026#34;isp\u0026#34;: \u0026#34;CenturyLink\u0026#34;, \u0026#34;region_code\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;latitude\u0026#34;: 37.9255, \u0026#34;as_org\u0026#34;: \u0026#34;Qwest Communications Company, LLC\u0026#34;, \u0026#34;country_code2\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;asn\u0026#34;: 209, \u0026#34;location\u0026#34;: { \u0026#34;lon\u0026#34;: -122.3437, \u0026#34;lat\u0026#34;: 37.9255 }, \u0026#34;organization\u0026#34;: \u0026#34;CenturyLink\u0026#34;, \u0026#34;city_name\u0026#34;: \u0026#34;Richmond\u0026#34;, \u0026#34;country_name\u0026#34;: \u0026#34;United States\u0026#34;, \u0026#34;timezone\u0026#34;: \u0026#34;America/Los_Angeles\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;Christos\u0026#34;, \u0026#34;age\u0026#34;: 12, \u0026#34;@version\u0026#34;: \u0026#34;1\u0026#34; } Thank God we have a geolocation result! We hope this article helped you get up and running!\n","date":"2018-08-14T18:35:00+03:00","permalink":"https://manios.org/2018/08/14/logstash-geoip-json-logs-maxmint-geolite-docker/","title":"Logstash - Enrich IP with Geolocation using Maxmind GeoLite2 City and ISP"},{"content":"I suspect that you are reading this post after having trouble creating a new Elastic Beat based on Metricbeat. Bud do not worry! We have you covered and solved the issues for you!\nLong story short, a few days ago we wanted to create a new Metricbeat. After reading and executing step by step the official guide \u0026ldquo;Creating a Beat based on Metricbeat\u0026rdquo; we stumpled upon a series of issues like Step 3 - Init and create the metricset failure. After spending 2 days trying to resolve our local dependencies and solve the issues in our development environment, we decided to make everything in a sterilized environment, hence a Docker container.\nTo begin with, we used Go language official Docker image with tag golang:1.10.2-stretch and we run everything in it. So in a shell, we executed:\n1 docker run -it --rm --name go1.10 golang:1.10.2-stretch /bin/bash When the container is started, a new shell is loaded and we can download all the dependencies needed, by executing inside it:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apt-get update \\ \u0026amp;\u0026amp; apt-get install -y wget curl git vim \\ \u0026amp;\u0026amp; echo \u0026#34;set number\u0026#34; | tee -a /etc/vim/vimrc \\ \u0026amp;\u0026amp; echo \u0026#34;syntax on\u0026#34; | tee -a /etc/vim/vimrc \\ \u0026amp;\u0026amp; echo \u0026#34;colorscheme evening\u0026#34; | tee -a /etc/vim/vimrc \\ \u0026amp;\u0026amp; wget https://bootstrap.pypa.io/get-pip.py \\ \u0026amp;\u0026amp; python get-pip.py \\ \u0026amp;\u0026amp; rm get-pip.py \\ \u0026amp;\u0026amp; pip install virtualenv \\ \u0026amp;\u0026amp; echo -n -e \u0026#34;\\n\\nOur environment:\\n===================================\\n\u0026#34; \\ \u0026amp;\u0026amp; go version \\ \u0026amp;\u0026amp; echo -n \u0026#34;Python version: \u0026#34; \\ \u0026amp;\u0026amp; python --version \\ \u0026amp;\u0026amp; echo -n \u0026#34;virtualenv version: \u0026#34; \\ \u0026amp;\u0026amp; virtualenv --version \\ \u0026amp;\u0026amp; echo \u0026#34;===================================\u0026#34; \\ \u0026amp;\u0026amp; echo -n \u0026#34;Fetching metricbeat source code: \u0026#34; \\ \u0026amp;\u0026amp; go get github.com/elastic/beats/metricbeat \\ \u0026amp;\u0026amp; echo \u0026#34;[OK]\u0026#34; \\ \u0026amp;\u0026amp; cd ${GOPATH}/src/github.com/elastic/beats \\ \u0026amp;\u0026amp; echo \u0026#34;Checking out commit 2619e137c1f7eb031fc89132045117a85c7ef818 to new branch in order to avoid bug\u0026#34; \\ \u0026amp;\u0026amp; git checkout -b working 2619e137c1f7eb031fc89132045117a85c7ef818 \\ \u0026amp;\u0026amp; cd $GOPATH \\ \u0026amp;\u0026amp; sed -i \u0026#39;s/@-cp -r module\\/[*]\\/_meta\\/kibana _meta\\//@-cp -r ${GOPATH}\\/src\\/github.com\\/elastic\\/beats\\/metricbeat\\/module\\/*\\/_meta\\/kibana _meta\\//\u0026#39; ${GOPATH}/src/github.com/elastic/beats/metricbeat/Makefile \\ \u0026amp;\u0026amp; echo \u0026#34;Elastic beats environment has been initialised successfully! You can now generate a new Metricbeat!\u0026#34; If everything works as expected you should see something like this in the output:\n1 2 3 4 5 6 7 8 9 10 Our environment: =================================== go version go1.10.2 linux/amd64 Python version: Python 2.7.13 virtualenv version: 16.0.0 =================================== Fetching metricbeat source code: [OK] Checking out commit 2619e137c1f7eb031fc89132045117a85c7ef818 to new branch in order to avoid bug Switched to a new branch \u0026#39;working\u0026#39; Elastic beats environment has been initialised successfully! You can now generate a new Metricbeat! The environment is set up correctly. Let\u0026rsquo;s create a new Metricbeat. So execute:\n1 python ${GOPATH}/src/github.com/elastic/beats/script/generate.py --type=metricbeat and respond to the input as the example:\n1 2 3 4 5 root@1e407b4d73a7:/go# python ${GOPATH}/src/github.com/elastic/beats/script/generate.py --type=metricbeat Beat Name [Examplebeat]: weatherbeat Your Github Name [your-github-name]: manios Beat Path [github.com/manios/weatherbeat]: Firstname Lastname: Christos Manios The newly created Metricbeat will be stored in ${GOPATH}/src/github.com/manios/weatherbeat/.\nNavigate to the Metricbeat directory. Execute the following to avoid weird errors when creating a metricset:\n1 2 cd ${GOPATH}/src/github.com/manios/weatherbeat \\ \u0026amp;\u0026amp; cp -r $GOPATH/src/github.com/elastic/beats/metricbeat/scripts . Now we are ready to create a new Metricset. The metricset module will be called skgtemperatures and the metricset itself hydro. According to the official guide we execute make setup and the output will be the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 mkdir -p vendor/github.com/elastic/ cp -R /go/src/github.com/elastic/beats vendor/github.com/elastic/ rm -rf vendor/github.com/elastic/beats/.git make create-metricset make[1]: Entering directory \u0026#39;/go/src/github.com/manios/weatherbeat\u0026#39; New python executable in /go/src/github.com/manios/weatherbeat/build/python-env/bin/python Installing setuptools, pip, wheel...done. docker-compose 1.22.0 has requirement requests!=2.11.0,!=2.12.2,!=2.18.0,\u0026lt;2.19,\u0026gt;=2.6.1, but you\u0026#39;ll have requests 2.19.1 which is incompatible. # Work around pip bug. See: https://github.com/pypa/pip/issues/4464 find /go/src/github.com/manios/weatherbeat/build/python-env -type d -name \u0026#39;dist-packages\u0026#39; -exec sh -c \u0026#34;echo dist-packages \u0026gt; {}.pth\u0026#34; \u0026#39;;\u0026#39; Module name: skgtemperature Metricset name: hydro Module skgtemperature created. Metricset hydro created. make[1]: Leaving directory \u0026#39;/go/src/github.com/manios/weatherbeat\u0026#39; make collect make[1]: Entering directory \u0026#39;/go/src/github.com/manios/weatherbeat\u0026#39; # Work around pip bug. See: https://github.com/pypa/pip/issues/4464 find /go/src/github.com/manios/weatherbeat/build/python-env -type d -name \u0026#39;dist-packages\u0026#39; -exec sh -c \u0026#34;echo dist-packages \u0026gt; {}.pth\u0026#34; \u0026#39;;\u0026#39; make[1]: Leaving directory \u0026#39;/go/src/github.com/manios/weatherbeat\u0026#39; If everything works as expected then if you execute a tree command, to module directory you will probably have the following structure:\n1 2 3 4 5 6 7 8 9 10 11 12 -- skgtemperature |-- _meta | |-- config.yml | |-- docs.asciidoc | `-- fields.yml |-- doc.go `-- hydro |-- _meta | |-- data.json | |-- docs.asciidoc | `-- fields.yml `-- hydro.go That\u0026rsquo;s it! I hope it helps!\n","date":"2018-07-23T22:31:00+03:00","permalink":"https://manios.org/2018/07/23/create-a-beat-based-on-metricbeat-resolving-bugs/","title":"Creating a Beat based on Metricbeat resolving Init and create metricset failures"},{"content":"Today we wanted to use Tortoise Git Merge, a Windows Git utility which comes with Tortoise Git. It helps us do a 3 way merge and solve our conflicts easily. The advantage of it is that it automatically merges the lines that where uniquely modified in every file and you only need to merge the common changes where the conflicts are. So in our case we had a file , let\u0026rsquo;s call it README.md, in which we had 1000 changed lines out of which only 2 had a conflict with the file pulled from upstream server. Eclipse and IntelliJ do provide a merge editor but as far as we are concerned they are not automatically merging your unique changes (please correct us if we are wrong). After some searching on the web of how to do so, we found StackOverflow Question 5190188 : Why can\u0026rsquo;t I use TortoiseMerge as my git merge tool on Windows? which explains different approaches.\nSo according to the aforementioned question and our experiments, in order to use Tortoise Git Merge you have to download and install Tortoise Git and configure your local .gitconfig file with the following lines:\n1 2 3 4 5 6 7 [core] longpaths = true autocrlf = true [merge] tool=tortoisemerge [mergetool \u0026#34;tortoisemerge\u0026#34;] cmd = \\\u0026#34;C:\\\\Program Files\\\\TortoiseGit\\\\bin\\\\TortoiseGitMerge.exe\\\u0026#34; -base:\\\u0026#34;$BASE\\\u0026#34; -theirs:\\\u0026#34;$LOCAL\\\u0026#34; -mine:\\\u0026#34;$REMOTE\\\u0026#34; -merged:\\\u0026#34;$MERGED\\\u0026#34; If you want an one liner equivalent command, you can run the following:\n1 git config --global mergetool.tortoisemerge.cmd \u0026#34;\\\u0026#34;C:\\\\Program Files\\\\TortoiseGit\\\\bin\\\\TortoiseGitMerge.exe\\\u0026#34; -base:\\\u0026#34;$BASE\\\u0026#34; -theirs:\\\u0026#34;$LOCAL\\\u0026#34; -mine:\\\u0026#34;$REMOTE\\\u0026#34; -merged:\\\u0026#34;$MERGED\\\u0026#34;\u0026#34; NOTE: The core section is complimentary for Windows in order to help Git use long paths and overcome Windows 260 character MAX_PATH limitation. Furthermore we need to automatically checkout with file ending using CRLF and push to LF, thus the autocrlf is set to true.\nAfter this short parenthesis, back in action! When you pull from a remote repository, or you merge, or rebase a branch and Git complains about conflicts you can use in your shell or in Git Bash:\n1 git mergetool Tortoise merge will appear with a display like the following:\nIn this image, conflicts are marked with red and the automatically merged lines are marked with yellow. Left side (Theirs) is the version of the file which comes from repository and right side (Mine) is your local version with your changes.\nThat\u0026rsquo;s it. We hope this quick guide helped you!\n","date":"2018-05-30T20:42:00+03:00","permalink":"https://manios.org/2018/05/30/git-merge-conflicts-using-tortoise-git-merge-windows/","title":"Git merge conflicts using Tortoise Git merge in Windows"},{"content":"Today I wanted to debug in Java a JAX-WS SOAP web service client and see the exact message that is send as an HTTP request. So after searching, I found in this StackOverflow question that if we insert the following lines before calling the service:\n1 2 3 4 System.setProperty(\u0026#34;com.sun.xml.ws.transport.http.client.HttpTransportPipe.dump\u0026#34;, \u0026#34;true\u0026#34;); System.setProperty(\u0026#34;com.sun.xml.internal.ws.transport.http.client.HttpTransportPipe.dump\u0026#34;, \u0026#34;true\u0026#34;); System.setProperty(\u0026#34;com.sun.xml.ws.transport.http.HttpAdapter.dump\u0026#34;, \u0026#34;true\u0026#34;); System.setProperty(\u0026#34;com.sun.xml.internal.ws.transport.http.HttpAdapter.dump\u0026#34;, \u0026#34;true\u0026#34;); when we call the service it will produce the following output in logs (or console output):\n1 2 3 4 5 6 ---[HTTP request - http://localhost:9090/ContactRequestServicePort]--- Accept: text/xml, multipart/related Content-Type: text/xml; charset=utf-8 SOAPAction: \u0026#34;newContact\u0026#34; User-Agent: JAX-WS RI 2.2.9-b130926.1035 svn-revision#5f6196f2b90e9460065a4c2f4e30e065b245e51e \u0026lt;?xml version=\u0026#34;1.0\u0026#34; ?\u0026gt;\u0026lt;S:Envelope xmlns:S=\u0026#34;http://schemas.xmlsoap.org/soap/envelope/\u0026#34;\u0026gt;\u0026lt;S:Body\u0026gt;\u0026lt;ns2:newContact xmlns:ns4=\u0026#34;http://www.acmecorp.com/contacts/Exception\u0026#34; xmlns:ns3=\u0026#34;http://www.acmecorp.com/contacts/RequestMetaInfo\u0026#34; xmlns:ns2=\u0026#34;http://www.acmecorp.com/contacts/crm-contact/Service\u0026#34;\u0026gt;\u0026lt;ns2:contact\u0026gt;\u0026lt;ns2:prefix\u0026gt;Mr\u0026lt;/ns2:prefix\u0026gt;\u0026lt;ns2:name\u0026gt;John\u0026lt;/ns2:name\u0026gt;\u0026lt;ns2:surname\u0026gt;Longjohn\u0026lt;/ns2:surname\u0026gt;\u0026lt;/ns2:contact\u0026gt;\u0026lt;/ns2:newContact\u0026gt;\u0026lt;/S:Body\u0026gt;\u0026lt;/S:Envelope\u0026gt;-------------------- It works! Now we have the entire HTTP request and we can pretty print the XML which is included in the message body of HTTP request to see what we send to the web service:\n1 2 3 4 5 6 7 8 9 10 11 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; ?\u0026gt;\u0026lt;S:Envelope xmlns:S=\u0026#34;http://schemas.xmlsoap.org/soap/envelope/\u0026#34;\u0026gt; \u0026lt;S:Body\u0026gt; \u0026lt;ns2:newContact xmlns:ns4=\u0026#34;http://www.acmecorp.com/contacts/Exception\u0026#34; xmlns:ns3=\u0026#34;http://www.acmecorp.com/contacts/RequestMetaInfo\u0026#34; xmlns:ns2=\u0026#34;http://www.acmecorp.com/contacts/crm-contact/Service\u0026#34;\u0026gt; \u0026lt;ns2:contact\u0026gt; \u0026lt;ns2:prefix\u0026gt;Mr\u0026lt;/ns2:prefix\u0026gt; \u0026lt;ns2:name\u0026gt;John\u0026lt;/ns2:name\u0026gt; \u0026lt;ns2:surname\u0026gt;Longjohn\u0026lt;/ns2:surname\u0026gt; \u0026lt;/ns2:contact\u0026gt; \u0026lt;/ns2:newContact\u0026gt; \u0026lt;/S:Body\u0026gt; \u0026lt;/S:Envelope\u0026gt; ","date":"2018-04-03T19:42:00+03:00","permalink":"https://manios.org/2018/04/03/java-soap-service-client-debug-http-message/","title":"Debug HTTP request of Java SOAP JAX-WS web service client"},{"content":"Today I had to create a Java client for a SOAP web service. So I opened Netbeans IDE and used the great tool which generates JAX-WS client code using the respective service .wsdl file. However, the service needed to be called using Basic Http Authentication.\nNOTE: The solution provided works if the WSDL is not protected itself by Basic Http Authentication. The Basic Authentication is added as a header in the HTTP request.\nAfter a lot of searching I found that (StackOverflow question) this is possible:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 ContactCreationRequestService_Service srv = new ContactCreationRequestService_Service(); ContactCreationRequestService porta ; // Get service port try { porta = srv.getContactCreationRequestServicePort(); }catch (Exception e){ logger.error(e); return; } // Add username and password for Basic Authentication Map\u0026lt;String, Object\u0026gt; reqContext = ((BindingProvider) porta).getRequestContext(); reqContext.put(BindingProvider.USERNAME_PROPERTY, \u0026#34;username\u0026#34;); reqContext.put(BindingProvider.PASSWORD_PROPERTY, \u0026#34;password\u0026#34;); // Create a contact request ContactRequest ka = new ContactRequest(); ka.setPrefix(\u0026#34;Mr\u0026#34;); ka.setName(\u0026#34;Christos\u0026#34;); ka.setSurname(\u0026#34;Manios\u0026#34;); NewContact nka = new NewContact(); nka.setContactRequest(ka); // Call the web service try { porta.newContactCreationRequest(nka); } catch (SoapServiceException e) { logger.error(e); } If we enable debugging to see the actual HTTP request, then the logs will show output like the following:\n1 2 3 4 5 6 7 ---[HTTP request - http://localhost:9090/ContactRequestServicePort]--- Accept: text/xml, multipart/related Authorization: Basic dXNlcm5hbWU6cGFzc3dvcmQ= Content-Type: text/xml; charset=utf-8 SOAPAction: \u0026#34;newContact\u0026#34; User-Agent: JAX-WS RI 2.2.9-b130926.1035 svn-revision#5f6196f2b90e9460065a4c2f4e30e065b245e51e \u0026lt;?xml version=\u0026#34;1.0\u0026#34; ?\u0026gt;\u0026lt;S:Envelope xmlns:S=\u0026#34;http://schemas.xmlsoap.org/soap/envelope/\u0026#34;\u0026gt;\u0026lt;S:Body\u0026gt;\u0026lt;ns2:newContact xmlns:ns4=\u0026#34;http://www.acmecorp.com/contacts/Exception\u0026#34; xmlns:ns3=\u0026#34;http://www.acmecorp.com/contacts/RequestMetaInfo\u0026#34; xmlns:ns2=\u0026#34;http://www.acmecorp.com/contacts/crm-contact/Service\u0026#34;\u0026gt;\u0026lt;ns2:contact\u0026gt;\u0026lt;ns2:prefix\u0026gt;Mr\u0026lt;/ns2:prefix\u0026gt;\u0026lt;ns2:name\u0026gt;John\u0026lt;/ns2:name\u0026gt;\u0026lt;ns2:surname\u0026gt;Longjohn\u0026lt;/ns2:surname\u0026gt;\u0026lt;/ns2:contact\u0026gt;\u0026lt;/ns2:newContact\u0026gt;\u0026lt;/S:Body\u0026gt;\u0026lt;/S:Envelope\u0026gt;-------------------- You can observe that the request contains the Authorization header as expected:\n1 Authorization: Basic dXNlcm5hbWU6cGFzc3dvcmQ= ","date":"2018-03-30T17:42:00+03:00","permalink":"https://manios.org/2018/03/30/java-soap-jax-ws-client-basic-auth/","title":"HTTP Basic Authentication in Java SOAP JAX-WS web service client"},{"content":"If you are reading this article, perhaps you have searched a lot about NTP server installation in Linux, issues and problems encountered and how to troubleshoot them! We know that there are so many articles about how to configure NTP servers but we could not find any which contains all the issues we faced in the same place!\nOur story A client of ours has a sever infrastructure where from the 40+ VMs in the tenant environment, only 3 are allowed to communicate with external NTP servers. Thus we had to install NTP servers in 3 Linux virtual machines in order to synchronise their clocks with the external pools and in the meantime act as internal environment primary NTP servers. The following diagram (kudos to AsciiFlow) summarizes the installation:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 +---------------------+ | External NTP | |1.europe.pool.ntp.org| +----------+----------+ ^ | | +---------------------------------------+ | | | | | | +-------+--------+ +--------+-------+ +-------+--------+ | VM1 | | VM2 | | VM3 | | 192.168.10.1 | | 192.168.10.2 | | 192.168.10.3 | | NTP srv1 | | NTP srv2 | | NTP srv3 | +-------+--------+ +--------+-------+ +-----+----------+ ^ ^ ^ | | | +-------------------------------------+ | +-------+--------+ | VM4 (any) | | 192.168.10.4 | | NTP client | +----------------+ Notes on diagram:\nExternal NTP is one of the many NTP pools in the internet. VM1, VM2 and VM3 are the Linux virtual machines which are allowed to connect to the internet using port 123 and synchronise their clocks with the external pool. VM4 is any virtual machine (Linux or Windows) in the infrastructure which cannot communicate with external NTP servers, thus it will use VM1, VM2 and VM3 to update its clock. IMPORTANT NOTES:\nIf you are a developer or a devops person and you have to communicate with network admins for any network issues you might encounter, please be meticulous and describe the problems in detail. During this operation (yes, debugging was like war!), the majority of our problems were network related so we had to debug every step and inform network administrators in order to configure firewalls. If your infrastructure is completely blocked by a firewall and you cannot access UDP port 123 (ntp) in any way, then you might consider using htpdate in VM1, VM2, VM3. Install NTP Server in Debian Linux So, to install NTP server in Debian Linux, you just execute:\n1 sudo apt-get install ntp in VM1, VM2, VM3 and VM4 . After it is installed, our efforts are focused on one single file: /etc/ntp.conf.\nConfigure NTP Server As we mentioned, we open /etc/ntp.conf and edit the pools in VM1, VM2, VM3 according to our needs:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 # /etc/ntp.conf, configuration for ntpd; see ntp.conf(5) for help driftfile /var/lib/ntp/ntp.drift # Enable this if you want statistics to be logged. #statsdir /var/log/ntpstats/ statistics loopstats peerstats clockstats filegen loopstats file loopstats type day enable filegen peerstats file peerstats type day enable filegen clockstats file clockstats type day enable # You do need to talk to an NTP server or two (or three). #server ntp.your-provider.example # pool.ntp.org maps to about 1000 low-stratum NTP servers. Your server will # pick a different set every time it starts up. Please consider joining the # pool: \u0026lt;http://www.pool.ntp.org/join.html\u0026gt; server 3.gr.pool.ntp.org iburst minpoll 6 maxpoll 8 server 1.europe.pool.ntp.org iburst minpoll 6 maxpoll 8 server 2.europe.pool.ntp.org iburst minpoll 6 maxpoll 8 # Undisciplined Local Clock. This is a fake driver intended for backup # and when no outside source of synchronized time is available. server 127.127.1.0 # local clock fudge 127.127.1.0 stratum 8 # Access control configuration; see /usr/share/doc/ntp-doc/html/accopt.html for # details. The web page \u0026lt;http://support.ntp.org/bin/view/Support/AccessRestrictions\u0026gt; # might also be helpful. # # Note that \u0026#34;restrict\u0026#34; applies to both servers and clients, so a configuration # that might be intended to block requests from certain clients could also end # up blocking replies from your own upstream servers. # By default, exchange time with everybody, but don\u0026#39;t allow configuration. restrict -4 default kod notrap nomodify nopeer noquery limited restrict -6 default kod notrap nomodify nopeer noquery limited # Local users may interrogate the ntp server more closely. #restrict 127.0.0.1 #restrict ::1 # Needed for adding pool entries restrict source notrap nomodify noquery # Clients from this (example!) subnet have unlimited access, but only if # cryptographically authenticated. #restrict 192.168.123.0 mask 255.255.255.0 notrust # If you want to provide time to your local subnet, change the next line. # (Again, the address is an example only.) #broadcast 192.168.123.255 # If you want to listen to time broadcasts on your local subnet, de-comment the # next lines. Please do this only if you trust everybody on the network! #disable auth #broadcastclient NOTES:\nWe have put minpoll to 6 (2^6 = 64 sec) and maxpoll to 8 (2^8 = 256 sec) as we noticed that our local NTP servers tended to drift for minutes. Sometimes, +4 minutes in a day! You can find more information about minpoll and maxpoll here.\nThe following lines:\n1 2 server 127.127.1.0 # local clock fudge 127.127.1.0 stratum 8 are used in order to instruct the local server to act as a low stratum server and be preferred from the local clients. (more information : here)\nConfigure Linux NTP Client We edit /etc/ntp.conf of VM4 and replace any external pools with our local NTP servers (VM1, VM2, VM3). Your configuration might look like the following:\n1 2 3 4 5 6 7 8 9 10 # pool.ntp.org maps to about 1000 low-stratum NTP servers. Your server will # pick a different set every time it starts up. Please consider joining the # pool: \u0026lt;http://www.pool.ntp.org/join.html\u0026gt; #pool 0.debian.pool.ntp.org iburst #pool 1.debian.pool.ntp.org iburst #pool 2.debian.pool.ntp.org iburst #pool 3.debian.pool.ntp.org iburst server 192.168.10.1 iburst minpoll 6 maxpoll 8 server 192.168.10.2 iburst minpoll 6 maxpoll 8 server 192.168.10.3 iburst minpoll 6 maxpoll 8 Again as you can you may notice we have put minpoll to 6 (2^6 = 64 sec) and maxpoll to 8 (2^8 = 256 sec) as we noticed that our local NTP servers tended to drift for minutes. Sometimes, +4 minutes in a day! You can find more information about minpoll and maxpoll here.\nConfigure Windows NTP Client Our virtual machines use Windows Server 2012 R2 (for Windows 2003 and older see here). We followed the instructions from pool.ntp.org and this article. So we open a windows command line prompt with administrative privileges (WinKey + R + type \u0026ldquo;cmd\u0026rdquo; + hit enter key) and run :\n1 w32tm /config /syncfromflags:manual /manualpeerlist:\u0026#34;192.168.10.1 192.168.10.2 192.168.10.3\u0026#34; and then restart the time service so changes take effect and force it to resync:\n1 2 net stop W32Time net start W32Time Troubleshooting This is going to be the longest of all sections! Yes! Debugging was hard, we dare say! For our debugging purposes in Linux VMs we used ntpdate tool at first, although is deprecated. Also we have used ntpq and nmap. So this is what you have to do if you encounter the following errors both in your local NTP servers and clients.\nNTP Server dropped: strata too high If you see in syslog or in the output of the command:\n1 sudo ntpdate -dv 2.europe.pool.ntp.org the error:\n1 91.228.108.200: Server dropped: strata too high then, acccording to this and this article, the server is too far out of sync with the upstream servers, so it sets an artificially high stratum value to prevent other computers trusting it.\nA possible solution is to set the time in the server manually as described here and then restart ntp service to see what happens.\nNo server suitable for synchronization found If you use ntpdate to debug like this:\nWARNING! NTP service has to be stopped in order to perform the following test. {: .notice\u0026ndash;warning}\n1 sudo ntpdate -s -B -v 3.gr.pool.ntp.org and the output in syslog is like this:\n1 2 3 bobos@WEBSRV01:~$ sudo tail -f -n 10 /var/log/syslog Nov 24 13:33:39 WEBWEBSRV01 ntpdate[26635]: ntpdate 4.2.8p10@1.3728-o Sat Nov 24 19:02:40 UTC 2017 (1) Nov 24 13:33:48 WEBSRV01 ntpdate[26635]: no server suitable for synchronization found then it might a network issue. To be sure, run nmap :\n1 2 3 4 5 6 7 bobos@WEBSRV01:~$ sudo nmap -p123 -sU -P0 3.gr.pool.ntp.org Starting Nmap 7.40 ( https://nmap.org ) at 2017-11-24 13:43 EEST Nmap scan report for 3.gr.pool.ntp.org (194.177.210.54) Host is up (0.00011s latency). Other addresses for 3.gr.pool.ntp.org (not scanned): ::1 PORT STATE SERVICE 123/udp closed ntp As you can see UTDP port 123 is closed, so we cannot communicate. Let\u0026rsquo;s run a second test:\nWARNING! NTP service has to be stopped in order to perform the following test. {: .notice\u0026ndash;warning}\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 bobos@WEBSRV01:~$ sudo ntpdate -dv 2.europe.pool.ntp.org 24 Oct 12:02:20 ntpdate[26399]: ntpdate 4.2.8p10@1.3728-o Sat Sep 23 19:02:40 UTC 2017 (1) transmit(87.118.124.35) transmit(91.228.108.200) transmit(194.177.4.1) transmit(81.16.38.161) transmit(87.118.124.35) transmit(91.228.108.200) transmit(194.177.4.1) transmit(81.16.38.161) transmit(87.118.124.35) transmit(91.228.108.200) transmit(194.177.4.1) transmit(81.16.38.161) transmit(87.118.124.35) transmit(91.228.108.200) transmit(194.177.4.1) transmit(81.16.38.161) transmit(87.118.124.35) transmit(91.228.108.200) transmit(194.177.4.1) transmit(81.16.38.161) 87.118.124.35: Server dropped: no data 91.228.108.200: Server dropped: no data 194.177.4.1: Server dropped: no data 81.16.38.161: Server dropped: no data server 87.118.124.35, port 123 stratum 0, precision 0, leap 00, trust 000 refid [87.118.124.35], delay 0.00000, dispersion 64.00000 transmitted 4, in filter 4 reference time: 00000000.00000000 Thu, Feb 7 2036 8:28:16.000 originate timestamp: 00000000.00000000 Thu, Feb 7 2036 8:28:16.000 transmit timestamp: dd998022.d88eee1a Tue, Oct 24 2017 12:02:26.845 filter delay: 0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 filter offset: 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 delay 0.00000, dispersion 64.00000 offset 0.000000 server 91.228.108.200, port 123 stratum 0, precision 0, leap 00, trust 000 refid [91.228.108.200], delay 0.00000, dispersion 64.00000 transmitted 4, in filter 4 reference time: 00000000.00000000 Thu, Feb 7 2036 8:28:16.000 originate timestamp: 00000000.00000000 Thu, Feb 7 2036 8:28:16.000 transmit timestamp: dd998023.0bc09a0a Tue, Oct 24 2017 12:02:27.045 filter delay: 0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 filter offset: 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 delay 0.00000, dispersion 64.00000 offset 0.000000 server 194.177.4.1, port 123 stratum 0, precision 0, leap 00, trust 000 refid [194.177.4.1], delay 0.00000, dispersion 64.00000 transmitted 4, in filter 4 reference time: 00000000.00000000 Thu, Feb 7 2036 8:28:16.000 originate timestamp: 00000000.00000000 Thu, Feb 7 2036 8:28:16.000 transmit timestamp: dd998023.3ef4248c Tue, Oct 24 2017 12:02:27.245 filter delay: 0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 filter offset: 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 delay 0.00000, dispersion 64.00000 offset 0.000000 server 81.16.38.161, port 123 stratum 0, precision 0, leap 00, trust 000 refid [81.16.38.161], delay 0.00000, dispersion 64.00000 transmitted 4, in filter 4 reference time: 00000000.00000000 Thu, Feb 7 2036 8:28:16.000 originate timestamp: 00000000.00000000 Thu, Feb 7 2036 8:28:16.000 transmit timestamp: dd998023.72276991 Tue, Oct 24 2017 12:02:27.445 filter delay: 0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 filter offset: 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 delay 0.00000, dispersion 64.00000 offset 0.000000 24 Oct 12:02:29 ntpdate[26399]: no server suitable for synchronization found As you can see from the output above, we cannot communicate with any server. Contact your network administrator.\nNTPQ tool command shows zeroes in output If you use ntpq tool and the output is the following:\n1 2 3 4 5 6 7 bobos@WEBSRV01:~$ sudo date \u0026amp;\u0026amp; ntpq -p Tue Nov 24 13:26:45 EEST 2017 remote refid st t when poll reach delay offset jitter ============================================================================== 3.gr.pool.ntp.o .POOL. 16 p - 64 0 0.000 0.000 0.000 1.europe.pool.n .POOL. 16 p - 64 0 0.000 0.000 0.000 2.europe.pool.n .POOL. 16 p - 64 0 0.000 0.000 0.000 then it means that you have no communication at all with your NTP servers. Contact your network administrator. A valid and working output would be:\n1 2 3 4 5 6 7 bobos@WEBSRV01:~$ date \u0026amp;\u0026amp; ntpq -p Tue Nov 24 11:00:34 EEST 2017 remote refid st t when poll reach delay offset jitter ============================================================================== * 3.gr.pool.ntp.o .POOL. 4 u 988 1024 377 1.180 39.213 24.397 * 1.europe.pool.n .POOL. 4 u 986 1024 377 1.080 38.984 24.397 * 2.europe.pool.n .POOL. 4 u 985 1024 377 1.710 39.213 24.397 Bibliography For a more \u0026ldquo;in depth dive\u0026rdquo; you can find more in the following links:\nhttps://help.ubuntu.com/lts/serverguide/NTP.html https://wiki.debian.org/NTP https://askubuntu.com/questions/429306/ntpdate-no-server-suitable-for-synchronization-found https://askubuntu.com/questions/825869/ntpd-does-not-sync-clock-while-ntpdate-does That\u0026rsquo;s it! We hope it helped you! It was a long story to debug and actually narrate afterwards!\n","date":"2017-12-14T10:42:00+02:00","permalink":"https://manios.org/2017/12/14/install-and-configure-and-troubleshoot-ntp-servers-in-debian-linux/","title":"Install, configure and troubleshoot NTP servers in Debian Linux"},{"content":"We run MongoDB 2.6.0 on a production system configured with a replica set of 3 members:\nPrimary Secondary Arbiter About 3 weeks ago we had to move the data files from a data centre to another due to disk size problems. Database size had reached 15.6TB. Thus we copied only the data from the PRIMARY node to the new data centre and we started the SECONDARY node totally empty in order to recover via full replication. You can read more about the procedure in this blog post: .\nDue to the large size of the database we had to check the recovery replication process daily. During the last 3 days the status of the replica was as usual STARTUP2 and the logs showed that it builds its indexes:\n1 2 3 2017-11-20T08:53:55.491+0200 [rsSync] Index Build: 62017200/69819295 88% 2017-11-21T09:10:01.192+0200 [rsSync] Index Build: 66154400/69819295 94% 2017-11-22T08:33:52.992+0200 [rsSync] Index: (2/3) BTree Bottom Up Progress: 33284400/53472142 62% In the meantime, Nagios monitoring showed an increase in the replication lag:\n1 2 3 2017-11-20T08:53:55.491+0200 Replication lag: 586379 seconds (6.79 days) 2017-11-21T09:10:01.192+0200 Replication lag: 673901 seconds (7.8 days) 2017-11-22T08:33:52.992+0200 Nagios - Replication lag: 758020 seconds (8.77 days) It made us quite skeptikal until today, when in the daily check, we stumbled upon an error! What a coincidence! Replication had stopped with the following error:\n1 2 3 4 5 6 7 8 9 2017-11-23T08:54:25.937+0200 [rsBackgroundSync] replSet not trying to sync from 192.168.10.4:27017, it is vetoed for 8 more seconds 2017-11-23T08:54:25.937+0200 [rsBackgroundSync] replSet not trying to sync from 192.168.10.4:27017, it is vetoed for 8 more seconds 2017-11-23T08:54:30.000+0200 [rsBackgroundSync] replSet not trying to sync from 192.168.10.4:27017, it is vetoed for 3 more seconds 2017-11-23T08:54:30.000+0200 [rsBackgroundSync] replSet not trying to sync from 192.168.10.4:27017, it is vetoed for 3 more seconds 2017-11-23T08:54:34.062+0200 [rsBackgroundSync] replSet syncing to: 192.168.10.4:27017 2017-11-23T08:54:34.109+0200 [rsBackgroundSync] replSet error RS102 too stale to catch up, at least from 192.168.10.4:27017 2017-11-23T08:54:34.109+0200 [rsBackgroundSync] replSet our last optime : Nov 13 14:06:29 5a098ac5:6 2017-11-23T08:54:34.109+0200 [rsBackgroundSync] replSet oldest at 192.168.10.4:27017 : Nov 16 10:30:19 5a0d4c9b:6 2017-11-23T08:54:34.109+0200 [rsBackgroundSync] replSet See http://dochub.mongodb.org/core/resyncingaverystalereplicasetmember Also when we run on the PRIMARY the command rs.status() it gave us the following output:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 { \u0026#34;set\u0026#34; : \u0026#34;panamapapers-rs\u0026#34;, \u0026#34;date\u0026#34; : ISODate(\u0026#34;2017-11-23T07:12:11.000Z\u0026#34;), \u0026#34;myState\u0026#34; : 1, \u0026#34;members\u0026#34; : [ { \u0026#34;_id\u0026#34; : 0, \u0026#34;name\u0026#34; : \u0026#34;192.168.10.4:27017\u0026#34;, \u0026#34;health\u0026#34; : 1.0, \u0026#34;state\u0026#34; : 1, \u0026#34;stateStr\u0026#34; : \u0026#34;PRIMARY\u0026#34;, \u0026#34;uptime\u0026#34; : 1380243, \u0026#34;optime\u0026#34; : Timestamp(1511421121, 7), \u0026#34;optimeDate\u0026#34; : ISODate(\u0026#34;2017-11-23T07:12:01.000Z\u0026#34;), \u0026#34;electionTime\u0026#34; : Timestamp(1510041044, 1), \u0026#34;electionDate\u0026#34; : ISODate(\u0026#34;2017-11-07T07:50:44.000Z\u0026#34;), \u0026#34;self\u0026#34; : true }, { \u0026#34;_id\u0026#34; : 1, \u0026#34;name\u0026#34; : \u0026#34;192.168.10.5:27017\u0026#34;, \u0026#34;health\u0026#34; : 1.0, \u0026#34;state\u0026#34; : 3, \u0026#34;stateStr\u0026#34; : \u0026#34;RECOVERING\u0026#34;, \u0026#34;uptime\u0026#34; : 836950, \u0026#34;optime\u0026#34; : Timestamp(1510574789, 6), \u0026#34;optimeDate\u0026#34; : ISODate(\u0026#34;2017-11-13T12:06:29.000Z\u0026#34;), \u0026#34;lastHeartbeat\u0026#34; : ISODate(\u0026#34;2017-11-23T07:12:11.000Z\u0026#34;), \u0026#34;lastHeartbeatRecv\u0026#34; : ISODate(\u0026#34;2017-11-23T07:12:10.000Z\u0026#34;), \u0026#34;pingMs\u0026#34; : 0, \u0026#34;lastHeartbeatMessage\u0026#34; : \u0026#34;error RS102 too stale to catch up\u0026#34; }, { \u0026#34;_id\u0026#34; : 2, \u0026#34;name\u0026#34; : \u0026#34;192.168.10.9:27017\u0026#34;, \u0026#34;health\u0026#34; : 1.0, \u0026#34;state\u0026#34; : 7, \u0026#34;stateStr\u0026#34; : \u0026#34;ARBITER\u0026#34;, \u0026#34;uptime\u0026#34; : 1380095, \u0026#34;lastHeartbeat\u0026#34; : ISODate(\u0026#34;2017-11-23T07:12:10.000Z\u0026#34;), \u0026#34;lastHeartbeatRecv\u0026#34; : ISODate(\u0026#34;2017-11-23T07:12:10.000Z\u0026#34;), \u0026#34;pingMs\u0026#34; : 0 } ], \u0026#34;ok\u0026#34; : 1.0 } After searching in the internet we found issue SERVER-14523 where a guy had the same problem as ours. Furthermore we read again the official documentation of how to Resync a Member of a Replica Set. The solutions to our problem were the following:\nResize the oplog to a large value and restart initial sync replication by deleting all files in the SECONDARY node. Copy all data files from the PRIMARY to the SECONDARY node and restart both of them. We rejected resizing the oplog because the database grows every day +9GB so the oplog had to become at least 81GB in order to contain the changes of the aforementioned last 9 days.\nThat\u0026rsquo;s it! I hope it helped you!\n","date":"2017-11-23T10:09:00+02:00","permalink":"https://manios.org/2017/11/23/mongodb-replica-stale-during-recovery/","title":"MongoDB replica error RS102 too stale to catch up during full recovery"},{"content":"We use Solr 4.6 with SolrCloud configuration in a production system. You may say: \u0026ldquo;It is hight time you upgraded to the latest version, isn\u0026rsquo;t it?\u0026rdquo;. And you will be absolute right! But currently this is not the case because full reindexing costs! Our SolrCloud consists of one shard and five replicas as seen in the screenshot bellow:\nThe problem we were facing was that SolrJ library does balance the requests using round robin algorithm when using class LBHttpSolrServer. Furthermore, when a request to a node fails for any reason (timeout, http 403, http 404, etc), it puts the node to a zombie list for 60 seconds. This can become a burden when the SolrCloud has heavy loads and you will start to see in your application logs:\n1 2 3 4 5 6 Exception in thread \u0026#34;main\u0026#34; org.apache.solr.client.solrj.SolrServerException: No live SolrServers available to handle this request at org.apache.solr.client.solrj.impl.LBHttpSolrServer.request(LBHttpSolrServer.java:289) at org.apache.solr.client.solrj.impl.CloudSolrServer.request(CloudSolrServer.java:310) at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:117) at org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:116) at org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:102) So we decided to put a HAProxy instance to load balance the query requests to the SolrCloud using leastconn (least connections) algorithm. After a lot of search and using a combination of official documentation and articles (link1, link2) and the official HAProxy Docker image we created a container using the following configuration:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 global log 127.0.0.1 local0 notice maxconn 2000 #user haproxy #group haproxy defaults log global mode http option httplog option dontlognull retries 3 timeout connect 5000 timeout client 10000 timeout server 10000 listen solrcloud bind 0.0.0.0:8983 mode http stats enable stats uri /haproxy?stats stats realm Strictly\\ Private stats auth haproxyuser:haproxypassword balance leastconn option httpclose option forwardfor option httpchk GET /solr/paradise-papers/admin/ping?wt=json server search1 192.168.1.1:8983 check port 8983 inter 20s fastinter 2s server search2 192.168.1.2:8983 check port 8983 inter 20s fastinter 2s server search3 192.168.1.3:8983 check port 8983 inter 20s fastinter 2s server search4 192.168.1.4:8983 check port 8983 inter 20s fastinter 2s server search5 192.168.1.5:8983 check port 8983 inter 20s fastinter 2s Notes on configuration: Our HAProxy runs in 192.168.1.10 and all the queries are handled by port 8983. We can see statistics about the distribution of requests using HAProxy statistics page which is accessible in http://192.168.1.10:8983/haproxy?stats. See: 1 2 3 stats uri /haproxy?stats stats realm Strictly\\ Private stats auth haproxyuser:haproxypassword We check if every Solr node is alive by pinging it (see: option httpchk GET /solr/paradise-papers/admin/ping?wt=json) Check interval is 20 seconds (see: inter 20s) Now our search queries are handled better and we have already noticed an increase in query performance, faster response times and no downtime! This is a sample output from HAProxy statistics page:\nWe hope you find this article helpful!\n","date":"2017-11-15T12:14:00+02:00","permalink":"https://manios.org/2017/11/15/use-haproxy-and-docker-for-solr-load-balancing/","title":"Use HAProxy and Docker to load balance requests to Solr and SolrCloud"},{"content":"Yesterday I installed SublimeText 3 editor (build 3143) in my PC and the first thing I realised was that I could not type any Greek accented characters. Thus when I wanted to type: ex\n1 Το γοργόν και χάριν έχει. , Sublime actually showed\n1 Το γοργ´ον και χ´αριν ´εχει At first place I thought that maybe was a bug of this new version (I used SublimeText 2 for quite a long time without any problems). I search for bugs and then after quite some time I found in this StackOverflow article that many people had experienced the same problem.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 [ // missing accented characters fix for Linux Mint 18.1 Serena // ά,ό,ί,ϊ,ή,έ,ύ,ώ,ύ,ΰ { \u0026#34;keys\u0026#34;: [\u0026#34;´\u0026#34;,\u0026#34;α\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;characters\u0026#34;: \u0026#34;ά\u0026#34;}}, { \u0026#34;keys\u0026#34;: [\u0026#34;´\u0026#34;,\u0026#34;ο\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;characters\u0026#34;: \u0026#34;ό\u0026#34;}}, { \u0026#34;keys\u0026#34;: [\u0026#34;´\u0026#34;,\u0026#34;ι\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;characters\u0026#34;: \u0026#34;ί\u0026#34;}}, { \u0026#34;keys\u0026#34;: [\u0026#34;¨\u0026#34;,\u0026#34;ι\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;characters\u0026#34;: \u0026#34;ϊ\u0026#34;}}, { \u0026#34;keys\u0026#34;: [\u0026#34;´\u0026#34;,\u0026#34;η\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;characters\u0026#34;: \u0026#34;ή\u0026#34;}}, { \u0026#34;keys\u0026#34;: [\u0026#34;´\u0026#34;,\u0026#34;ε\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;characters\u0026#34;: \u0026#34;έ\u0026#34;}}, { \u0026#34;keys\u0026#34;: [\u0026#34;´\u0026#34;,\u0026#34;υ\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;characters\u0026#34;: \u0026#34;ύ\u0026#34;}}, { \u0026#34;keys\u0026#34;: [\u0026#34;¨\u0026#34;,\u0026#34;υ\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;characters\u0026#34;: \u0026#34;ϋ\u0026#34;}}, { \u0026#34;keys\u0026#34;: [\u0026#34;´\u0026#34;,\u0026#34;ω\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;characters\u0026#34;: \u0026#34;ώ\u0026#34;}}, // Ά,Ό,Ί,Ή,Έ,Ύ,Ώ { \u0026#34;keys\u0026#34;: [\u0026#34;´\u0026#34;,\u0026#34;Α\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;characters\u0026#34;: \u0026#34;Ά\u0026#34;}}, { \u0026#34;keys\u0026#34;: [\u0026#34;´\u0026#34;,\u0026#34;Ο\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;characters\u0026#34;: \u0026#34;Ό\u0026#34;}}, { \u0026#34;keys\u0026#34;: [\u0026#34;´\u0026#34;,\u0026#34;Ι\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;characters\u0026#34;: \u0026#34;Ί\u0026#34;}}, { \u0026#34;keys\u0026#34;: [\u0026#34;´\u0026#34;,\u0026#34;Η\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;characters\u0026#34;: \u0026#34;Ή\u0026#34;}}, { \u0026#34;keys\u0026#34;: [\u0026#34;´\u0026#34;,\u0026#34;Ε\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;characters\u0026#34;: \u0026#34;Έ\u0026#34;}}, { \u0026#34;keys\u0026#34;: [\u0026#34;´\u0026#34;,\u0026#34;Υ\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;characters\u0026#34;: \u0026#34;Ύ\u0026#34;}}, { \u0026#34;keys\u0026#34;: [\u0026#34;´\u0026#34;,\u0026#34;Ω\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;insert\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;characters\u0026#34;: \u0026#34;Ώ\u0026#34;}}, ] ","date":"2017-10-17T21:27:48+03:00","permalink":"https://manios.org/2017/10/17/sublime-3-fix-greek-accented-characters/","title":"SublimeText 3 Fix greek (and other languages) accented chars input"},{"content":"Today I was working on a Java Spring MVC project which uses Maven. My editor was the Eclipse IDE, with the current version of the M2E plug-in. When I pulled the latest commits from the Git repository, my colleagues informed me that we will be using MapStruct as a bean mapper between Entities and DTOs.\nI read a little bit about it and I Ran the project in Eclipse. The Pivotal Tc Server started, but the deployment failed with the following error:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 2017-08-09 13:23:07,364 ERROR ContextLoader - Context initialization failed org.springframework.beans.factory.BeanCreationException: Error creating bean with name \u0026#39;driverProviderServiceImpl\u0026#39;: Injection of autowired dependencies failed; nested exception is org.springframework.beans.factory.BeanCreationException: Could not autowire field: private com.mycompany.bobproject.mapper.DriverMapper com.mycompany.bobproject.service.impl.DriverProviderServiceImpl.DriverMapper; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.mycompany.bobproject.mapper.DriverMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@org.springframework.beans.factory.annotation.Autowired(required=true)} at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessPropertyValues(AutowiredAnnotationBeanPostProcessor.java:334) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1214) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:543) at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:482) at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:306) at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230) at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:302) at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:197) at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:772) at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:839) at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:538) at org.springframework.web.context.ContextLoader.configureAndRefreshWebApplicationContext(ContextLoader.java:444) at org.springframework.web.context.ContextLoader.initWebApplicationContext(ContextLoader.java:326) at org.springframework.web.context.ContextLoaderListener.contextInitialized(ContextLoaderListener.java:107) at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4745) at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5207) at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150) at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:752) at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:728) at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:734) at org.apache.catalina.startup.HostConfig.deployDescriptor(HostConfig.java:596) at org.apache.catalina.startup.HostConfig$DeployDescriptor.run(HostConfig.java:1805) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Caused by: org.springframework.beans.factory.BeanCreationException: Could not autowire field: private com.mycompany.bobproject.mapper.DriverMapper com.mycompany.bobproject.service.impl.DriverProviderServiceImpl.DriverMapper; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.mycompany.bobproject.mapper.DriverMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@org.springframework.beans.factory.annotation.Autowired(required=true)} at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:573) at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:88) at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessPropertyValues(AutowiredAnnotationBeanPostProcessor.java:331) ... 26 more Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [com.mycompany.bobproject.mapper.DriverMapper] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@org.springframework.beans.factory.annotation.Autowired(required=true)} at org.springframework.beans.factory.support.DefaultListableBeanFactory.raiseNoSuchBeanDefinitionException(DefaultListableBeanFactory.java:1373) at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1119) at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1014) at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:545) ... 28 more After a lot of searching I found out that Eclipse does not automatically recognise and run the mapstruct-processor which is declared in pom.xml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.6.1\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;source\u0026gt;${java-version}\u0026lt;/source\u0026gt; \u0026lt;target\u0026gt;${java-version}\u0026lt;/target\u0026gt; \u0026lt;encoding\u0026gt;UTF-8\u0026lt;/encoding\u0026gt; \u0026lt;showWarnings\u0026gt;true\u0026lt;/showWarnings\u0026gt; \u0026lt;showDeprecation\u0026gt;true\u0026lt;/showDeprecation\u0026gt; \u0026lt;annotationProcessorPaths\u0026gt; \u0026lt;path\u0026gt; \u0026lt;groupId\u0026gt;org.mapstruct\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mapstruct-processor\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${org.mapstruct.version}\u0026lt;/version\u0026gt; \u0026lt;/path\u0026gt; \u0026lt;/annotationProcessorPaths\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; However, if when I issued mvn clean package the maven-compiler-plugin ran the processor and generated a DriverMapperImpl located in target/generated-sources/com.mycompany.bobproject.mapper directory.\nThus, after a lot of searching I found out in MapStruct documentation, that in order to use Eclipse and run the processor when project is build (automatically or manually), we have to install the m2e-apt plugin using Eclipse Marketplace and configure the project using the following steps:\n1. Install m2e-apt plugin Using Eclipse menu we go to Help \u0026gt; Eclipse Marketplace ... and we search for m2e-apt. When search completes it will look like the following:\nWhen installation is complete, restart Eclipse. When started you will have to click on \u0026ldquo;Installed\u0026rdquo; tab of Eclipse Marketplace and verify that it looks like the following:\n2. Configure Eclipse to run apt automatically when building the project In order to configure Eclipse to run apt automatically when building the project and generate MaptStruct mappings in Package or Project Explorer, we make a right click on our Project and select Properties. Then we navigate to Maven \u0026gt; Annotation Processing and Enable Project Specific Settings as shown in the following picture:\nWe hit \u0026ldquo;Apply\u0026rdquo; and \u0026ldquo;Apply and Close\u0026rdquo;\n3. Clean target directory and Build project Finally Clean target directory by deleting it or by issuing the command:\n1 mvn clean and build the project. Now it has to run fine.\nFinally, if Eclipse throws the following error:\n1 The declared package \u0026#34;com.mycompany.bobproject.mapper\u0026#34; does not match the expected package \u0026#34;annotations.com.mycompany.bobproject.mapper\u0026#34;\tDriverMapperImpl.java\t/parkalot/target/generated-sources/annotations/com/ots/mycompany/bobproject/mapper\tline 1 then you have to add\n1 \u0026lt;generatedSourcesDirectory\u0026gt;${project.build.directory}/generated-sources\u0026lt;/generatedSourcesDirectory\u0026gt; after the MapStruct declaration. By default, generatedSourcesDirectory property is configured to ${project.build.directory}/generated-sources/annotations. Thus the generated class resides inside annotations.com.mycompany.bobproject.mapper package and produces an error. After the addition, your pom.xml will look like the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.6.1\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;source\u0026gt;${java-version}\u0026lt;/source\u0026gt; \u0026lt;target\u0026gt;${java-version}\u0026lt;/target\u0026gt; \u0026lt;encoding\u0026gt;UTF-8\u0026lt;/encoding\u0026gt; \u0026lt;showWarnings\u0026gt;true\u0026lt;/showWarnings\u0026gt; \u0026lt;showDeprecation\u0026gt;true\u0026lt;/showDeprecation\u0026gt; \u0026lt;annotationProcessorPaths\u0026gt; \u0026lt;path\u0026gt; \u0026lt;groupId\u0026gt;org.mapstruct\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mapstruct-processor\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${org.mapstruct.version}\u0026lt;/version\u0026gt; \u0026lt;/path\u0026gt; \u0026lt;/annotationProcessorPaths\u0026gt; \u0026lt;generatedSourcesDirectory\u0026gt;${project.build.directory}/generated-sources\u0026lt;/generatedSourcesDirectory\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; That\u0026rsquo;s it! I hope it helped you!\n","date":"2017-08-09T13:14:00+03:00","permalink":"https://manios.org/2017/08/09/configure-eclipse-in-order-to-build-mapstruct-in-java-projects/","title":"Configure Eclipse in order to build MapStruct in Java projects"},{"content":"Five days ago we had to renew a number of SSL certificates using free Let\u0026rsquo;s Encrypt SSL authority to a few Apache servers which were installed in Jelastic PaaS environment. The Apache servers were installed in an environment where:\nWe did have SSH access.\nWe did not have any root privileges or sudo command permission.\nThe SSH access was done using a limited apache user.\nThe underlying operating system was CentOS release 6.6 (Final) . (found out about it by running: cat /etc/centos-release)\nThus we could not use the official Let\u0026rsquo;s Encrypt client as it requires root privileges. After 3 hours of searching and striving to use multiple client implementations, we stumbled upon letsencrypt.sh. With the guidance of a tutorial written in German!! we finally managed to create our SSL certificates. So today, at last, we will show you how!!!\nIn this example, we assume that our domain name is bobos.org. We want to create a certificate for bobos.org and www.bobos.org. Note that Let\u0026rsquo;s Encrypt does not generate wildcard certificates yet!!.\nPrerequisites No root access or sudo is required.\nWe Obtain an A record for bobos.org and www.bobos.org which points to the IP of the server you are using.\nInside the aforementioned server we have to run a web server which listens to port 80. Listening to port 443 is optional.\nShell access to the server.\nIf you want to have a thourough understanding of why we are going to perform the following steps, you can refer to How it works official article. Few! It is high time we started. Shall we?\nKnow thy environment. Clone letsecrypt.sh script. Configure the script. Configure your web server. Run the script. Find the certificates. Configure your web server to use the certificates Upload SSL certificates to Jelastic Administration Panel Reload or restart web server and test configuration Know thy environment After login via SSH we are in /var/www/ directory:\n1 2 apache@apache2 ~ $ pwd /var/www Then we check CentOS version, because we can!\n1 2 apache@apache2 ~ $ cat /etc/centos-release CentOS release 6.6 (Final) Clone letsecrypt.sh script While being in that directory we clone letsencrypt.sh:\n1 2 3 4 5 6 7 8 9 10 11 apache@apache2 ~ $ git clone https://github.com/lukas2511/letsencrypt.sh.git Cloning into \u0026#39;letsencrypt.sh\u0026#39;... remote: Counting objects: 873, done. remote: Total 873 (delta 0), reused 0 (delta 0), pack-reused 873 Receiving objects: 100% (873/873), 223.44 KiB | 0 bytes/s, done. Resolving deltas: 100% (530/530), done. Checking connectivity... done. apache@apache2 ~ $ chown -R apache:apache letsencrypt.sh apache@apache2 ~ $ cd letsencrypt.sh apache@apache2 ~ $ pwd /var/www/etsencrypt.sh Configure the script Then, we have perform a little configuration so the script is aware of our environment and the domains for which we want to generate free SSL certificates:\nPrepare base and conf directories: 1 2 3 4 apache@apache2 ~/letsencrypt.sh $ mkdir conf apache@apache2 ~/letsencrypt.sh $ mkdir base apache@apache2 ~/letsencrypt.sh $ cp config.sh.example conf/config.sh apache@apache2 ~/letsencrypt.sh $ cp domains.txt.exampleconfig.sh.example conf/domains.txt Edit configuration in /var/www/letsencrypt.sh/conf/config.sh: 1 2 3 4 CONFIG_D=\u0026#34;/var/www/letsencrypt.sh\u0026#34; BASEDIR=\u0026#34;/var/www/letsencrypt.sh/base\u0026#34; WELLKNOWN=\u0026#34;${BASEDIR}/.acme-challenges\u0026#34; CONTACT_EMAIL=\u0026#34;aValidEmail@whateverServer.com\u0026#34; Add the domain names in /var/www/letsencrypt.sh/conf/domains.txt```: 1 bobos.org www.bobos.org Configure your web server Now we have to put the following lines in your site\u0026rsquo;s special apache conf i.e. /etc/httpd/sites-available/bobos.conf or in /etc/httpd/conf/httpd.conf. This will help Let\u0026rsquo;s Encrypt to access your server, perform domain validation and perform some challenges in order to generate our SSL certificate.\n1 2 3 4 5 6 7 8 Alias /.well-known/acme-challenge /var/www/letsencrypt.sh/base/.acme-challenges \u0026lt;Directory /var/www/letsencrypt.sh/base/.acme-challenges\u0026gt; Options None AllowOverride None Order allow,deny Allow from all \u0026lt;/Directory\u0026gt; Run the script 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apache@apache2 ~/letsencrypt.sh $ ./letsencrypt.sh --config /var/www/letsencrypt.sh/conf/config.sh -c # INFO: Using main config file /var/www/letsencrypt.sh/conf/config.sh # INFO: Using additional config file /var/www/letsencrypt.sh/conf/config.sh Processing bobos.org with alternative names: www.bobos.org + Signing domains... + Creating new directory /var/www/letsencrypt.sh/base/certs/bobos.org ... + Generating private key... + Generating signing request... + Requesting challenge for bobos.org... + Requesting challenge for www.bobos.org... + Responding to challenge for bobos.org... + Challenge is valid! + Responding to challenge for www.bobos.org... + Challenge is valid! + Requesting certificate... + Checking certificate... + Done! + Creating fullchain.pem... + Done! Find the certificates The certificates are created in $BASE/certs// directory:\n1 2 3 4 5 6 7 8 9 10 11 12 apache@apache2 ~/letsencrypt.sh $ ls -l base/certs/bobos.org/ total 20 -rw------- 1 apache apache 1655 Jul 7 06:49 cert-1457333387.csr -rw------- 1 apache apache 2143 Jul 7 06:49 cert-1457333387.pem lrwxrwxrwx 1 apache apache 19 Jul 7 06:50 cert.csr -\u0026gt; cert-1457333387.csr lrwxrwxrwx 1 apache apache 19 Jul 7 06:50 cert.pem -\u0026gt; cert-1457333387.pem -rw------- 1 apache apache 1675 Jul 7 06:50 chain-1457333387.pem lrwxrwxrwx 1 apache apache 20 Jul 7 06:50 chain.pem -\u0026gt; chain-1457333387.pem -rw------- 1 apache apache 3818 Jul 7 06:50 fullchain-1457333387.pem lrwxrwxrwx 1 apache apache 24 Jul 7 06:50 fullchain.pem -\u0026gt; fullchain-1457333387.pem -rw------- 1 apache apache 3243 Jul 7 06:49 privkey-1457333387.pem lrwxrwxrwx 1 apache apache 22 Jul 7 06:50 privkey.pem -\u0026gt; privkey-1457333387.pem Configure your web server to use the certificates Configure Apache in order to use the created certificates. It might by your site\u0026rsquo;s special conf i.e. /etc/httpd/sites-available/bobos.conf or global /etc/httpd/conf/httpd.conf:\n1 2 3 4 5 6 7 SSLEngine On SSLCertificateFile /var/www/letsencrypt.sh/base/certs/bobos.org/cert.pem SSLCertificateKeyFile /var/www/letsencrypt.sh/base/certs/bobos.org/privkey.pem SSLCertificateChainFile /var/www/letsencrypt.sh/base/certs/bobos.org/chain.pem SSLCACertificateFile /var/www/letsencrypt.sh/base/certs/bobos.org/fullchain.pem SSLHonorCipherOrder On SSLCipherSuite ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES128-SHA256:DHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES256-GCM-SHA384:AES128-GCM-SHA256:AES256-SHA256:AES128-SHA256:AES256-SHA:AES128-SHA:DES-CBC3-SHA:HIGH:!aNULL:!eNULL:!EXPORT:!DES:!MD5:!PSK:!RC4 IMPORTANT NOTE!! The previous configuration will not work in Jelastic Paas environment, as it forces us to add the certificates via Jelastic Administration Panel. Jelastic puts the certificates to /var/lib/jelastic/SSL.\n1 2 3 4 5 6 7 8 9 SSLEngine on SSLProtocol ALL -SSLv2 -SSLv3 SSLHonorCipherOrder On SSLCipherSuite ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS SSLCertificateFile /var/lib/jelastic/SSL/jelastic.crt SSLCertificateKeyFile /var/lib/jelastic/SSL/jelastic.key SSLCACertificateFile /var/lib/jelastic/SSL/jelastic-ca.crt Upload SSL certificates to Jelastic Administration Panel Then we have to upload SSL certificates to Jelastic Administration Panel. To achieve that you can use this guide. Otherwise (i.e. putting files via SSH) it will not work.\nReload or restart web server and test configuration Finally we reload sudo /etc/init.d/apache2 reload or restart (sudo /etc/init.d/apache2 restart) Apache web server and verify that https://bobos.org works.\nThat is all folks! Greetings from a hot and striving for the best Greece!\n","date":"2016-07-12T22:32:53+03:00","image":"https://manios.org/hubble_space.png","permalink":"https://manios.org/2016/07/12/lets-encrypt-ssl-certificate-without-root-or-sudo-privileges/","title":"Let's Encrypt SSL certificate without root or sudo privileges"},{"content":"Update 2016-05-13: The following procedure works perfectly for migrations from version 8.0.4 to 8.7.5\nA few days ago, I had to migrate a Gitlab instance installed in Docker from version 8.04 to 8.2.0. I followed the exact steps described in GitLab Docker images documentation. But it was not so easy! Of cource, it required some more! Thus, after 4 hours of errors, searching and trials I found the way! Thank god!\nIn this article we will make the assumption that Gitlab volumes are stored in /home/bob/docker-data/gitlab directory. Here are the exact steps I followed to make a successful migration:\nStop docker container 1 docker stop gitlab Backup docker volumes (all gitlab files) 1 2 3 backupDate=$(date +\u0026#34;%Y%m%d%H%M%S\u0026#34;) \\ \u0026amp;\u0026amp; cd /home/bob/docker-data/ \\ \u0026amp;\u0026amp; sudo tar zvcf gitlab-data-${backupDate}.tar.gz gitlab/ Optionally backup docker image 1 docker save -o /home/bob/gitlab-ce-image.tar gitlab/gitlab-ce:latest Remove docker container 1 docker rm gitlab Download the latest Gitlab docker image 1 sudo docker pull gitlab/gitlab-ce:latest After the image is downloaded create and run the container 1 2 3 4 5 6 7 8 9 10 docker run -d \\ --hostname 192.168.1.1 \\ --publish 8443:443 --publish 8082:80 --publish 2224:22 \\ --name gitlab \\ --restart always \\ --volume /etc/localtime:/etc/localtime \\ --volume /home/bob/docker-data/gitlab/config:/etc/gitlab \\ --volume /home/bob/docker-data/gitlab/logs:/var/log/gitlab \\ --volume /home/bob/docker-data/gitlab/data:/var/opt/gitlab \\ gitlab/gitlab-ce:latest The container starts and we observer the logs via: 1 docker logs -f --tail 10 gitlab The docker container started and we observed in the logs: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 [2015-11-26T15:12:26+02:00] INFO: Retrying execution of execute[create gitlab database user], 19 attempt(s) left [2015-11-26T15:12:28+02:00] INFO: Retrying execution of execute[create gitlab database user], 18 attempt(s) left ... (some lines omitted) ... [2015-11-26T15:13:09+02:00] INFO: Retrying execution of execute[create gitlab database user], 0 attempt(s) left Error executing action `run` on resource \u0026#39;execute[create gitlab database user]\u0026#39; Mixlib::ShellOut::ShellCommandFailed Expected process to exit with [0], but received \u0026#39;2\u0026#39; ---- Begin output of /opt/gitlab/embedded/bin/psql --port 5432 -h /var/opt/gitlab/postgresql -d template1 -c \u0026#34;CREATE USER gitlab\u0026#34; ---- STDOUT: STDERR: psql: could not connect to server: No such file or directory Is the server running locally and accepting connections on Unix domain socket \u0026#34;/var/opt/gitlab/postgresql/.s.PGSQL.5432\u0026#34;? ---- End output of /opt/gitlab/embedded/bin/psql --port 5432 -h /var/opt/gitlab/postgresql -d template1 -c \u0026#34;CREATE USER gitlab\u0026#34; ---- Ran /opt/gitlab/embedded/bin/psql --port 5432 -h /var/opt/gitlab/postgresql -d template1 -c \u0026#34;CREATE USER gitlab\u0026#34; returned 2 Resource Declaration: # In /opt/gitlab/embedded/cookbooks/cache/cookbooks/gitlab/recipes/postgresql.rb 153: execute \u0026#34;create #{sql_user} database user\u0026#34; do 154: command \u0026#34;#{bin_dir}/psql --port #{pg_port} -h #{postgresql_socket_dir} -d template1 -c \\\u0026#34;CREATE USER #{sql_user}\\\u0026#34;\u0026#34; 155: user postgresql_user 156: # Added retries to give the service time to start on slower systems 157: retries 20 158: not_if { !pg_helper.is_running? || pg_helper.user_exists?(sql_user) } 159: end 160: Compiled Resource: # Declared in /opt/gitlab/embedded/cookbooks/cache/cookbooks/gitlab/recipes/postgresql.rb:153:in `block in from_file\u0026#39; execute(\u0026#34;create gitlab database user\u0026#34;) do action [:run] retries 20 retry_delay 2 default_guard_interpreter :execute command \u0026#34;/opt/gitlab/embedded/bin/psql --port 5432 -h /var/opt/gitlab/postgresql -d template1 -c \\\u0026#34;CREATE USER gitlab\\\u0026#34;\u0026#34; backup 5 returns 0 user \u0026#34;gitlab-psql\u0026#34; declared_type :execute cookbook_name \u0026#34;gitlab\u0026#34; recipe_name \u0026#34;postgresql\u0026#34; not_if { #code block } end These are two known permission issues described in official documentation and in issue #9611. In order to overcome them, we execute: 1 2 docker exec -it gitlab update-permissions docker exec gitlab chown -R gitlab-redis /var/opt/gitlab/redis and finally we restart the docker container: 1 docker restart gitlab Then we have to check if the database migration was successfull and avoid issue #3255. Thus, we log into bash shell: 1 docker exec -t -i gitlab /bin/bash and we check the db migration status: 1 sudo gitlab-rake db:migrate:status If everything is stated as up, we are OK. However, if we notice down migrations like this 1 2 3 up 20150920161119 Add line code to sent notification down 20150924125150 Add project id to ci commit down 20150924125436 Migrate project id for ci commits we have to rerun the database migration ourserlves: 1 sudo gitlab-rake db:migrate When it finishes we have to check that everything went ok: 1 sudo gitlab-rake db:migrate:status While being inside the container bash shell, we reconfigure the installation: 1 sudo gitlab-ctl reconfigure and then check that everything went ok: 1 sudo gitlab-rake gitlab:check If everything is OK, then the command: 1 sudo gitlab-rake gitlab:env:info RAILS_ENV=production must return something like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 System information System: Ubuntu 14.04 Current User: git Using RVM: no Ruby Version: 2.1.7p400 Gem Version: 2.2.5 Bundler Version:1.10.6 Rake Version: 10.4.2 Sidekiq Version:3.3.0 GitLab information Version: 8.2.0 Revision: d6bcf44 Directory: /opt/gitlab/embedded/service/gitlab-rails DB Adapter: postgresql URL: http://192.168.1.1:8082 HTTP Clone URL: http://192.168.1.1:8082/some-group/some-project.git SSH Clone URL: git@192.168.1.1:some-group/some-project.git Using LDAP: yes Using Omniauth: no GitLab Shell Version: 2.6.7 Repositories: /var/opt/gitlab/git-data/repositories Hooks: /opt/gitlab/embedded/service/gitlab-shell/hooks/ Git: /opt/gitlab/embedded/bin/git Finally we have to clear Redis cache, or we will face the issue #3619 or issue #3609: 1 sudo gitlab-rake cache:clear After that you will be able to login properly to http://localhost:8082 ","date":"2015-12-04T22:47:06+02:00","image":"https://manios.org/hubble_space.png","permalink":"https://manios.org/2015/12/04/migrate-a-gitlab-docker-container-from-version-8-0-4-to-8-2-0/","title":"Migrate a Gitlab Docker container from version 8.0.4 to 8.2.0"},{"content":"Today I had to parse a JSON response in a JUnit test and then reuse it in a cURL command line call. Unfortunately I could not read the JSON response from a file or from an HTTP request, so I had to place it inline as a String variable. My JSON looked like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \u0026#34;responseHeader\u0026#34;: { \u0026#34;status\u0026#34;: 0, \u0026#34;QTime\u0026#34;: 193 }, \u0026#34;defaultCoreName\u0026#34;: \u0026#34;collection1\u0026#34;, \u0026#34;initFailures\u0026#34;: {}, \u0026#34;status\u0026#34;: { \u0026#34;jet2pilot_shard1_replica2\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;jet2pilot_shard1_replica2\u0026#34;, \u0026#34;isDefaultCore\u0026#34;: false, \u0026#34;instanceDir\u0026#34;: \u0026#34;/opt/solr-4.8.1/example/solr/jet2pilot_shard1_replica2/\u0026#34;, \u0026#34;dataDir\u0026#34;: \u0026#34;/opt/solr-4.8.1/example/solr/jet2pilot_shard1_replica2/data/\u0026#34;, \u0026#34;config\u0026#34;: \u0026#34;solrconfig.xml\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;schema.xml\u0026#34;, \u0026#34;startTime\u0026#34;: \u0026#34;2014-12-31T19:12:41.633Z\u0026#34;, \u0026#34;uptime\u0026#34;: 1350610415 } } } 1. Use JSON in a Java string variable In order to parse JSON from a Java String variable, I had to:\nremove \\t,\\n,\\r characters and\nescape single and double quotes (',\u0026quot;) Thus, Linux shell came to the rescue! So, write your JSON to a file, for example myresponse.json and execute the following command:\n1 cat myresponse.json | tr \u0026#39;\\r\u0026#39; \u0026#39; \u0026#39; | tr \u0026#39;\\n\u0026#39; \u0026#39; \u0026#39; | sed \u0026#34;s/[\u0026#39;]/\\\\\\\u0026#39;/g\u0026#34; | sed \u0026#39;s/\\\u0026#34;/\\\\\u0026#34;/g\u0026#39; | sed \u0026#39;s/ \\{3,\\}/ /g\u0026#39; | sed \u0026#39;s/ / /g\u0026#39; \u0026gt; onelinejson.txt Then you can use JSON as a simple Java String variable like the following:\n1 2 3 4 5 6 7 @Test public void testAktiston(){ String jsonString = \u0026#34;{ \\\u0026#34;responseHeader\\\u0026#34;: { \\\u0026#34;status\\\u0026#34;: 0, \\\u0026#34;QTime\\\u0026#34;: 193 }, \\\u0026#34;defaultCoreName\\\u0026#34;: \\\u0026#34;collection1\\\u0026#34;, \\\u0026#34;initFailures\\\u0026#34;: {}, \\\u0026#34;status\\\u0026#34;: { \\\u0026#34;jet2pilot_shard1_replica2\\\u0026#34;: { \\\u0026#34;name\\\u0026#34;: \\\u0026#34;jet2pilot_shard1_replica2\\\u0026#34;, \\\u0026#34;isDefaultCore\\\u0026#34;: false, \\\u0026#34;instanceDir\\\u0026#34;: \\\u0026#34;/opt/solr-4.8.1/example/solr/jet2pilot_shard1_replica2/\\\u0026#34;, \\\u0026#34;dataDir\\\u0026#34;: \\\u0026#34;/opt/solr-4.8.1/example/solr/jet2pilot_shard1_replica2/data/\\\u0026#34;, \\\u0026#34;config\\\u0026#34;: \\\u0026#34;solrconfig.xml\\\u0026#34;, \\\u0026#34;schema\\\u0026#34;: \\\u0026#34;schema.xml\\\u0026#34;, \\\u0026#34;startTime\\\u0026#34;: \\\u0026#34;2014-12-31T19:12:41.633Z\\\u0026#34;, \\\u0026#34;uptime\\\u0026#34;: 1350610415 } } }\u0026#34;; // test code // parsing and other chaotic code } 2. Use JSON to perform a POST request using cURL In order to parse JSON from a Java String variable, I had to:\nremove \\t,\\n,\\r characters and Again, Linux shell makes the world go round! So, write your JSON to a file, for example myresponse.json and execute the following command:\n1 cat myresponse.json | tr \u0026#39;\\r\u0026#39; \u0026#39; \u0026#39; | tr \u0026#39;\\n\u0026#39; \u0026#39; \u0026#39; | sed \u0026#39;s/ \\{3,\\}/ /g\u0026#39; | sed \u0026#39;s/ / /g\u0026#39; \u0026gt; onelinejson.txt Then use it in your POST request via cURL command (the command is multilined in order to be legible):\n1 2 3 4 5 6 7 curl -v -X POST -H \u0026#34;Cookie: JSESSIONID=00213719A12D07F7E67BE8B580CD9BBC\u0026#34; -H \u0026#34;Content-Type: multipart/form-data\u0026#34; -H \u0026#34;Accept: application/json\u0026#34; -F \u0026#39;repEx={ \u0026#34;responseHeader\u0026#34;: { \u0026#34;status\u0026#34;: 0, \u0026#34;QTime\u0026#34;: 193 }, \u0026#34;defaultCoreName\u0026#34;: \u0026#34;collection1\u0026#34;, \u0026#34;initFailures\u0026#34;: {}, \u0026#34;status\u0026#34;: { \u0026#34;jet2pilot_shard1_replica2\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;jet2pilot_shard1_replica2\u0026#34;, \u0026#34;isDefaultCore\u0026#34;: false, \u0026#34;instanceDir\u0026#34;: \u0026#34;/opt/solr-4.8.1/example/solr/jet2pilot_shard1_replica2/\u0026#34;, \u0026#34;dataDir\u0026#34;: \u0026#34;/opt/solr-4.8.1/example/solr/jet2pilot_shard1_replica2/data/\u0026#34;, \u0026#34;config\u0026#34;: \u0026#34;solrconfig.xml\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;schema.xml\u0026#34;, \u0026#34;startTime\u0026#34;: \u0026#34;2014-12-31T19:12:41.633Z\u0026#34;, \u0026#34;uptime\u0026#34;: 1350610415 } } }\u0026#39; \u0026#34;http://192.168.1.89:8080/apios/insertios\u0026#34; That is all about καρντάσια (kardasia is translated as \u0026ldquo;folks\u0026rdquo; in Thessaloniki, Greece)! Have a nice year!\n","date":"2014-12-30T23:58:16+02:00","permalink":"https://manios.org/2014/12/30/json-remove-new-lines-via-linux-and-use-it-as-string-variable-and-as-param-in-curl-post-request/","title":"JSON remove new lines via Linux and use it as string variable and as param in cURL POST request"},{"content":"A few days ago, a Solr server in our SolrCloud installation stopped unexpectedly. After examining solr.log file I spotted that it could not start again because index.20140510031827076 file was corrupted. After some searching I found this Lucidworks article which described how to deal with a corrupted index file. So I proceeded to the following steps:\nWARNING!! This procedure may result in unrecoverable data loss. It is vital that you backup your index before performing index checking and repair.\nFind the guilty index file containing the corrupted segment. For me it was located in: 1 C:/Program Files/apache-solr-4.6.0/example/solr/hellaserver_shard1_replica4/data/index.20140510031827076 Find Lucene core .jar file. I work with Apache Solr 4.6.0 so it is a file named lucene-core-4.6.0.jar. It is usually in $SOLR_HOME/example/solr-webapp/webapp/WEB-INF/lib/lucene-core-4.6.0.jar. So, switch to the directory where it exists: 1 cd C:/Program Files/apache-solr-4.6.0/example/solr-webapp/webapp/WEB-INF/lib Check the segments of the corrupted index file in order to identify the problematic segment. To accomplish that, run: 1 java -cp lucene-core-4.6.0.jar -ea:org.apache.lucene... org.apache.lucene.index.CheckIndex \u0026#34;C:/Program Files/apache-solr-4.6.0/example/solr/hellaserver_shard1_replica4/data/index.20140510031827076\u0026#34; The check results for a healthy segment look like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 1 of 37: name=_48r7 docCount=3021717 codec=Lucene46 compound=false numFiles=11 size (MB)=5,020.87 diagnostics = {timestamp=1403095380034, os=Windows Server 2012, os.version=6 .2, mergeFactor=10, source=merge, lucene.version=4.6.0 1543363 - simon - 2013-11 -19 11:05:50, os.arch=amd64, mergeMaxNumSegments=-1, java.version=1.7.0_51, java .vendor=Oracle Corporation} has deletions [delGen=170] test: open reader.........OK [196 deleted docs] test: fields..............OK [135 fields] test: field norms.........OK [52 fields] test: terms, freq, prox...OK [16644025 terms; 441992389 terms/docs pairs; 39 0271915 tokens] test (ignoring deletes): terms, freq, prox...OK [16646658 terms; 442060892 t erms/docs pairs; 390381979 tokens] test: stored fields.......OK [107214499 total field count; avg 35.484 fields per doc] test: term vectors........OK [0 total vector count; avg 0 term/freq vector f ields per doc] test: docvalues...........OK [0 docvalues fields; 0 BINARY; 0 NUMERIC; 0 SOR TED; 0 SORTED_SET] 2 of 37: name=_3b16 docCount=2449309 codec=Lucene46 compound=false numFiles=11 size (MB)=3,831.743 diagnostics = {timestamp=1402370404453, os=Windows Server 2012, os.version=6 .2, mergeFactor=10, source=merge, lucene.version=4.6.0 1543363 - simon - 2013-11 -19 11:05:50, os.arch=amd64, mergeMaxNumSegments=-1, java.version=1.7.0_51, java .vendor=Oracle Corporation} has deletions [delGen=426] test: open reader.........OK [20262 deleted docs] test: fields..............OK [92 fields] test: field norms.........OK [35 fields] When the index checker encounters a corrupted segment, the output looks like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 37 of 37: name=_4gxr docCount=11 codec=Lucene46 compound=false numFiles=10 size (MB)=75.71 diagnostics = {timestamp=1403212995547, os=Windows Server 2012, os.version=6 .2, source=flush, lucene.version=4.6.0 1543363 - simon - 2013-11-19 11:05:50, os .arch=amd64, java.version=1.7.0_51, java.vendor=Oracle Corporation} no deletions test: open reader.........FAILED WARNING: fixIndex() would remove reference to this segment; full exception: org.apache.lucene.index.CorruptIndexException: invalid docCount: 48066 maxDoc: 1 1 (resource=MMapIndexInput(path=\u0026#34;C:\\Program Files\\apache-solr-4.6.0\\example\\solr \\hellasever_shard1_replica4\\data\\index.20140510031827076\\_4gxr_Lucene41_0.ti m\u0026#34;)) at org.apache.lucene.codecs.BlockTreeTermsReader.(BlockTreeTermsRe ader.java:166) at org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat.fieldsProduc er(Lucene41PostingsFormat.java:437) at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsReader .(PerFieldPostingsFormat.java:195) at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat.fieldsProduc er(PerFieldPostingsFormat.java:244) at org.apache.lucene.index.SegmentCoreReaders.(SegmentCoreReaders. java:115) at org.apache.lucene.index.SegmentReader.(SegmentReader.java:95) at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:554) at org.apache.lucene.index.CheckIndex.main(CheckIndex.java:1941) WARNING: 1 broken segments (containing 11 documents) detected WARNING: would write new segments file, and 11 documents would be lost, if -fix were specified After the reconnaissance of the corrupted segment, rerun the command with -fix parameter: 1 java -cp lucene-core-4.6.0.jar -ea:org.apache.lucene... org.apache.lucene.index.CheckIndex \u0026#34;C:/Program Files/apache-solr-4.6.0/example/solr/hellaserver_shard1_replica4/data/index.20140510031827076\u0026#34; -fix When the checker fixes the segment, the output will look like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 37 of 37: name=_4gxr docCount=11 codec=Lucene46 compound=false numFiles=10 size (MB)=75.71 diagnostics = {timestamp=1403212995547, os=Windows Server 2012, os.version=6 .2, source=flush, lucene.version=4.6.0 1543363 - simon - 2013-11-19 11:05:50, os .arch=amd64, java.version=1.7.0_51, java.vendor=Oracle Corporation} no deletions test: open reader.........FAILED WARNING: fixIndex() would remove reference to this segment; full exception: org.apache.lucene.index.CorruptIndexException: invalid docCount: 48066 maxDoc: 1 1 (resource=MMapIndexInput(path=\u0026#34;C:\\Program Files\\apache-solr-4.6.0\\example\\solr \\hellasever_shard1_replica4\\data\\index.20140510031827076\\_4gxr_Lucene41_0.ti m\u0026#34;)) at org.apache.lucene.codecs.BlockTreeTermsReader.\u0026lt;init\u0026gt;(BlockTreeTermsRe ader.java:166) at org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat.fieldsProduc er(Lucene41PostingsFormat.java:437) at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsReader .\u0026lt;init\u0026gt;(PerFieldPostingsFormat.java:195) at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat.fieldsProduc er(PerFieldPostingsFormat.java:244) at org.apache.lucene.index.SegmentCoreReaders.\u0026lt;init\u0026gt;(SegmentCoreReaders. java:115) at org.apache.lucene.index.SegmentReader.\u0026lt;init\u0026gt;(SegmentReader.java:95) at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:554) at org.apache.lucene.index.CheckIndex.main(CheckIndex.java:1941) WARNING: 1 broken segments (containing 11 documents) detected WARNING: 11 documents will be lost NOTE: will write new segments file in 5 seconds; this will remove 11 docs from t he index. THIS IS YOUR LAST CHANCE TO CTRL+C! 5... 4... 3... 2... 1... Writing... OK Wrote new segments file \u0026#34;segments_y5e\u0026#34; Finally, restart Solr server in order to resynchronise with the shard leader. ","date":"2014-11-01T16:40:40+02:00","permalink":"https://manios.org/2014/11/01/solr-fix-corrupted-index-using-lucene/","title":"Solr fix corrupted index using Lucene"},{"content":"Today I had some mp4 video files and I wanted to extract the audio and video into separate files. So to do that you can do the following:\nExtract audio from video file and convert it to mp3 To accomplish that, execute the command:\n1 ffmpeg -i logothetis-bureaucracy.mp4 -ab 128k -ac 2 -ar 44100 -vn logothetis-bureaucracy.mp3 Extract only the video segment from a video To accomplish that, execute the command:\n1 ffmpeg -i video.wmv -vcodec copy -an onlyvideo.wmv ","date":"2014-07-28T14:21:08+03:00","permalink":"https://manios.org/2014/07/28/linux-extract-audio-and-video-using-ffmpeg/","title":"Linux: Extract audio and video using ffmpeg "},{"content":"Today I had to hide a Wordpress 3.8.1 blog behind an Nginx reverse proxy configured to use only https. Nginx was behind an external firewall which forwarded https://bob.org:8080/blog to Nginx using https (port 4443). The difficulty was that whe Wordpress blog was installed in an Apache HTTP server in port 80 and it worked using http!. Lets say that my site was https://bob.org and I had to put the blog in https://bob.org:8080/blog. Thus my list of burdens was the following:\nConfigure Nginx to use only https and redirect http to https. Reverse proxy in Nginx from bob.org:8080/blog to 192.168.1.10/wordpress in Apache. Force Wordpress links to use port 8080 The following is the diagram summarised my server configuration:\n1 2 3 4 5 6 7 8 9 10 11 12 +--------------+ | Firewall | | bob.org:8080 | +------+-------+ | | v +–––––––––+ +––––––––––––––+ | Nginx | http | Wordpress | | (https) |+----------------\u0026gt;| Apache HTTP | | | | (port 80) | +–––––––––+ +––––––––––––––+ So you will say: \u0026ldquo;Are you completely insane ??!!\u0026rdquo; I was asked to do it! That is my answer!\nFinally after one hour of searching I used a combination of two support articles (post1, post2), some PHP knowledge and experience and a little of imagination, I created a solution:\nConfigure Nginx to reverse proxy all requests to /blog This is a fragment of my /etc/nginx/sites-available/default-ssl Nginx configuration file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 upstream blog-webservers { server 192.168.1.10:80; } # redirect http to https # http://serverfault.com/a/171238 server { listen 80; rewrite ^(.*) https://$host$1 permanent; } server { listen *:443; ssl on; ssl_certificate /etc/nginx/crypto/server.crt; ssl_certificate_key /etc/nginx/crypto/server.key; ssl_client_certificate /etc/nginx/crypto/ca.crt; ssl_verify_client optional; ssl_verify_depth 10; # pem key asking for password problem # http://pandemoniumillusion.wordpress.com/2008/04/21/nginx-ssl-passphrase-at-startup/ server_name bob.org; access_log /var/log/nginx/ssl.access.log; error_log /var/log/nginx/ssl.error.log; error_page 404 /404.html; # reverse proxy to blog web servers location /blog { proxy_pass http://blog-webservers/wordpress/; proxy_redirect https://server_name http://blog-webservers/wordpress; proxy_read_timeout 3500; proxy_connect_timeout 3250; proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto https; proxy_set_header SSL_PROTOCOL $ssl_protocol; proxy_set_header SSL_CLIENT_CERT $ssl_client_cert; proxy_set_header SSL_CLIENT_VERIFY $ssl_client_verify; proxy_set_header SSL_SERVER_S_DN $ssl_client_s_dn; } } Configure Wordpress wp-config.php I added the following lines on top of wp-config.php file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // If Wordpress is behind reverse proxy // which proxies https to http if ( (!empty( $_SERVER[\u0026#39;HTTP_X_FORWARDED_HOST\u0026#39;])) || (!empty( $_SERVER[\u0026#39;HTTP_X_FORWARDED_FOR\u0026#39;])) ) { // http://wordpress.org/support/topic/wordpress-behind-reverse-proxy-1 $_SERVER[\u0026#39;HTTP_HOST\u0026#39;] = $_SERVER[\u0026#39;HTTP_X_FORWARDED_HOST\u0026#39;]; define(\u0026#39;WP_HOME\u0026#39;, \u0026#39;https://bob.org:8080/blog\u0026#39;); define(\u0026#39;WP_SITEURL\u0026#39;, \u0026#39;https://bob.org:8080/blog\u0026#39;); // rewrite blog word with wordpress $_SERVER[\u0026#39;REQUEST_URI\u0026#39;] = str_replace(\u0026#34;wordpress\u0026#34;, \u0026#34;blog\u0026#34;, $_SERVER[\u0026#39;REQUEST_URI\u0026#39;]); // http://wordpress.org/support/topic/compatibility-with-wordpress-behind-a-reverse-proxy $_SERVER[\u0026#39;HTTPS\u0026#39;] = \u0026#39;on\u0026#39;; } If you want to debug the former PHP code fragment you can print $_SERVER variables before and after configuration changing command inside the if block:\n1 2 3 4 5 6 7 8 9 10 11 12 13 echo \u0026#39;\u0026lt;br\u0026gt;Before:\u0026#39;; echo \u0026#39;\u0026lt;br\u0026gt;$_SERVER[\\\u0026#39;HTTP_HOST\\\u0026#39;] : \u0026#39; . $_SERVER[\u0026#39;HTTP_HOST\u0026#39;]; echo \u0026#39;\u0026lt;br\u0026gt;$_SERVER[\\\u0026#39;HTTP_X_FORWARDED_HOST\\\u0026#39;]: \u0026#39; . $_SERVER[\u0026#39;HTTP_X_FORWARDED_HOST\u0026#39;]; echo \u0026#39;\u0026lt;br\u0026gt;$_SERVER[\\\u0026#39;REQUEST_URI\\\u0026#39;]: \u0026#39; . $_SERVER[\u0026#39;REQUEST_URI\u0026#39;]; echo \u0026#39;\u0026lt;br\u0026gt;$_SERVER[\\\u0026#39;HTTP_X_FORWARDED_SERVER\\\u0026#39;]: \u0026#39; . $_SERVER[\u0026#39;HTTP_X_FORWARDED_SERVER\u0026#39;]; echo \u0026#39;\u0026lt;br\u0026gt;$_SERVER[\\\u0026#39;HTTP_X_FORWARDED_FOR\\\u0026#39;]: \u0026#39; . $_SERVER[\u0026#39;HTTP_X_FORWARDED_FOR\u0026#39;]; echo \u0026#39;\u0026lt;br\u0026gt;$_SERVER[\\\u0026#39;HTTP_X_FORWARDED_FOR\\\u0026#39;]: \u0026#39; . $_SERVER[\u0026#39;HTTP_X_FORWARDED_FOR\u0026#39;]; echo \u0026#39;\u0026lt;br\u0026gt;$_SERVER[\\\u0026#39;HTTPS\\\u0026#39;]: \u0026#39; . $_SERVER[\u0026#39;HTTPS\u0026#39;]; echo \u0026#39;\u0026lt;br\u0026gt;$_SERVER[\\\u0026#39;REMOTE_ADDR\\\u0026#39;]: \u0026#39; . $_SERVER[\u0026#39;REMOTE_ADDR\u0026#39;]; echo \u0026#39;\u0026lt;br\u0026gt;$_SERVER[\\\u0026#39;SERVER_NAME\\\u0026#39;]: \u0026#39; . $_SERVER[\u0026#39;SERVER_NAME\u0026#39;]; echo \u0026#39;\u0026lt;br\u0026gt;$_SERVER[\\\u0026#39;SERVER_PROTOCOL\\\u0026#39;]: \u0026#39; . $_SERVER[\u0026#39;SERVER_PROTOCOL\u0026#39;]; echo \u0026#39;\u0026lt;br\u0026gt;WP_HOME: \u0026#39; . WP_HOME; echo \u0026#39;\u0026lt;br\u0026gt;WP_SITEURL : \u0026#39; . WP_SITEURL; I am exhausted, just writting it down! Be patient!\n","date":"2014-04-12T17:32:52+03:00","permalink":"https://manios.org/2014/04/12/nginx-https-reverse-proxy-to-wordpress-with-apache-http-and-different-port/","title":"Nginx https reverse proxy to Wordpress with Apache, http and different port"},{"content":"Today I wanted to export to a .tar.gz archive all unstracked (new) , modified and deleted files from a Git repository. After some searching, I found a StackOverflow post which helped a lot. So after you cd inside your repository and you have two options:\nExport modified moved and deleted files 1 tar zvcf ~/new-files-cache.tar.gz `git diff --name-only --diff-filter=ACMRT` Export Untracked (new), modified and deleted files To achieve that I followed another custom route. If you find something smarter, please inform me!\n1 tar zvcf ~/new-files-cache.tar.gz ` git status --short | sed \u0026#39;s/^ *[^ ]* \\(.*\\)/\\1/g\u0026#39; ` That\u0026rsquo;s it! Have fun with Git!\n","date":"2014-04-12T16:35:56+03:00","permalink":"https://manios.org/2014/04/12/export-untracked-modified-moved-and-deleted-files-from-a-git-repository-to-archive/","title":"Export Untracked, modified, moved and deleted files from a Git repository to archive"},{"content":"Today I wanted to install Apache Solr index in Windows 7, Windows Server 2008R2 and Windows Server 2012 machines. So I achieved that , using the following steps:\nDownload Apache Solr Download NSSM - the Non-Sucking Service Manager Create a .bat init file for Solr Create Windows service using NSSM Test Solr service In detail:\n1. Download Apache Solr Download Apache Solr .zip file from Apache Solr website. I am currently using version 4.6.0.\n2. Download NSSM - the Non-Sucking Service Manager After a lot of searching for a stable solution in creating Windows services I stumbled upon NSSM - the Non-Sucking Service Manager. Download it and place the nssm.exe executable in a directory that can be seen by $PATH.\n3. Create a .bat init file for Solr Unzip Apache Solr zip file to\n1 C:\\Program Files\\apache-solr-4.6.0\\ Create C:\\Program Files\\apache-solr-4.6.0\\example\\solrstart.bat file containing the following:\n1 2 3 @echo off cd \u0026#34;C:\\Program Files\\apache-solr-4.6.0\\example\\\u0026#34; java -Xms64M -Xmx256M -Djava.util.logging.config.file=etc/logging.properties -jar start.jar 4. Create Windows service using NSSM Open a command line prompt (WinKey + R and type \u0026ldquo;cmd\u0026rdquo;) and type the following 1 nssm install Solr4.6.0 \u0026#34;C:\\Program Files\\apache-solr-4.6.0\\example\\solrstart.bat\u0026#34; If the procedure succeeds you should see a message like this:Service \u0026ldquo;Solr4.6.0\u0026rdquo; installed successfully! If you want to uninstall the service open again a command line prompt and type the following: 1 nssm remove Solr4.6.0 confirm If the procedure succeeds you should see a message like this: Service \u0026ldquo;Solr4.6.0\u0026rdquo; removed successfully! 5. Test Solr service Open a command line prompt (WinKey + R and type \u0026ldquo;cmd\u0026rdquo;) and type the following 1 net start Solr4.6.0 If the service was started successfully, open a web browser and access admin page: http://localhost:8983 That is all about it folks! May the Solr be with you!\n","date":"2014-02-15T15:00:52+02:00","permalink":"https://manios.org/2014/02/15/install-apache-solr-as-windows-service-using-nssm/","title":"Install Apache Solr as Windows Service using NSSM"},{"content":"Today I had a database table which contained special and control characters (like DLE, DC1, ACK, DC3 etc.) in its records. After some searching I found a way to substitute those characters with their respective unicode escape value. And here Python comes to the rescue again!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #!/usr/bin/python # -*- coding: utf-8 -*- # DLE, DC1, ACK, DC3 characters # a string with special and control characters bob = \u0026#34;\u0026#34;\u0026#34;`Αδεια διεξαγωγής αγώνων κυνηγετικών ικανοτήτων σκύλων δεικτών.\u0026#34;\u0026#34;\u0026#34; # bob string encoded in escaped unicode string escapedbob = \u0026#34;\\u0010`\\u0391\\u03b4\\u03b5\\u03b9\\u03b1 \\u03b4\\u03b9\\u03b5\\u03be\\u03b1\\u03b3\\u03c9\\u03b3\\u03ae\\u03c2 \\u03b1\\u03b3\\u03ce\\u03bd\\u03c9\\u03bd \\u03ba\\u03c5\\u03bd\\u03b7\\u03b3\\u03b5\\u03c4\\u03b9\\u03ba\\u03ce\\u03bd \\u03b9\\u03ba\\u03b1\\u03bd\\u03bf\\u03c4\\u03ae\\u03c4\\u03c9\\u03bd \\u03c3\\u03ba\\u03cd\\u03bb\\u03c9\\u03bd \\u03b4\\u03b5\\u03b9\\u03ba\\u03c4\\u03ce\\u03bd.\\u0010\\u0011\\u0006\\u0013\\u0010\u0026#34; def main(): # encode special characters to unicode-escape print bob.decode(\u0026#39;utf-8\u0026#39;).encode(\u0026#39;unicode-escape\u0026#39;) # decode unicode-escape to simple characters print escapedbob.decode(\u0026#39;unicode-escape\u0026#39;) # exit exit(0) if __name__ == \u0026#39;__main__\u0026#39;: main() # used tutorial # http://stackoverflow.com/questions/10268518/python-string-to-unicode Good luck tampering and hacking with your database strings !\n","date":"2013-12-31T08:12:33+02:00","permalink":"https://manios.org/2013/12/31/remove-special-and-control-characters-from-string-using-python/","title":"Remove special and control characters from string using Python"},{"content":"Today I wanted to extract the SMS content out of the android SMS application SQLite database using ADB.\nNOTE: The whole procedure was tested on a rooted phone running Cyanogenmod Android. It may not work on unrooted phones\nSo I achieved that , using the following steps:\nFind SMS database file location Pull SQLite database file with ADB Identify sms tables Identify the thread _ID of the SMS you want to read Find all sms messages for a specific thread id In detail:\n1. Find SMS database file location After some searching I found that the location of sms database file is in\n1 /data/data/com.android.providers.telephony/databases/mmssms.db 2. Pull SQLite database file with ADB This step is optional. Open a shell and type:\n1 adb pull /data/data/com.android.providers.telephony/databases/mmssms.db mmssms.db Now the whole sms database is copied locally in your working directory!\n3. Identify sms tables Open the SQLite using command line or your favourite SQLite editor. The tables in which the android telephony application stores SMS messages are threads and sms. Table Threads stores a record as a header for every SMS thread which is started:\n1 2 3 4 5 6 7 8 9 10 CREATE TABLE threads ( _id INTEGER PRIMARY KEY AUTOINCREMENT, date INTEGER DEFAULT 0,message_count INTEGER DEFAULT 0, recipient_ids TEXT,snippet TEXT, snippet_cs INTEGER DEFAULT 0, read INTEGER DEFAULT 1, type INTEGER DEFAULT 0, error INTEGER DEFAULT 0, has_attachment INTEGER DEFAULT 0 ) Table SMS stores all outgoing and incoming messages for every thread:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 CREATE TABLE sms (_id INTEGER PRIMARY KEY, thread_id INTEGER, address TEXT, person INTEGER, date INTEGER, date_sent INTEGER DEFAULT 0, protocol INTEGER, read INTEGER DEFAULT 0, status INTEGER DEFAULT -1, type INTEGER, reply_path_present INTEGER, subject TEXT, body TEXT, service_center TEXT, locked INTEGER DEFAULT 0, error_code INTEGER DEFAULT 0, seen INTEGER DEFAULT 0 ) 4. Identify the thread _ID of the SMS you want to read Run a SQL query and read snippet column. This column stores the last sms in your thread and is the visible text of every SMS list item when you open the SMS application.\n1 2 SELECT _id, snippet FROM threads; 5. Find all sms messages for a specific thread id In my case the thread _id had the value 310. Knowing this id we run another query on sms table in order to get SMS messages ordered chronologically by date:\n1 2 3 4 SELECT datetime(date/1000, \u0026#39;unixepoch\u0026#39;,\u0026#39;localtime\u0026#39;) ,datetime(date_sent/1000, \u0026#39;unixepoch\u0026#39;,\u0026#39;localtime\u0026#39;) ,person,body FROM sms WHERE thread_id = 310 ORDER BY date The results are going to be similar to the following:\n1 2 3 4 5 6 7 8 +-----------------------------------------------------------------------------------------------------------------+ | date | date_sent | person | body\t| |------------------------------------------------------------------------------------------------------------------ | 2013-10-20 13:48:18 | 2013-10-20 13:48:16 |\t54\t| Hello Christos! How are you?\t| | 2013-10-20 16:34:03 | 1970-01-01 02:00:00 | | Fine, thanks ! I configure the left MFD of a F-16 jet | | 2013-10-20 16:40:02 | 2013-10-20 16:40:01 |\t54\t| Awesome! I am throwing a party tomorrow at 21:45!\t| | 2013-10-20 17:15:15 | 1970-01-01 02:00:00 |\t| Thanks! I will be there!\t| +-----------------------------------------------------------------------------------------------------------------+ Note that person with id 54 is my friend Kitsos and person = NULL is me!\nGood luck tampering and hacking with your Android device !\n","date":"2013-10-28T23:22:48+02:00","permalink":"https://manios.org/2013/10/28/read-sms-directly-from-sqlite-database-in-android/","title":"Read SMS directly from Sqlite database in Android"},{"content":"Today I wanted to collect disk usage information and send them to a remote server script. I used df command to collect disk usage information and Python to send them to a remote server. Note that the remote server script uses basic authentication. So, here is the script which does the dirty job:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 #!/usr/bin/env python # -*- coding: utf-8 -*- # used the following links: # http://docs.python.org/2/library/re.html # http://docs.python.org/2/library/httplib.html # http://mozgovipc.blogspot.gr/2012/06/python-http-basic-authentication-with.html # http://stackoverflow.com/questions/4760215/running-shell-command-from-python-and-capturing-the-output import sys import subprocess import re import httplib import urllib import base64 import logging authUser = \u0026#34;myuser\u0026#34; authPass = \u0026#34;mypadd\u0026#34; logFile = \u0026#34;./sendDiskData.log\u0026#34; def main(): # initialise logging logging.basicConfig(level=logging.DEBUG, format=\u0026#39;%(asctime)s %(levelname)s %(message)s\u0026#39;, filename=logFile, filemode=\u0026#39;w\u0026#39;) # execute df -H p = subprocess.Popen([\u0026#39;df\u0026#39;, \u0026#39;-H\u0026#39;], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # get stdout and stderr out, err = p.communicate() # find /dev/vda1 mato = re.findall(r\u0026#39;/dev/[sv]da.*\u0026#39;,out,re.MULTILINE) # execute hostname p = subprocess.Popen([\u0026#39;hostname\u0026#39;], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # get stdout and stderr out, err = p.communicate() # get Hostname hostName = out # check if object is None (null) if mato: diskData = re.sub(r\u0026#39;^(.dev..da[^ \\t]*)[ \\t]*([0-9]*[.]*[0-9]*[MKG])[ \\t]*([0-9]*[.]*[0-9]*[MKG])[ \\t]*([0-9]*[.]*[0-9]*[MKG])[ \\t]*([0-9]*[.]*[0-9]*[%]).*\u0026#39;,r\u0026#39;\\1,\\2,\\3,\\4,\\5\u0026#39;,mato[0]) # create basic authentication header auth = base64.encodestring(\u0026#39;%s:%s\u0026#39; % (authUser, authPass)).replace(\u0026#39;\\n\u0026#39;, \u0026#39;\u0026#39;) # create request params params = urllib.urlencode({\u0026#39;hostname\u0026#39;: hostName, \u0026#39;diskdata\u0026#39;: diskData}) # add Basic Authentication Header headers = {\u0026#34;Authorization\u0026#34;: \u0026#34;Basic %s\u0026#34; % auth} # create url string diavUrl = \u0026#34;/clusterdisks.php?%s\u0026#34; % params try: conn = httplib.HTTPConnection(\u0026#34;manios.org\u0026#34;) # perform request conn.request(\u0026#34;GET\u0026#34;, diavUrl , None, headers) response = conn.getresponse() logging.info(\u0026#34;Disk data sent. Code:%s , Reason:%s\u0026#34;,response.status, response.reason) except HTTPException : #logging.error(\u0026#34;Failed to send disk data of %s: with error : %s\u0026#34;,hostName,e1) logging.error(\u0026#34;Failed to send disk data of %s\u0026#34;,hostName) sys.exit(1) # Exit with success sys.exit(0) if __name__ == \u0026#39;__main__\u0026#39;: main() ","date":"2013-08-13T09:12:13+03:00","permalink":"https://manios.org/2013/08/13/linux-get-hard-disk-statistics-using-python/","title":"Linux get hard disk statistics using Python"},{"content":"Updates:\n2013-08-08: Added sec=ntlm 2014-09-04: Added domain=mydomain parameter in order to connect to Active Directory domain Today I wanted to mount a Windows shared directory on my Ubuntu Linux machine. Note that the Windows machine is connected to an Active directory domain. After some searching in the web and trying various solutions, I stumbled upon an answer in Ubuntu forums. I finally managed to mount the Windows directory using mount command and having installed Linux cifs utilities.\nImportant note: Verify that your Windows shared directory has read access or write access for the user you are connecting with.\nSo in order to get your job done follow these steps:\nDetermine your Windows share directory information In my case they are: Active Directory machine name: manios.mycompany.local IP: 192.168.1.75 Shared Directory: books Active Directory username: manios Active Directory password: maniospassword Install Linux cifs utils 1 sudo apt-get install cifs-utils Create a mount directory 1 mkdir /media/bobos Mount Windows shared directory to your local directory Mount the directory with read / write permissions. You can use either your domain as an address or you IP: 1 sudo mount -t cifs //manios.mycompany.local/books /media/bobos/ -o username=manios,rw,password=maniospassword,domain=mydomain OR mount the directory with read only permissions: 1 sudo mount -t cifs //manios.mycompany.local/books /media/bobos/ -o username=manios,password=maniospassword,domain=mydomain If the Windows host is not connected in an active directory domain and you log in with your local account, so you have to add sec=ntlm into -o parameter (according to this StackOverflow answer. So the command looks like this: 1 sudo mount -t cifs //manios.mycompany.local/books /media/bobos/ -o username=manios,password=maniospassword,sec=ntlm Congratulations! You are complete! Now if you want to unmount the directory:\nRun umount command:\n1 umount /media/bobos/ Remove local directory\n1 rm -f /media/bobos/ I hope you find this guide useful!\n","date":"2013-08-06T16:32:28+03:00","permalink":"https://manios.org/2013/08/06/linux-mount-windows-shared-directory-via-cifs-and-samba/","title":"Linux mount Windows shared directory via cifs and samba"},{"content":"UPDATE (2015-05-13): Confirming that the following approach works for Lubuntu versions 12.10, 14.04 and 14.10 and Linux Mint 15 and 17.\nToday I wanted to install VMware tools in a Linux Mint 15 and in a Lubuntu virtual machine. I followed the standard procedure as it is described in this VMWare knowledge base article. After setting gcc path I received the following error:\n1 run vmware-config-tools.pl, got same error message:**Searching for a valid kernel header path... The path \u0026#34;\u0026#34; is not valid. Would you like to change it? After an hour of searching and trying numerous solutions I found the solution in this AskUbuntu answer.So before executing sudo ./vmware-install.pl follow these steps:\nDownload linux headers using the command:\n1 sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install build-essential linux-headers-$(uname -r) Create a symbolic link to version.h file: The vmware tools installer is looking for the version.h file in under [kernelsource path]/include/linux/version.h, it\u0026rsquo;s not there. The location of version.h is [kernelsource path]include/generated/uapi/linux/version.h\n1 sudo ln -s /usr/src/linux-headers-$(uname -r)/include/generated/uapi/linux/version.h /usr/src/linux-headers-$(uname -r)/include/linux/versionh INFO: In some newer operating systems like Debian 9 the directory /usr/src/linux-headers-$(uname -r)/include/ does not exist, so we need to create it. (info courtesy of Eli Godoy)\nNow you are able to install VMware tools without a problem! Good luck!\n","date":"2013-06-12T08:57:35+03:00","permalink":"https://manios.org/2013/06/12/fix-vmware-tools-kernel-header-path-is-not-valid-error/","title":"Fix Vmware tools \" kernel header path is not valid \" error"},{"content":"Today I wanted to to set the Color Scheme in vi / vim in my Linux. The default configuration looks ugly:\nI wanted to look like this:\nSo after some searching in the web I found an article in talkbinary and I did the following:\nCreate in your home directory a file called .exrc 1 touch ~/.exrc Open this file and insert the following parameters: 1 2 3 set number syntax on colorscheme evening Save the file and you are done! Now you have a nice colourfull VI editor ","date":"2013-04-23T16:34:21+03:00","permalink":"https://manios.org/2013/04/23/vi-set-color-scheme-line-numbers-and-syntax-highlighting/","title":"Vi set color scheme , line numbers and syntax highlighting"},{"content":"Today I had to configure a MySQL database cluster and I faced the following error\nERROR 1129 (HY000): Host '192.168.1.12' is blocked because of many connection errors; unblock with 'mysqladmin flush-hosts' while I was trying to access a MySQL instance via HaProxy. After some searching in dba.stackexchange.com, I found that my failed connection attempts had surpassed the max_connect_errors variable which is set by default to 10. In order to be able to connect to the cluster again you have to follow these steps in every MySQL server that belongs to the cluster:\nOpen a command prompt (or shell in Linux) with administrative privilleges\nIf you are in Windows set character set to unicode. Linux is using UTF-8 by default.\nchcp 65001 Flush all hosts in MySQL using mysqladmin: mysqladmin flush-hosts -u root -p Open my.cnf (Linux) or my.ini (Windows) and change max_connect_errors variable to a large number. I used: max_connect_errors= 1000000 Restart MySQL server Linux: sudo /etc/init.d/mysql restart Windows: net stop mysql net start mysql You are done!\n","date":"2013-04-19T10:24:58+03:00","permalink":"https://manios.org/2013/04/19/mysql-connect-error-1129/","title":"MySQL connection ERROR 1129"},{"content":"Today I had to import a very large SQL dump file (6 Gb) to a MySQL database using windows command line. If you are using linux it is the same. The process is the following:\nOpen a command prompt (or shell in Linux) with administrative privilleges If you are in Windows set character set to unicode. Linux is using UTF-8 by default. 1 chcp 65001 Connect to a mysql instance using command line 1 $PATH_TO_MYSQL\\mysql.exe -h 192.168.1.1 --port=3306 -u root -p if you are in localhost you do not need host and port 1 $PATH_TO_MYSQL\\mysql.exe -u root -p You are now in mysql shell. Set network buffer length to a large byte number. The default value may throw errors for such large data files 1 set global net_buffer_length=1000000; Set maximum allowed packet size to a large byte number.The default value may throw errors for such large data files. 1 set global max_allowed_packet=1000000000; Disable foreign key checking to avoid delays,errors and unwanted behaviour 1 2 3 SET foreign_key_checks = 0; SET UNIQUE_CHECKS = 0; SET AUTOCOMMIT = 0; Import your sql dump file 1 source C:\\bob_db\\dbdump150113.sql You are done! Remember to enable foreign key checks when procedure is complete! 1 2 3 SET foreign_key_checks = 1; SET UNIQUE_CHECKS = 1; SET AUTOCOMMIT = 1; If you are in Linux you can create a Bash script which will do the dirty job and write to stdout start and end time of import:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #!/bin/sh # store start date to a variable imeron=`date` echo \u0026#34;Import started: OK\u0026#34; dumpfile=\u0026#34;/home/bob/bobiras.sql\u0026#34; ddl=\u0026#34;set names utf8; \u0026#34; ddl=\u0026#34;$ddl set global net_buffer_length=1000000;\u0026#34; ddl=\u0026#34;$ddl set global max_allowed_packet=1000000000; \u0026#34; ddl=\u0026#34;$ddl SET foreign_key_checks = 0; \u0026#34; ddl=\u0026#34;$ddl SET UNIQUE_CHECKS = 0; \u0026#34; ddl=\u0026#34;$ddl SET AUTOCOMMIT = 0; \u0026#34; # if your dump file does not create a database, select one ddl=\u0026#34;$ddl USE jetdb; \u0026#34; ddl=\u0026#34;$ddl source $dumpfile; \u0026#34; ddl=\u0026#34;$ddl SET foreign_key_checks = 1; \u0026#34; ddl=\u0026#34;$ddl SET UNIQUE_CHECKS = 1; \u0026#34; ddl=\u0026#34;$ddl SET AUTOCOMMIT = 1; \u0026#34; ddl=\u0026#34;$ddl COMMIT ; \u0026#34; echo \u0026#34;Import started: OK\u0026#34; time mysql -h 127.0.0.1 -u root -proot -e \u0026#34;$ddl\u0026#34; # store end date to a variable imeron2=`date` echo \u0026#34;Start import:$imeron\u0026#34; echo \u0026#34;End import:$imeron2\u0026#34; ","date":"2013-03-19T17:05:00+02:00","permalink":"https://manios.org/2013/03/19/import-a-large-sql-dump-file-to-a-mysql-database-from-command-line/","title":"Import a large sql dump file to a MySQL database from command line"},{"content":"Επιτέλους! (at last! in greek) After a month and a half of waiting, I received my Raspberry Pi! Now I am going to show you how to install Raspbian Wheezy Linux into your Raspberry pi! To complete this task I used Lubuntu Linux.\n1. Get an SD card The SD card will be your hard drive! It must be at least 2Gb in size. Raspberry organisation recommends 4Gb.\n2. Download Raspbian Wheezy You can download Raspbian Wheezy Linux from here. and unzip it to a directory.\n3. Insert your SD card and get mount information When you insert your SD card the system asks you whether you want to mount it or not. Mount it, so you can explore its contents from the file explorer.\n4. Run df to find mount point and device name Run df command and find your device in /dev directory.For me it was /dev/mmcblk0. The output is goint to appear as the following:\n1 2 3 4 5 6 7 8 9 linux@ubuntu:~$ df Filesystem 1K-blocks Used Available Use% Mounted on /dev/loop0 25948028 13156440 11473496 54% / udev 1018844 4 1018840 1% /dev tmpfs 410444 872 409572 1% /run none 5120 0 5120 0% /run/lock /dev/sda6 42074200 35229872 6844328 84% /host none 1026108 0 1026108 0% /run/shm \u0026lt;strong\u0026gt;/dev/mmcblk0 1946896 4 1946892 1% /medi/mysdcard\u0026lt;/strong\u0026gt; 5. Unmount the SD card 1 umount /media/mysdcard 6. Format SD card in FAT 32 1 sudo mkdosfs -F 32 -v /dev/mmcblk0 7. Copy .img file to SD card using dd 1 sudo dd bs=1M if=\u0026#34;/home/linux/2013-02-09-wheezy-raspbian.img\u0026#34; of=/dev/mmcblk0 When procedure finishes you are ready! Unplug your SD card from your computer and insert it into your Raspberry Pi. You can find more installation information in Raspberry Pi Quick Start Guide\n","date":"2013-03-10T13:09:32+02:00","permalink":"https://manios.org/2013/03/10/install-raspbian-wheezy-linux-to-raspberry-pi-using-linux/","title":"Install Raspbian Wheezy Linux to Raspberry Pi using Linux"},{"content":"I have installed XAMPP in my Windows machine and I want to alter the timezone in Apache HTTP server, PHP module and MySql in order to see proper dates in server error.log, access.log and php_error_log output.\nAfter some searching I found the solution:\nIn httpd.conf (\\xampp\\apache\\conf\\httpd.conf) , add the following line: 1 2 # Set timezone to Europe/Athens UTC+02:00 SetEnv TZ Europe/Athens Edit php.ini (\\xampp\\php\\php.ini) date.timezone value in [Date] section: 1 date.timezone = \u0026#34;Europe/Athens\u0026#34; You can find the available PHP timezones in http://php.net/date.timezone\nIn my.ini (\\xampp\\mysql\\bin\\my.ini) add or replace 1 default-time-zone = \u0026#34;Europe/Athens\u0026#34; Now restart Apache HTTP and MySQL server and you are done!\n","date":"2013-03-05T14:36:24+02:00","permalink":"https://manios.org/2013/03/05/change-timezone-to-apache-server-and-php/","title":"Change timezone to Apache server , PHP and MySql in XAMPP"},{"content":"Today I wanted to make a mail notification mechanism for a project of mine. After some web searching I stumbled upon a mkyong.com tutorial where he uses SMTP with Java Mail API to send emails via Gmail. I modified the code in some places to make it more general and API friendly. Now I can send emails easily using different authentication and encryption methods (TLS,SSL)! I hope you find it useful!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 import java.util.Properties; import javax.mail.Message; import javax.mail.MessagingException; import javax.mail.PasswordAuthentication; import javax.mail.Session; import javax.mail.Transport; import javax.mail.internet.InternetAddress; import javax.mail.internet.MimeMessage; /** * * Simple SMTP mail class built using the example from: \u0026lt;a * href=\u0026#34;http://www.mkyong.com/java/javamail-api-sending-email-via-gmail-smtp-example\u0026#34;\u0026gt;mkyong.com\u0026lt;/a\u0026gt; * * \u0026lt;p\u0026gt;If you get {@link java.net.UnknownHostException}: smtp.gmail.com , try * ping smtp.gmail.com and make sure you got a response (able to access). Often * times, your connection may block by your firewall or proxy behind.\u0026lt;/p\u0026gt; */ public class MailSender { private String userName; private String password; private Properties props; private final static String CONTENT_CHARSET = \u0026#34;text/html; charset=utf-8\u0026#34;; public MailSender(Properties props) { this.userName = (String) props.get(\u0026#34;userName\u0026#34;); this.password = (String) props.get(\u0026#34;password\u0026#34;); this.props = props; } public void sendMail(String subject, String bodyText) throws MessagingException { sendMail(subject, bodyText, false); } public void sendMail(String subject, String bodyText, boolean isHtmlBody) throws MessagingException { Session session = Session.getDefaultInstance(props, new javax.mail.Authenticator() { @Override protected PasswordAuthentication getPasswordAuthentication() { return new PasswordAuthentication(userName, password); } }); Message message = new MimeMessage(session); // set from (sender) address message.setFrom(new InternetAddress((String) props.get(\u0026#34;fromAddress\u0026#34;))); // set recipients addresses (may be more than one) message.setRecipients(Message.RecipientType.TO, InternetAddress.parse((String) props.get(\u0026#34;recipientsAddresses\u0026#34;))); // set message subject message.setSubject(subject); // if it is html change multipart information if (isHtmlBody) { message.setContent(bodyText, CONTENT_CHARSET); } else { message.setText(bodyText); } Transport.send(message); System.out.println(\u0026#34;Done\u0026#34;); } public static void main(String[] args) throws MessagingException { Properties proper = new Properties(); // put credentials proper.put(\u0026#34;userName\u0026#34;, \u0026#34;user\u0026#34;); proper.put(\u0026#34;password\u0026#34;, \u0026#34;password\u0026#34;); // put server information proper.put(\u0026#34;mail.smtp.host\u0026#34;, \u0026#34;smtp.gmail.com\u0026#34;); proper.put(\u0026#34;mail.smtp.socketFactory.port\u0026#34;, \u0026#34;465\u0026#34;); proper.put(\u0026#34;mail.smtp.socketFactory.class\u0026#34;, \u0026#34;javax.net.ssl.SSLSocketFactory\u0026#34;); proper.put(\u0026#34;mail.smtp.auth\u0026#34;, \u0026#34;true\u0026#34;); proper.put(\u0026#34;mail.smtp.port\u0026#34;, \u0026#34;465\u0026#34;); // if you do not want to use authentication //proper.put(\u0026#34;mail.smtp.host\u0026#34;, \u0026#34;192.168.1.13\u0026#34;); //proper.put(\u0026#34;mail.smtp.socketFactory.port\u0026#34;, \u0026#34;25\u0026#34;); //proper.put(\u0026#34;mail.smtp.socketFactory.class\u0026#34;, \u0026#34;javax.net.ssl.SSLSocketFactory\u0026#34;); //proper.put(\u0026#34;mail.smtp.connectiontimeout\u0026#34;,30000); //proper.put(\u0026#34;mail.smtp.auth\u0026#34;, \u0026#34;false\u0026#34;); //proper.put(\u0026#34;mail.smtp.port\u0026#34;, \u0026#34;25\u0026#34;); // put sender information proper.put(\u0026#34;fromAddress\u0026#34;, \u0026#34;bob@gmail.com\u0026#34;); // put recipient information proper.put(\u0026#34;recipientsAddresses\u0026#34;, \u0026#34;bob@gmail.com\u0026#34;); new MailSender(proper).sendMail(\u0026#34;Bob\u0026#34;, \u0026#34;Some text body for bob\u0026#34;); } } And if you are using maven you can add the dependency to Java Mail API with the following code to your pom.xml. For me, the latest version is 1.4.6\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;javax.mail\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mail\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.4.6\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; ","date":"2013-02-27T16:45:15+02:00","permalink":"https://manios.org/2013/02/27/java-send-smtp-mail-using-gmail/","title":"Java send smtp mail using Gmail"},{"content":"Unfortunately, Java does not include a proper String manipulation class in its Java SE SDK. I really do not understand why\u0026hellip; However, we see so many implementations and libraries including the distinguished Apache Commons Lang and android.text package. If your project faces a string war (war is hell!) you have to add dependencies or create your own class!\nSo today I wrote down a simple String utility class with common methods which has no external dependencies. As you can see some methods are borrowed from apache.commons.lang and android.text. I hope you find it useful! Update 2013-05-02: Added join methods\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 /** * A utility class which contains common and useful String manipulation methods. * Wrap and Join methods are taken from \u0026lt;a href= * \u0026#34;http://commons.apache.org/lang/api-2.6/org/apache/commons/lang/WordUtils.html\u0026#34; * \u0026gt;org.apache.commons.lang.WordUtils\u0026lt;/a\u0026gt; * \u0026lt;p\u0026gt; * isEmpty() method is taken from \u0026lt;a * href=\u0026#34;http://developer.android.com/reference/android/text/TextUtils.html\u0026#34; * \u0026gt;android.text.TextUtils\u0026lt;/a\u0026gt; * */ public class StringUtils { /** * Returns true if the string is null or 0-length. * * @param str * the string to be examined * @return true if str is null or zero length */ public static boolean isEmpty(CharSequence str) { if (str == null || str.length() == 0) return true; else return false; } /** * Replaces a null String with an empty string (\u0026#34;\u0026#34;) * * @param str * source string */ public static String replaceNullWithEmpty(String str) { return isEmpty(str) ? \u0026#34;\u0026#34; : str; } /** * Replaces an empty string (\u0026#34;\u0026#34;) with null * * @param str * source string */ public static String replaceEmptyWithNull(String str) { return isEmpty(str) ? null : str; } /** * Truncates s to fit within len. If s is an empty string, it will return an * empty string. If s is null, null is returned. * * @param s * source string * @param len * number of characters we want **/ public static String truncate(String s, int len) { if (s == null) { return null; } if (s.equals(\u0026#34;\u0026#34;)) { return \u0026#34;\u0026#34;; } return s.substring(0, Math.min(len, s.length())); } /** * \u0026lt;p\u0026gt; * Wraps a single line of text, identifying words by \u0026lt;code\u0026gt;\u0026#39; \u0026#39;\u0026lt;/code\u0026gt;. * \u0026lt;/p\u0026gt; * * \u0026lt;p\u0026gt; * New lines will be separated by the system property line separator. Very * long words, such as URLs will \u0026lt;i\u0026gt;not\u0026lt;/i\u0026gt; be wrapped. * \u0026lt;/p\u0026gt; * * \u0026lt;p\u0026gt; * Leading spaces on a new line are stripped. Trailing spaces are not * stripped. * \u0026lt;/p\u0026gt; * * \u0026lt;pre\u0026gt; * StringUtils.wrap(null, *) = null * StringUtils.wrap(\u0026#34;\u0026#34;, *) = \u0026#34;\u0026#34; * \u0026lt;/pre\u0026gt; * * This method is a copy of org.apache.commons.lang.WordUtils.wrap() method * * @param str * the String to be word wrapped, may be null * @param wrapLength * the column to wrap the words at, less than 1 is treated as 1 * @return a line with newlines inserted, \u0026lt;code\u0026gt;null\u0026lt;/code\u0026gt; if null input */ public static String wrap(String str, int wrapLength) { return wrap(str, wrapLength, null, false); } /** * \u0026lt;p\u0026gt; * Wraps a single line of text, identifying words by \u0026lt;code\u0026gt;\u0026#39; \u0026#39;\u0026lt;/code\u0026gt;. * \u0026lt;/p\u0026gt; * * \u0026lt;p\u0026gt; * Leading spaces on a new line are stripped. Trailing spaces are not * stripped. * \u0026lt;/p\u0026gt; * * \u0026lt;pre\u0026gt; * StringUtils.wrap(null, *, *, *) = null * StringUtils.wrap(\u0026#34;\u0026#34;, *, *, *) = \u0026#34;\u0026#34; * \u0026lt;/pre\u0026gt; * * This method is a copy of org.apache.commons.lang.WordUtils.wrap() method * * @param str * the String to be word wrapped, may be null * @param wrapLength * the column to wrap the words at, less than 1 is treated as 1 * @param newLineStr * the string to insert for a new line, \u0026lt;code\u0026gt;null\u0026lt;/code\u0026gt; uses * the system property line separator * @param wrapLongWords * true if long words (such as URLs) should be wrapped * @return a line with newlines inserted, \u0026lt;code\u0026gt;null\u0026lt;/code\u0026gt; if null input */ public static String wrap(String str, int wrapLength, String newLineStr, boolean wrapLongWords) { if (str == null) { return null; } if (newLineStr == null) { newLineStr = \u0026#34;\\n\u0026#34;; } if (wrapLength \u0026lt; 1) { wrapLength = 1; } int inputLineLength = str.length(); int offset = 0; StringBuffer wrappedLine = new StringBuffer(inputLineLength + 32); while ((inputLineLength - offset) \u0026gt; wrapLength) { if (str.charAt(offset) == \u0026#39; \u0026#39;) { offset++; continue; } int spaceToWrapAt = str.lastIndexOf(\u0026#39; \u0026#39;, wrapLength + offset); if (spaceToWrapAt \u0026gt;= offset) { // normal case wrappedLine.append(str.substring(offset, spaceToWrapAt)); wrappedLine.append(newLineStr); offset = spaceToWrapAt + 1; } else { // really long word or URL if (wrapLongWords) { // wrap really long word one line at a time wrappedLine.append(str.substring(offset, wrapLength + offset)); wrappedLine.append(newLineStr); offset += wrapLength; } else { // do not wrap really long word, just extend beyond limit spaceToWrapAt = str.indexOf(\u0026#39; \u0026#39;, wrapLength + offset); if (spaceToWrapAt \u0026gt;= 0) { wrappedLine .append(str.substring(offset, spaceToWrapAt)); wrappedLine.append(newLineStr); offset = spaceToWrapAt + 1; } else { wrappedLine.append(str.substring(offset)); offset = inputLineLength; } } } } // Whatever is left in line is short enough to just pass through wrappedLine.append(str.substring(offset)); return wrappedLine.toString(); } /** * \u0026lt;p\u0026gt; * Joins the elements of the provided array into a single String containing * the provided list of elements. * \u0026lt;/p\u0026gt; * * \u0026lt;p\u0026gt; * No delimiter is added before or after the list. Null objects or empty * strings within the array are represented by empty strings. * \u0026lt;/p\u0026gt; * * \u0026lt;pre\u0026gt; * StringUtils.join(null, *) = null * StringUtils.join([], *) = \u0026#34;\u0026#34; * StringUtils.join([null], *) = \u0026#34;\u0026#34; * StringUtils.join([\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;], \u0026#39;;\u0026#39;) = \u0026#34;a;b;c\u0026#34; * StringUtils.join([\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;], null) = \u0026#34;abc\u0026#34; * StringUtils.join([null, \u0026#34;\u0026#34;, \u0026#34;a\u0026#34;], \u0026#39;;\u0026#39;) = \u0026#34;;;a\u0026#34; * \u0026lt;/pre\u0026gt; * * @param array * the array of values to join together, may be null * @param separator * the separator character to use * @return the joined String, \u0026lt;code\u0026gt;null\u0026lt;/code\u0026gt; if null array input * @since 2.0 */ public static String join(long[] array, char separator) { if (array == null) { return null; } return join(array, separator, 0, array.length); } /** * \u0026lt;p\u0026gt; * Joins the elements of the provided array into a single String containing * the provided list of elements. * \u0026lt;/p\u0026gt; * * \u0026lt;p\u0026gt; * No delimiter is added before or after the list. Null objects or empty * strings within the array are represented by empty strings. * \u0026lt;/p\u0026gt; * * \u0026lt;pre\u0026gt; * StringUtils.join(null, *) = null * StringUtils.join([], *) = \u0026#34;\u0026#34; * StringUtils.join([null], *) = \u0026#34;\u0026#34; * StringUtils.join([\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;], \u0026#39;;\u0026#39;) = \u0026#34;a;b;c\u0026#34; * StringUtils.join([\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;], null) = \u0026#34;abc\u0026#34; * StringUtils.join([null, \u0026#34;\u0026#34;, \u0026#34;a\u0026#34;], \u0026#39;;\u0026#39;) = \u0026#34;;;a\u0026#34; * \u0026lt;/pre\u0026gt; * * @param array * the array of values to join together, may be null * @param separator * the separator character to use * @param startIndex * the first index to start joining from. It is an error to pass * in an end index past the end of the array * @param endIndex * the index to stop joining from (exclusive). It is an error to * pass in an end index past the end of the array * @return the joined String, \u0026lt;code\u0026gt;null\u0026lt;/code\u0026gt; if null array input * @since 2.0 */ public static String join(long[] array, char separator, int startIndex, int endIndex) { if (array == null) { return null; } int bufSize = (endIndex - startIndex); if (bufSize \u0026lt;= 0) { return \u0026#34;\u0026#34;; } StringBuffer buf = new StringBuffer(bufSize); for (int i = startIndex; i \u0026lt; endIndex; i++) { if (i \u0026gt; startIndex) { buf.append(separator); } buf.append(array[i]); } return buf.toString(); } /** * \u0026lt;p\u0026gt; * Joins the elements of the provided array into a single String containing * the provided list of elements. * \u0026lt;/p\u0026gt; * * \u0026lt;p\u0026gt; * No delimiter is added before or after the list. Null objects or empty * strings within the array are represented by empty strings. * \u0026lt;/p\u0026gt; * * \u0026lt;pre\u0026gt; * StringUtils.join(null, *) = null * StringUtils.join([], *) = \u0026#34;\u0026#34; * StringUtils.join([null], *) = \u0026#34;\u0026#34; * StringUtils.join([\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;], \u0026#39;;\u0026#39;) = \u0026#34;a;b;c\u0026#34; * StringUtils.join([\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;], null) = \u0026#34;abc\u0026#34; * StringUtils.join([null, \u0026#34;\u0026#34;, \u0026#34;a\u0026#34;], \u0026#39;;\u0026#39;) = \u0026#34;;;a\u0026#34; * \u0026lt;/pre\u0026gt; * * @param array * the array of values to join together, may be null * @param separator * the separator character to use * @return the joined String, \u0026lt;code\u0026gt;null\u0026lt;/code\u0026gt; if null array input * @since 2.0 */ public static String join(Object[] array, char separator) { if (array == null) { return null; } return join(array, separator, 0, array.length); } /** * \u0026lt;p\u0026gt; * Joins the elements of the provided array into a single String containing * the provided list of elements. * \u0026lt;/p\u0026gt; * * \u0026lt;p\u0026gt; * No delimiter is added before or after the list. Null objects or empty * strings within the array are represented by empty strings. * \u0026lt;/p\u0026gt; * * \u0026lt;pre\u0026gt; * StringUtils.join(null, *) = null * StringUtils.join([], *) = \u0026#34;\u0026#34; * StringUtils.join([null], *) = \u0026#34;\u0026#34; * StringUtils.join([\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;], \u0026#39;;\u0026#39;) = \u0026#34;a;b;c\u0026#34; * StringUtils.join([\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;], null) = \u0026#34;abc\u0026#34; * StringUtils.join([null, \u0026#34;\u0026#34;, \u0026#34;a\u0026#34;], \u0026#39;;\u0026#39;) = \u0026#34;;;a\u0026#34; * \u0026lt;/pre\u0026gt; * * @param array * the array of values to join together, may be null * @param separator * the separator character to use * @param startIndex * the first index to start joining from. It is an error to pass * in an end index past the end of the array * @param endIndex * the index to stop joining from (exclusive). It is an error to * pass in an end index past the end of the array * @return the joined String, \u0026lt;code\u0026gt;null\u0026lt;/code\u0026gt; if null array input * @since 2.0 */ public static String join(Object[] array, char separator, int startIndex, int endIndex) { if (array == null) { return null; } int bufSize = (endIndex - startIndex); if (bufSize \u0026lt;= 0) { return \u0026#34;\u0026#34;; } bufSize *= ((array[startIndex] == null ? 16 : array[startIndex] .toString().length()) + 1); StringBuffer buf = new StringBuffer(bufSize); for (int i = startIndex; i \u0026lt; endIndex; i++) { if (i \u0026gt; startIndex) { buf.append(separator); } if (array[i] != null) { buf.append(array[i]); } } return buf.toString(); } } ","date":"2012-12-27T09:52:19+02:00","permalink":"https://manios.org/2012/12/27/java-string-util-class/","title":"Java String Util Class"},{"content":"Today I was trying to create and ODBC Data Source to a database using 32 bit database drivers. When I accessed ODBC Data Sources (32bit) from Control Panel, I could not create a data source using 32 bit drivers and I was facing two Driver not found errors which can be seen in the following screenshots:\nAfter searching in the web and asking my colleagues I found that when you want to use Microsoft ODBC Administrator for:\n64bit database drivers you have to run the 64bit version which is located in: C:\\Windows\\System32\\odbcad32.exe 32bit database drivers you have to run the 32bit version which is located in: C:\\Windows\\SysWOW64\\odbcad32.exe Both windows look identical but the underlying implementation is different.\n","date":"2012-09-26T14:52:39+03:00","permalink":"https://manios.org/2012/09/26/windows-7-64bit-odbc-data-sources/","title":"Windows 7 64bit ODBC Data sources"},{"content":"Today I wanted to send an email using SMTP from my gmail account. I searched the web and I found this and this and this which helped me a lot. So this is the vbscript:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 on error resume next Const schema = \u0026#34;http://schemas.microsoft.com/cdo/configuration/\u0026#34; Const cdoBasic = 1 Const cdoSendUsingPort = 2 Dim oMsg, oConf \u0026#39; E-mail properties Set oMsg = CreateObject(\u0026#34;CDO.Message\u0026#34;) oMsg.From = \u0026#34;from@gmail.com\u0026#34; \u0026#39; or \u0026#34;Sender Name \u0026lt;from@gmail.com\u0026gt;\u0026#34; oMsg.To = \u0026#34;test@test.gr\u0026#34; \u0026#39; or \u0026#34;Recipient Name \u0026lt;to@gmail.com\u0026gt;\u0026#34; oMsg.Subject = \u0026#34;Test from VBScript\u0026#34; oMsg.TextBody = \u0026#34;If you can read this, the script worked!\u0026#34; \u0026#39; GMail SMTP server configuration and authentication info Set oConf = oMsg.Configuration oConf.Fields(schema \u0026amp; \u0026#34;smtpserver\u0026#34;) = \u0026#34;smtp.gmail.com\u0026#34; \u0026#39;server address oConf.Fields(schema \u0026amp; \u0026#34;smtpserverport\u0026#34;) = 465 \u0026#39;port number oConf.Fields(schema \u0026amp; \u0026#34;sendusing\u0026#34;) = cdoSendUsingPort oConf.Fields(schema \u0026amp; \u0026#34;smtpauthenticate\u0026#34;) = cdoBasic \u0026#39;authentication type oConf.Fields(schema \u0026amp; \u0026#34;smtpusessl\u0026#34;) = True \u0026#39;use SSL encryption oConf.Fields(schema \u0026amp; \u0026#34;sendusername\u0026#34;) = \u0026#34;from@gmail.com\u0026#34; \u0026#39;sender username oConf.Fields(schema \u0026amp; \u0026#34;sendpassword\u0026#34;) = \u0026#34;passwordi\u0026#34; \u0026#39;sender password oConf.Fields.Update() \u0026#39; send message oMsg.Send() \u0026#39; Return status message If Err Then resultMessage = \u0026#34;ERROR \u0026#34; \u0026amp; Err.Number \u0026amp; \u0026#34;: \u0026#34; \u0026amp; Err.Description Err.Clear() Else resultMessage = \u0026#34;Message sent ok\u0026#34; End If Wscript.echo(resultMessage) ","date":"2012-09-12T09:32:55+03:00","permalink":"https://manios.org/2012/09/12/send-email-from-vbscript-using-smtp-and-gmail/","title":"Send email from VBScript using SMTP and Gmail"},{"content":"Today I wanted to delete the selected rows of a datawindow grid in Powerbuilder. When I say delete I wanted just to remove them from the screen and not from the database. I used deleteRow(long row) function but it was slow in large datasets. So I asked my departments head and he gave me a very nice and efficient approach:\n1 2 3 4 5 6 7 8 9 10 // move selected rows to Filter Buffer dw_vehicles.Setfilter(\u0026#34;NOT isselected()\u0026#34;) dw_vehicles.Filter() // move selected rows from Filter to Delete Buffer dw_vehicles.RowsMove(1,dw_vehicles_noowner.FilteredCount(), Filter!,dw_vehicles_noowner,1,Delete!) // clear filter dw_vehicles.Setfilter(\u0026#34;\u0026#34;) dw_vehicles.Filter() ","date":"2012-08-24T14:27:26+03:00","permalink":"https://manios.org/2012/08/24/powerbuilder-datawindow-grid-delete-selected-rows-efficiently/","title":"PowerBuilder Datawindow Grid delete selected rows efficiently"},{"content":"Today I wanted to explore Perl\u0026rsquo;s Unicode and regular expression capabilities, so I wrote down this simple script. It is quite amazing how simply Perl handles strings and regular expressions! Otherwise you have to use multiple sed or egrep commands with pipelines.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #!/usr/bin/perl use Encode; use utf8; # mercy in Greek my $bob = \u0026#34;\u0026lt;b\u0026gt;Έλεος\u0026lt;/b\u0026gt;\u0026#34;; # get the first argument of script and decode it to utf8 string my $telis = decode(\u0026#39;UTF-8\u0026#39;,$ARGV[0]); # beta \u0026#39;β\u0026#39; letter my $ter = ord(\u0026#39;β\u0026#39;); $ter+=4; my $arithmouba = 2; $arithmouba = $arithmouba \u0026lt;\u0026lt; 3; # convert number back to letter $ter = chr($ter); # regular expression substitution $bob =~ s/\u0026lt;b\u0026gt;/\u0026lt;b\u0026gt;\\n/g; # encode output to utf8 $bob = encode(\u0026#39;UTF-8\u0026#39;, $bob); $ter = encode(\u0026#39;UTF-8\u0026#39;, $ter); print \u0026#34;$bob\\n$telis\\n$ter\\n$arithmouba\\n\u0026#34;; ","date":"2012-08-15T14:12:00+03:00","permalink":"https://manios.org/2012/08/15/perl-utf-8-and-regular-expressions/","title":"Perl UTF-8 and Regular Expressions"},{"content":"Today I wondered if there is an algorithm to calculate the date of Orthodox Easter. After some searching in the web I found the algorithm and converted it to Java code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 /** * @param myear * the year of which we want to get the Orthodox Easter date * @return the date of the Orthodox easter of the given year */ public Calendar getOrthodoxEaster(int myear) { Calendar dof = Calendar.getInstance(); int r1 = myear % 4; int r2 = myear % 7; int r3 = myear % 19; int r4 = (19 * r3 + 15) % 30; int r5 = (2 * r1 + 4 * r2 + 6 * r4 + 6) % 7; int mdays = r5 + r4 + 13; if (mdays \u0026gt; 39) { mdays = mdays - 39; dof.set(myear, 4, mdays); } else if (mdays \u0026gt; 9) { mdays = mdays - 9; dof.set(myear, 3, mdays); } else { mdays = mdays + 22; dof.set(myear, 2, mdays); } return dof; } ","date":"2012-08-14T12:45:31+03:00","permalink":"https://manios.org/2012/08/14/java-calculate-orthodox-easter-date/","title":"Java Calculate Orthodox Easter Date"},{"content":"Updated: 2015-09-08\nChange keyboard layout to support both English (US) and Greek (GR) 1 setxkbmap -option grp:alt_shift_toggle us,gr Audjust monitor brightness and gamma 1 2 3 4 5 # make it darker xrandr --output LVDS1 --brightness 0.4 --gamma 1.6:1.6:1.6 # restore to default brightness and gamma xrandr --output LVDS1 --brightness 1 --gamma 1:1:1 Find top 10 largest files in / 1 2 3 4 sudo du -a / | sort -n -r | head -n 10 # or in human readable format cd / ; sudo du -hsx * | sort -rh | head -10 Source: cybercity\nFind IP without using ifconfig 1 netstat -n -t | awk \u0026#39;{print $4}\u0026#39; | grep -o \u0026amp;quot;[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*\u0026amp;quot; |grep -v \u0026amp;quot;127.0.0.1\u0026amp;quot; | sort -u Run logrotate 1 logrotate -f /etc/logrotate.conf Curl get http code and store it to Bash variable 1 bob=`curl -L -s -f -w \u0026amp;quot;%{http_code}\u0026amp;quot; --output /dev/null \u0026amp;quot;http://www.google.gr\u0026amp;quot;` Curl GET with url encode in BASH 1 curl -G \u0026amp;quot;$urlGet\u0026amp;quot; --data-urlencode \u0026amp;quot;date=$datos\u0026amp;quot; --data-urlencode \u0026amp;quot;diskdata=$diskuse\u0026amp;quot; Curl basic authentication 1 curl -u \u0026amp;quot;myusername:mypass\u0026amp;quot; \u0026amp;quot;http://www.google.com\u0026amp;quot; Url Encode String with Perl from Bash 1 adaUrlEncoded=`perl -e \u0026#39; use URI::Escape; print uri_escape(\u0026amp;quot;$row\u0026amp;quot;);\u0026#39;` Cron Job every five minutes with sterr redirection 1 0,5,10,15,20,25,30,35,40,45,50,50,55 * * * * /home/bob/sendServerData.py \u0026amp;gt;/dev/null 2\u0026amp;gt;\u0026amp;amp;1 More examples in Cyberciti tutorial\nAdd a GPG key to apt manually Lets say we want to add MongoDB GPG key to apt in order to install MongoDB in Debian Linux. We download the key from here or copy it to a file, lets say 10gen-gpg-key.asc. Then run:\n1 cat 10gen-gpg-key.asc | sudo apt-key add - **Change your default locale ** Edit your locale variable values using:\n1 sudo vi /etc/default/locale Merge PDF files Append multiple pdf files (source*.pdf) to one (alltogether.pdf) using the following command. NOTE: The last document is the generated destination pdf document where the rest will be appended.\n1 pdfunite source1.pdf source2.pdf source3.pdf alltogether.pdf Sort Nginx access log by Response Codes 1 sudo awk \u0026#39;{print $9}\u0026#39; /var/log/nginx/ssl.access.log | sort | uniq -c | sort -r Source: rtcamp\nCount IP requests from Nginx access log 1 sudo awk \u0026#39;{print $1}\u0026#39; /var/log/nginx/ssl.access.log | sort | uniq -c | sort -nr Source: Mkyong\nFast Delete files modified before 10 minutes 1 time find $i/*.tmp -mmin +10 -type f -delete Source: slashroot\nRemove newline character (\u0026rsquo;\\n\u0026rsquo;) from a file 1 tr \u0026#39;\\n\u0026#39; \u0026#39; \u0026#39; \u0026lt; input_filename or more hackish\n1 sed \u0026#39;:a;N;$!ba;s/\\n/ /g\u0026#39; Source: StackOverflow\nView Apache requests per minute (from logs) 1 grep \u0026#34;23/Jan/2013:06\u0026#34; example.com | cut -d[ -f2 | cut -d] -f1 | awk -F: \u0026#39;{print $2\u0026#34;:\u0026#34;$3}\u0026#39; | sort -nk1 -nk2 | uniq -c | awk \u0026#39;{ if ($1 \u0026gt; 10) print $0}\u0026#39; Source: inmotionhost\nRedirect script output to syslog (while it is in crontab)\n1 0 1,9,17 * * * myscript.sh \u0026amp;2\u0026gt;1 | /usr/bin/logger -t myscript Grep Solr logs for errors\n1 egrep \u0026#34;^(ERROR|[[:blank:]])\u0026#34; solr.log.1 | egrep -v \u0026#34;^[[:blank:]]commit\u0026#34; \u0026gt; 20170515001700-errorlog.txt Create an SSH tunnel to a remote VM hosting MySQL server\n1 2 # This will create a tunnel which attaches to port 3306 of the remote server. To use it, we connect to port 3307 of our local machine. ssh -N -L 3307:localhost:3306 mysql.manios.org -p 3022 ","date":"2012-08-10T00:27:29+03:00","permalink":"https://manios.org/2012/08/10/ubuntu-linux-useful-commands/","title":"Ubuntu Debian Linux useful commands"},{"content":"Today I wanted to download a file using Http protocol but before that determine its size. First aff all I open an Http connection and get file size using Content-Length header (see getFileSize method). Then I download the binary file. You can download the source code from here.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 import java.io.ByteArrayOutputStream; import java.io.File; import java.io.FileOutputStream; import java.io.IOException; import java.io.InputStream; import java.net.HttpURLConnection; import java.net.MalformedURLException; import java.net.URL; import java.net.URLConnection; import org.slf4j.Logger; import org.slf4j.LoggerFactory; /** * A simple utility class which provides methods for file downloading using HTTP * protocol. */ public class FileDownloader { private static final Logger logger = LoggerFactory .getLogger(FileDownloader.class); /** * Gets file size using an HTTP connection with GET method * * @return file size in bytes * @throws IOException */ public static long getFileSize(String fileUrl) throws IOException { URL oracle = new URL(fileUrl); HttpURLConnection yc = (HttpURLConnection) oracle.openConnection(); populateDesktopHttpHeaders(yc); long fileSize = 0; try { // retrieve file size from Content-Length header field fileSize = Long.parseLong(yc.getHeaderField(\u0026#34;Content-Length\u0026#34;)); } catch (NumberFormatException nfe) { } return fileSize; } /** * Downloads a file from a given url and writes it to a file in current * working directory * * @param urli * Input file url * * @throws IOException * ,MalformedURLException */ public static void downloadFile(String urli) throws IOException, MalformedURLException { String fileName = urli.substring(urli.lastIndexOf(\u0026#39;/\u0026#39;) + 1, urli.length()); // download the file in the current working directory File outFile = new File(fileName); logger.debug(\u0026#34;url file name: {}\u0026#34;, fileName); logger.debug(\u0026#34;Output file path: {}\u0026#34;, outFile.getAbsolutePath()); downloadFile(urli, outFile); } /** * Downloads a file from a given url and writes it to a given File object * * @param urli * Input file url * @param outputFile * The output file to write to * */ public static void downloadFile(String urli, File outputFile) throws IOException, MalformedURLException { long startTime = System.currentTimeMillis(); // Get a connection to the URL and start up a buffered reader. URL url = new URL(urli); url.openConnection(); InputStream reader = url.openStream(); // Setup a buffered file writer to write out what we read from the // website. FileOutputStream writer = new FileOutputStream(outputFile); byte[] buffer = new byte[153600]; long totalBytesRead = 0; int bytesRead = 0; while ((bytesRead = reader.read(buffer)) \u0026gt; 0) { writer.write(buffer, 0, bytesRead); // buffer = new byte[153600]; totalBytesRead += bytesRead; // logger.debug(\u0026#34;Downloaded {} Kb \u0026#34;, (totalBytesRead / 1024)); } long endTime = System.currentTimeMillis(); logger.debug(\u0026#34;Done. {} bytes read in : {} millseconds\u0026#34;, String.valueOf(totalBytesRead), String.valueOf(endTime - startTime)); writer.close(); reader.close(); } /** * Downloads a file from a given url and writes it byte array * * @param urli * Input file url * * */ public static byte[] downloadFileToArray(String urli) throws IOException, MalformedURLException { return downloadFileToArray(urli, 0, 0); } /** * Downloads a file from a given url and writes it to a byte array * * @param urli * Input file URL * @param connectionTimeout * the maximum time in milliseconds to wait while connecting * @param readTimeout * the read timeout in milliseconds, or 0 if reads never timeout * * */ public static byte[] downloadFileToArray(String urli, int connectionTimeout, int readTimeout) throws IOException, MalformedURLException { long startTime = System.currentTimeMillis(); // Get a connection to the URL and start up a buffered reader. URL url = new URL(urli); HttpURLConnection con = (HttpURLConnection) url.openConnection(); populateDesktopHttpHeaders(con); con.setConnectTimeout(connectionTimeout); con.setReadTimeout(readTimeout); InputStream reader = con.getInputStream(); // Setup a buffered file writer to write out what we read from the // website. ByteArrayOutputStream writer = new ByteArrayOutputStream(); byte[] buffer = new byte[153600]; long totalBytesRead = 0; int bytesRead = 0; while ((bytesRead = reader.read(buffer)) \u0026gt; 0) { writer.write(buffer, 0, bytesRead); // buffer = new byte[153600]; totalBytesRead += bytesRead; // logger.debug(\u0026#34;Downloaded {} Kb \u0026#34;, (totalBytesRead / 1024)); } long endTime = System.currentTimeMillis(); // write all bytes to buffer buffer = writer.toByteArray(); // logger.debug( // \u0026#34;Downloaded {}. {} bytes read in {} \u0026#34;, // new Object[] { urli, String.valueOf(totalBytesRead), // TimeUtils.getDuration(startTime, endTime) }); con.disconnect(); writer.close(); reader.close(); return buffer; } private static void populateDesktopHttpHeaders(URLConnection urlCon) { // add custom header in order to be easily detected urlCon.setRequestProperty(\u0026#34;User-Agent\u0026#34;, \u0026#34;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:25.0) Gecko/20100101 Firefox/25.0\u0026#34;); urlCon.setRequestProperty(\u0026#34;Accept-Language\u0026#34;, \u0026#34;el-gr,el;q=0.8,en-us;q=0.5,en;q=0.3\u0026#34;); urlCon.setRequestProperty(\u0026#34;Accept-Charset\u0026#34;, \u0026#34;ISO-8859-7,utf-8;q=0.7,*;q=0.7\u0026#34;); } } UPDATE: 2013-12-31: Added more methods and timeout options 2013-06-14: Minor fixes\n","date":"2012-08-09T12:50:03+03:00","permalink":"https://manios.org/2012/08/09/java-get-file-size-and-download-using-http/","title":"Java get file size and download using HTTP"},{"content":"Today I wanted to retrieve the selected system language from PowerBuilder 10. To accomplish that I had to do some search in the web. After a long period I found this post in Google Groups.\nIt seems that Powerbuilder datawindow fields do have their onwn charset defined in font.charset property. If you change the language in a field, when this field loses focus (for example with a tab press), the selected language is changed back to system default.\nThis becomes quite annoying, especially when you have to access large datawindow forms via keyboard. To retrieve the current system language and change it, you have to use Win32 GetKeyboardLayout and ActivateKeyboardLayout functions which are defined in user32.dll.\nMy machine uses two charsets, English (EN) and Greek (EL). So I followed these steps:\n1.Define Win32 functions in Global External Functions\n1 2 Function long GetKeyboardLayout (long IdThread) Library \u0026#34;user32.dll\u0026#34; Function long ActivateKeyboardLayout (long HKL, long flags) Library \u0026#34;user32.dll\u0026#34; Define Global Variables\nNOTE: I found the numeric language values by calling GetKeyboardLayout.\n1 2 3 4 Constant long KLF_REORDER = 8 Constant long LANG_US_ENGLISH = 67699721 Constant long LANG_EL_GREEK = 67634184 long CurrentLanguage Use the functions to retrieve current selected System Language or change it**\n1 2 3 4 5 // get and display current System Language MessageBox(\u0026#39;Keyboard language\u0026#39;,GetKeyboardLayout(0)) // set System Language to Greek ActivateKeyboardLayout(LANG_EL_GREEK , KLF_REORDER) ","date":"2012-08-07T14:46:33+03:00","permalink":"https://manios.org/2012/08/07/powerbuilder-10-get-and-change-system-language/","title":"PowerBuilder 10 Get and Set System Language"},{"content":"Today I am going to show you how do we obtain schema information in SQL Server 2005/2008. If you want to select all table names from a specific schema you need to use sys.tables and sys.schemas tables. For example I wanted to select all tables from schema bob:\n1 2 3 4 SELECT t.* FROM sys.tables t INNER JOIN sys.schemas s ON s.schema_id = t.schema_id WHERE s.name = \u0026#39;bob\u0026#39; Another way to do this is by using INFORMATION_SCHEMA view:\n1 2 3 SELECT * FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = \u0026#39;bob\u0026#39; Lets say now that we want to select all columns from a specific table the name of which matches \u0026lsquo;%empl%\u0026rsquo;\n1 2 3 4 SELECT * FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = \u0026#39;bob\u0026#39; AND COLUMN_NAME LIKE \u0026#39;%empl%\u0026#39; ","date":"2012-06-22T11:53:57+03:00","permalink":"https://manios.org/2012/06/22/sql-server-select-table-information/","title":"SQL Server Select Table information"},{"content":"Today I want to truncate ( delete all table data) in a large SQL Server database instance. Plain delete statements are not going to work as there is a big number of constraints which do not allow that. After quering the web I found this solution which follows three steps:\nTell SQL Server NOT TO check for constraints for ALL tables Delete all data from all tables at once Restore constraint check for ALL tables To do that I created a small stored procedure. NOTE that it may take a while and reserve quite a lot of CPU and memory to complete.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 USE [chris] GO -- Object: StoredProcedure [dbo].[deleteAllTableDatos] Script Date: 06/01/2012 15:06:14 SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO -- ============================================= -- Author: Manios Christos -- Create date: 2012-06-01 -- Description: Deletes all table data ignoring -- constraints -- ============================================= CREATE PROCEDURE [dbo].[deleteAllTableDatos] AS BEGIN BEGIN TRANSACTION EXEC sp_MSForEachTable \u0026#39;ALTER TABLE ? NOCHECK CONSTRAINT ALL\u0026#39; EXEC sp_MSForEachTable \u0026#39;DELETE FROM ?\u0026#39; EXEC sp_MSForEachTable \u0026#39;ALTER TABLE ? CHECK CONSTRAINT ALL\u0026#39; COMMIT END ","date":"2012-06-01T15:33:11+03:00","permalink":"https://manios.org/2012/06/01/sql-server-truncate-all-table-data/","title":"SQL Server Truncate All Table Data"},{"content":"Update: 2012-07-27\nToday I wanted to keep my DatabaseHelper class unique and public in all my activities. Till now I kept creating and destroying a DatabaseHelper object for each Activity I was bringing in foreground. Thanks to Konstantinos Vaggelakos post (Database SQLiteOpenHelper singleton class for Android) I added the following code to my class and now I have a singleton class which helps me to access my database.\nIt is useful to keep only one reference to an open writabe SQLiteDatabase object, so I added myWritableDb as a member and the method getMyWritableDatabase() which returns the same writable database object every time.\nNow we have a SQLiteDatabase object as a class member and we do not want to forget it open and leak. So we override onClose() method and close it manually.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 public class DatabaseHelper extends SQLiteOpenHelper { private final Context myContext; private static DatabaseHelper mInstance; private static SQLiteDatabase myWritableDb; /** * Constructor takes and keeps a reference of the passed context in order to * access to the application assets and resources. * * @param context * the application context */ private DatabaseHelper(Context context) { super(context, DB_NAME, null, 1); this.myContext = context; } /** * Get default instance of the class to keep it a singleton * * @param context * the application context */ public static DatabaseHelper getInstance(Context context) { if (mInstance == null) { mInstance = new DatabaseHelper(context); } return mInstance; } /** * Returns a writable database instance in order not to open and close many * SQLiteDatabase objects simultaneously * * @return a writable instance to SQLiteDatabase */ public SQLiteDatabase getMyWritableDatabase() { if ((myWritableDb == null) || (!myWritableDb.isOpen())) { myWritableDb = this.getWritableDatabase(); } return myWritableDb; } @Override public void close() { super.close(); if (myWritableDb != null) { myWritableDb.close(); myWritableDb = null; } } ","date":"2012-05-17T13:13:50+03:00","permalink":"https://manios.org/2012/05/17/extend-sqliteopenhelper-as-a-singleton-class-in-android/","title":"Extend SQLiteOpenHelper as a singleton class in Android "},{"content":"Today I wanted to write logcat output to a file while my application is running. To do so I executed the following code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public void executeLogcat(){ File logFile = new File(\u0026#34;/mnt/sdcard/arxeion.log\u0026#34;); // log file name int sizePerFile = 60; // size in kilobytes int rotationCount = 10; // file rotation count String filter = \u0026#34;D\u0026#34;; // Debug priority String[] args = new String[] { \u0026#34;logcat\u0026#34;, \u0026#34;-v\u0026#34;, \u0026#34;time\u0026#34;, \u0026#34;-f\u0026#34;,logFile.getAbsolutePath(), \u0026#34;-r\u0026#34;, Integer.toString(sizePerFile), \u0026#34;-n\u0026#34;, Integer.toString(rotationCount), \u0026#34;*:\u0026#34; + filter }; try { Runtime.getRuntime().exec(args); } catch (IOException e) { e.printStackTrace(); } } To execute logcat process we are obliged to add\nxml\nuses permission in AndroidManifest.xml file.\nWARNING!!\nThis code should NOT be used in a release version of your application. You should control when to kill logcat process, as it will run forever ","date":"2012-05-15T11:47:00+03:00","permalink":"https://manios.org/2012/05/15/check-network-connectivity-and-obtain-wifi-mac-address-in-android/","title":"Launch Logcat programmatically in Android"},{"content":"Today I am going to show you how to create an android application which comes with a pre-populated database. The database file has to be stored in assets directory.\nEvery file that is stored in Assets directory is compressed. Prior to Android 2.3, any compressed asset file with an uncompressed size of over 1 MB cannot be read from the APK. The limit on the uncompressed size of compressed assets was removed in Android 2.3. So someday in the future, when you don’t have to worry about Android versions lower than 2.3, you can avoid this heartache. (source: Brian Hardy-Dealing with Asset Compression in Android Apps\nSo if you have a database file over 1 mb and you write an application that supports Android versions prior to 2.3 you have 2 options\nSplit your database file in chunks which are smaller than 1Mb. Ship your file with an extension from the following and it will not be compressed. (This will increase your .apk file size by the size of database file): 1 2 3 4 5 6 7 8 9 10 11 //extract from Package.cpp in the aapt source code, on which types of files are not compressed by default: /* these formats are already compressed, or don\u0026#39;t compress well */ static const char* kNoCompressExt[] = { \u0026#34;.jpg\u0026#34;, \u0026#34;.jpeg\u0026#34;, \u0026#34;.png\u0026#34;, \u0026#34;.gif\u0026#34;, \u0026#34;.wav\u0026#34;, \u0026#34;.mp2\u0026#34;, \u0026#34;.mp3\u0026#34;, \u0026#34;.ogg\u0026#34;, \u0026#34;.aac\u0026#34;, \u0026#34;.mpg\u0026#34;, \u0026#34;.mpeg\u0026#34;, \u0026#34;.mid\u0026#34;, \u0026#34;.midi\u0026#34;, \u0026#34;.smf\u0026#34;, \u0026#34;.jet\u0026#34;, \u0026#34;.rtttl\u0026#34;, \u0026#34;.imy\u0026#34;, \u0026#34;.xmf\u0026#34;, \u0026#34;.mp4\u0026#34;, \u0026#34;.m4a\u0026#34;, \u0026#34;.m4v\u0026#34;, \u0026#34;.3gp\u0026#34;, \u0026#34;.3gpp\u0026#34;, \u0026#34;.3g2\u0026#34;, \u0026#34;.3gpp2\u0026#34;, \u0026#34;.amr\u0026#34;, \u0026#34;.awb\u0026#34;, \u0026#34;.wma\u0026#34;, \u0026#34;.wmv\u0026#34; }; Then create your pre-populated database file which has android_metadata table in it and insert a record. (Android creates automatically this table when you are creating your database in a traditional way in SQLiteOpenHelper onCreate() method.) :\n1 2 3 4 5 -- create table CREATE TABLE android_metadata (locale TEXT ); -- insert record INSERT INTO android_metadata VALUES(\u0026#39;en_US\u0026#39;); Then create your class which extends SQLiteOpenHelper like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 public class DatabaseHelper extends SQLiteOpenHelper { private static final String TAG = \u0026#34;DatabaseHelper\u0026#34;; private static final int DATABASE_VERSION = 1; // database file name. private static final String DB_NAME = \u0026#34;mp.jet\u0026#34;; private final Context myContext; private static SQLiteDatabase myWritableDb; /** * Constructor takes and keeps a reference of the passed context in order to * access to the application assets and resources. * * @param context * the application context */ public DatabaseHelper(Context context) { super(context, DB_NAME, null, 1); this.myContext = context; } @Override public void onCreate(SQLiteDatabase db) { } @Override public void onUpgrade(SQLiteDatabase db, int oldVersion, int newVersion) { } /** * Returns a writable database instance in order not to open and close many * SQLiteDatabase objects simultaneously * * @return a writable instance to SQLiteDatabase */ public SQLiteDatabase getMyWritableDatabase() { if (myWritableDb == null || (!myWritableDb.isOpen())) { myWritableDb = this.getWritableDatabase(); } return myWritableDb; } @Override public void close() { super.close(); if (myWritableDb != null) { myWritableDb.close(); myWritableDb = null; } } /** * Creates an empty database on the system and rewrites it with your own * database. * */ public void createDataBase() throws IOException { boolean dbExist = databaseExists(); if (dbExist) { // do nothing , database already exists } else { /** * By calling this method and empty database will be created into * the default system path of your application so we are gonna be * able to overwrite that database with our database. */ this.getReadableDatabase(); // release all opened database objects // if you omit this line you will get a \u0026#34;no such table exception\u0026#34; // and // the application will crash ONLY in the first run. this.close(); try { copyDataBase(); } catch (IOException e) { throw new Error(\u0026#34;Error copying database\u0026#34;); } } } /** * Check if the database already exist to avoid re-copying the file each * time you open the application. * * @return true if it exists, false if it doesn\u0026#39;t */ private boolean databaseExists() { SQLiteDatabase checkDB = null; try { String myPath = getDatabasePath(); checkDB = SQLiteDatabase.openDatabase(myPath, null, SQLiteDatabase.OPEN_READONLY); } catch (SQLiteException e) { // database does not exist yet. } if (checkDB != null) { checkDB.close(); } return checkDB != null ? true : false; } /** * Copies your database from your local assets-folder to the just created * empty database in the system folder, from where it can be accessed and * handled. This is done by transferring bytestream. * */ private void copyDataBase() throws IOException { Log.d(TAG, \u0026#34;copyDatabase()\u0026#34;); // Open your local db as the input stream InputStream myInput = myContext.getAssets().open(DB_NAME); // Path to the just created empty db String outFileName = getDatabasePath(); // Open the empty db as the output stream OutputStream myOutput = new FileOutputStream(outFileName); // transfer bytes from the inputfile to the outputfile byte[] buffer = new byte[1024]; int length; while ((length = myInput.read(buffer)) \u0026gt; 0) { myOutput.write(buffer, 0, length); } SQLiteDatabase checkDB = null; // get a reference to the db. try { checkDB = SQLiteDatabase.openDatabase(getDatabasePath(), null, SQLiteDatabase.OPEN_READWRITE); // once the db has been copied, set the new version.. checkDB.setVersion(DATABASE_VERSION); checkDB.close(); } catch (SQLiteException e) { // database does?t exist yet. } // Close the streams myOutput.flush(); myOutput.close(); myInput.close(); } /** * Get absolute path to database file. The Android\u0026#39;s default system path of * your application database is /data/data/\u0026amp;ltpackage * name\u0026amp;gt/databases/\u0026amp;ltdatabase name\u0026amp;gt * * @return path to database file */ private String getDatabasePath() { // The Android\u0026#39;s default system path of your application database. // /data/data/\u0026lt;package name\u0026gt;/databases/\u0026lt;databasename\u0026gt; return myContext.getFilesDir().getParentFile().getAbsolutePath() + \u0026#34;/databases/\u0026#34; + DB_NAME; } ","date":"2012-05-14T09:12:48+03:00","permalink":"https://manios.org/2012/05/14/android-pre-populated-database/","title":"Android pre-populated Database"},{"content":"Today I am going to show you how to get the bluetooth adapter device name (if exists) and MAC address programmatically in Android. Note that this code will not work in emulator as it does not support bluetooth. To avoid a possible exception we check if mBluetoothAdapter==null in lines 09 and 25.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 /** * get bluetooth local device name * @return device name String */ public static String getLocalBluetoothName() { BluetoothAdapter mBluetoothAdapter = BluetoothAdapter.getDefaultAdapter(); // if device does not support Bluetooth if(mBluetoothAdapter==null){ Log.d(TAG,\u0026#34;device does not support bluetooth\u0026#34;); return null; } return mBluetoothAdapter.getName(); } /** * get bluetooth adapter MAC address * @return MAC address String */ public static String getBluetoothMacAddress() { BluetoothAdapter mBluetoothAdapter = BluetoothAdapter.getDefaultAdapter(); // if device does not support Bluetooth if(mBluetoothAdapter==null){ Log.d(TAG,\u0026#34;device does not support bluetooth\u0026#34;); return null; } return mBluetoothAdapter.getAddress(); } The above methods require javaandroid.permission.BLUETOOTH uses permissions in AndroidManifest.xml file.\n","date":"2012-05-09T11:12:27+03:00","permalink":"https://manios.org/2012/05/09/bluetooth-adapter-device-name-and-mac-address-in-android/","title":"Bluetooth adapter device name and MAC address in Android"},{"content":"Today I am going to show you how to install Jboss 5.1.0.GA on a Windows 7 machine.\nFirst of all download the binaries from http://www.jboss.org/jbossas/downloads/ The downloaded file is a zip is named jboss-5.1.0.GA-jdk6.zip. As you can see it is a zip archive and not an executable. Create an installation folder (I created one at C:\\Program Files\\Jboss) and extract the archive there. Add the path to JBoss directory to JBOSS_HOME enviroment variable (picture 1). For me it is: 1 C:\\Program Files\\Jboss\\jboss-5.1.0.GA Add the path to JDK directory to JAVA_HOME enviroment variable (picture 1). For me it is C:\\Program Files\\Java\\jdk1.6.0_31 Gongratulations! We are ready! Now to start the application server open a shell window using WinKey + R then type cmd and hit Enter. Type 1 C:\\Program Files\\Jboss\\jboss-5.1.0.GA\\bin\\run.bat -b 0.0.0.0 Notice the parameter -b! This parameter assures that server is visible from all computers inside the network. If you want to see if the application server has started correctly, inspect the log output in the shell will it is loading. When the process is finished open a browser and type http://localhost:8080/web-console/ ","date":"2012-05-08T10:31:12+03:00","permalink":"https://manios.org/2012/05/08/install-jboss-5-1-0-ga-to-windows-7/","title":"Install Jboss 5.1.0.GA to  Windows 7"},{"content":"No Content Found\n","date":"2012-05-07T16:39:23+03:00","permalink":"https://manios.org/2012/05/07/android-custom-dialogs-part-1-set-value/","title":"Android Custom Dialogs Part 1: Set value"},{"content":"Updated: 2012-05-09\nToday I wanted to check if my Android device is connected to a TCP/IP network. So I searched in the web for a while and I found this StackOverFlow answer on connectivity.\n1 2 3 4 5 6 7 8 public boolean isOnline() { ConnectivityManager cm = (ConnectivityManager) getSystemService(Context.CONNECTIVITY_SERVICE); NetworkInfo netInfo = cm.getActiveNetworkInfo(); if (netInfo != null \u0026amp;\u0026amp; netInfo.isConnectedOrConnecting()) { return true; } return false; } Also I wanted to obtain my Wifi network card MAC address. The code snippet to do this is the following:\n1 2 3 4 5 6 7 8 9 10 /** * Get Wifi network card MAC address * @return mac address String */ public String getMacAddress() { WifiManager wifiMan = (WifiManager) this.getSystemService(Context.WIFI_SERVICE); WifiInfo wifiInf = wifiMan.getConnectionInfo(); return wifiInf.getMacAddress();\t} The above methods require\n1 2 android.permission.ACCESS_NETWORK_STATE android.permission.ACCESS_WIFI_STATE uses permissions in AndroidManifest.xml file.\n","date":"2012-04-05T13:26:35+03:00","permalink":"https://manios.org/2012/04/05/check-network-connectivity-in-android/","title":"Check Network connectivity and obtain Wifi MAC address in Android"},{"content":"Today I will show you how to generate the MD5 hash String for a given String in Android\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 /** * function md5 encryption for passwords * * @param password * @return passwordEncrypted */ private static final String md5(final String password) { try { MessageDigest digest = java.security.MessageDigest .getInstance(\u0026#34;MD5\u0026#34;); digest.update(password.getBytes()); byte messageDigest[] = digest.digest(); StringBuffer hexString = new StringBuffer(); for (int i = 0; i \u0026lt; messageDigest.length; i++) { String h = Integer.toHexString(0xFF \u0026amp; messageDigest[i]); while (h.length() \u0026lt; 2) h = \u0026#34;0\u0026#34; + h; hexString.append(h); } return hexString.toString(); } catch (NoSuchAlgorithmException e) { e.printStackTrace(); } return \u0026#34;\u0026#34;; } ","date":"2012-03-19T11:56:53+02:00","permalink":"https://manios.org/2012/03/19/android-md5-password-encryption/","title":"Android MD5 password encryption"},{"content":"Today I wanted to copy and paste a piece of code from Eclipse Indigo to Microsoft Word 2007 and keep the syntax highlighting. In some cases it worked but in some other not. After searching in the webI found that if the area you want to copy has folded code in it, the syntax highlight will not be preserved (picture 1)\nSo before you start copying and pasting from Eclipse to Microsoft Word unfold any part of the code you want to copy (picture 2)\n","date":"2012-03-08T09:53:02+02:00","permalink":"https://manios.org/2012/03/08/copypaste-source-code-from-eclipse-to-microsoft-word/","title":"Copy/Paste source code from Eclipse to Microsoft Word"},{"content":"Today I wanted to store some images to SD card and I wanted to find the available space:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 /** * Get the available free space in the external storage (usually SD card) * * @return the available space in gigabytes */ public double getExternalStorageFreeSpace() { StatFs stat = new StatFs(Environment.getExternalStorageDirectory().getPath()); double sdAvailSize = (double) stat.getAvailableBlocks()\t* (double) stat.getBlockSize(); // One binary gigabyte equals 1,073,741,824 bytes. double gigaAvailable = sdAvailSize / 1073741824; return gigaAvailable; } ","date":"2012-03-07T13:40:01+02:00","permalink":"https://manios.org/2012/03/07/android-external-storage-free-space/","title":"Android External Storage Free Space"},{"content":"Let\u0026rsquo;s say we have a simple class called Person\n1 2 3 4 5 6 7 8 9 10 public class Person{ public String name; public int age; public Person(String name, int age) { this.name = name; this.age = age; } } If you want to create an ArrayList of Person objects and sort it by name you can do the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // populate the ArrayList with some objects ArrayList\u0026lt;Person\u0026gt; arpList = new ArrayList\u0026lt;TransgressionTab.Person\u0026gt;(); arpList.add(new Person(\u0026#34;Onasis\u0026#34;, 54)); arpList.add(new Person(\u0026#34;Spartakus\u0026#34;, 12)); arpList.add(new Person(\u0026#34;Oneiro\u0026#34;, 1)); arpList.add(new Person(\u0026#34;Bomb\u0026#34;, 12)); // sort it Collections.sort(arpList,new Comparator\u0026lt;Person\u0026gt;() { @Override public int compare(Person s1,Person s2) { return s1.name.compareToIgnoreCase(s2.name); } }); ","date":"2012-02-28T15:19:55+02:00","permalink":"https://manios.org/2012/02/28/object-arraylist-sort/","title":"Object ArrayList Sort"},{"content":" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /** * get the version of the SQLite Library that is installed in the android * system * * @return version , for example 3.5.2 */ public String getSqliteLibraryVersion() { Cursor cursor = SQLiteDatabase.openOrCreateDatabase(\u0026#34;:memory:\u0026#34;, null) .rawQuery(\u0026#34;select sqlite_version() AS sqlite_version\u0026#34;, null); String sqliteVersion = \u0026#34;\u0026#34;; while (cursor.moveToNext()) { sqliteVersion += cursor.getString(0); } return sqliteVersion; } ","date":"2012-02-22T15:19:02+02:00","permalink":"https://manios.org/2012/02/22/android-get-sqlite-library-version/","title":"Android get SQLite Library Version"},{"content":" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 package com.ots.pocketpolice.sync; import java.io.BufferedOutputStream; import java.io.File; import java.io.FileInputStream; import java.io.FileOutputStream; import java.io.IOException; import java.io.InputStream; import java.util.Enumeration; import java.util.zip.ZipEntry; import java.util.zip.ZipFile; import java.util.zip.ZipOutputStream; /** * A convenience class that compresses and decompresses files to and from Zip * archives.\u0026lt;/br\u0026gt;Updated: 2012-01-19 09:54 * @author CManios * @version 1.0 */ public class Zipper { // links that helped a lot // http://www.crazysquirrel.com/computing/java/basics/java-directory-zipping.jspx // http://stackoverflow.com/questions/1399126/java-util-zip-recreating-directory-structure private static final int BUFFER_SIZE = 2048; /** * Compresses a file or a directory to a Zip archive. The method is * recursive and includes all subdirectories * * @param source * a file or a directory * @param zip * the Zip archive to create * @throws IOException */ public static void zip(File source, File zip) throws IOException { ZipOutputStream zos = new ZipOutputStream(new FileOutputStream(zip)); zipAll(source, source, zos); zos.close(); } private static void zipAll(File sourceFile, File base, ZipOutputStream zos) throws IOException { byte[] buffer = new byte[BUFFER_SIZE]; FileInputStream in; int read = 0; if (!sourceFile.isDirectory()) { in = new FileInputStream(sourceFile); ZipEntry entry = getZipEntry(sourceFile, base); zos.putNextEntry(entry); while ((read = in.read(buffer)) != -1) { zos.write(buffer, 0, read); } } else { File[] files = sourceFile.listFiles(); for (int i = 0; i \u0026lt; files.length; i++) { if (files[i].isDirectory()) { zipAll(files[i], base, zos); } else { in = new FileInputStream(files[i]); ZipEntry entry = getZipEntry(files[i], base); zos.putNextEntry(entry); while ((read = in.read(buffer)) != -1) { zos.write(buffer, 0, read); } in.close(); } } } } private static ZipEntry getZipEntry(File fileos, File base) { if (fileos.equals(base)) { return new ZipEntry(fileos.getName()); } return new ZipEntry(fileos.getAbsolutePath().substring( base.getAbsolutePath().length() + 1)); } /** * Compresses an array of files or directories to a Zip archive. The method * is recursive and includes all subdirectories * * @param zip * the Zip archive to create * @param sourceFiles * an array of files or directories * @throws IOException */ public static void zip(File sourceFiles[], File zip) throws IOException { ZipOutputStream zos = new ZipOutputStream(new FileOutputStream(zip)); zipAll(sourceFiles, null, zos); zos.close(); } private static void zipAll(File sourceFile[], File base, ZipOutputStream zos) throws IOException { for (int i = 0; i \u0026lt; sourceFile.length; i++) { if (sourceFile[i].isDirectory()) { zipAll(sourceFile[i], sourceFile[i].getAbsoluteFile() .getParentFile(), zos); continue; } zipAll(sourceFile[i], sourceFile[i], zos); } } /** * Extracts the contents of a Zip archive to a specific directory. If the * directory does not exist, it is created * * @param zip * the Zip archive * @param extractTo * directory path * @throws IOException */ public static final void unzip(File zip, File extractTo) throws IOException { ZipFile archive = new ZipFile(zip); Enumeration\u0026lt;? extends ZipEntry\u0026gt; e = archive.entries(); while (e.hasMoreElements()) { ZipEntry entry = (ZipEntry) e.nextElement(); File file = new File(extractTo, entry.getName()); if (entry.isDirectory() \u0026amp;\u0026amp; !file.exists()) { file.mkdirs(); } else { if (!file.getParentFile().exists()) { file.getParentFile().mkdirs(); } InputStream in = archive.getInputStream(entry); BufferedOutputStream out = new BufferedOutputStream( new FileOutputStream(file)); byte[] buffer = new byte[BUFFER_SIZE]; int read; while (-1 != (read = in.read(buffer))) { out.write(buffer, 0, read); } in.close(); out.close(); } } } } ","date":"2012-01-18T09:49:30+02:00","permalink":"https://manios.org/2012/01/18/zip-and-unzip-in-java/","title":"Zip and UnZip in Java"},{"content":"Welcome to WordPress.com. After you read this, you should delete and write your own post, with a new title above. Or hit Add New on the left (of the admin dashboard) to start a fresh post.\nHere are some suggestions for your first post.\nYou can find new ideas for what to blog about by reading the Daily Post.\nAdd PressThis to your browser. It creates a new blog post for you about any interesting page you read on the web.\nMake some changes to this page, and then hit preview on the right. You can always preview any post or edit it before you share it to the world.\n","date":"2011-10-14T22:53:46+03:00","permalink":"https://manios.org/2011/10/14/hello-world/","title":"Test1"},{"content":"author: cmanios draft: true date: 2017-10-17 13:14:00+03:00 slug: eclipse-vs-intellij title: Eclipse vs IntelliJ categories:\nEclipse Java tags: eclipse java Organize imports (StackOverflow question #14716283) Find class and Ctrl+Space is case insensitive. https://intellij-support.jetbrains.com/hc/en-us/community/posts/205806989-Case-insensitive-code-completion https://confluence.jetbrains.com/display/IDEADEV/Filesystem+Case-Sensitivity+Mismatch Συνεχώς πρέπει να κάνεις Build (Ctrl+F9). Quite often we had code being pushed/committed with basic compilation errors. In Eclipse copying classes from a project package to another project package, fixes the package names. Git suppoort: worse than Netbeans No staging. Currently work in progress (IDEA-63391) Git reset is sometime working in IntelliJ. Copy from tooltips inside sourcecode is not possible in IntelliJ. equals() and hashCode() autogenerated methods may throw NullPointerException. QueryDSL and Maven does not work out of the box. IntelliJ has problems with annotation processors and is not able to see generated-sources directory. https://stackoverflow.com/questions/43613069/intellij-does-not-recognize-generated-sources https://stackoverflow.com/questions/45143135/intellij-idea-cannot-see-generated-sources-directory/47278829#47278829 https://stackoverflow.com/questions/5170620/unable-to-use-intellij-with-a-generated-sources-folder https://intellij-support.jetbrains.com/hc/en-us/community/posts/206198829-maven-integration-and-directories-under-generated-sources- https://intellij-support.jetbrains.com/hc/en-us/community/posts/206878975-How-can-I-make-IDEA-recognize-my-generated-sources-using-Gradle- https://stackoverflow.com/questions/46422919/changing-querydsl-generated-files-directory https://blog.jdriven.com/2018/10/using-querydsl-annotation-processor-with-gradle-and-intellij-idea/ https://intellij-support.jetbrains.com/hc/en-us/community/posts/115000387224-Trying-to-use-maven-plugin-querydsl-maven-plugin-to-generate-sources http://bsideup.blogspot.com/2015/04/querydsl-with-gradle-and-idea.html https://groups.google.com/forum/#!topic/querydsl/FYR2GP2NeWU https://stackoverflow.com/questions/34837101/querydsl-maven-set-up-project Generated sources directories and * (asterisk in import) do not work properly: https://stackoverflow.com/questions/3348816/intellij-never-use-wildcard-imports https://intellij-support.jetbrains.com/hc/en-us/community/posts/115000704690-depending-on-generated-sources- Download Maven sources does not work. You have to do it manually. Instead IntelliJ decompiler fires up. You have to learn a lot of useless keyboard shortcuts which by default work in the majority of other applications. For example, Ctrl+W to close current file, Ctrl+PageUp, Ctrl+PageDown. Showing Javadoc on method, class, package hover does not work by default. You have to configure it. Javadoc Quick documentation window does not wrap text and has always an horizontal line. Related StackOverflow question 42691347 and issue IDEA-169414. References https://techblog.bozho.net/still-prefer-eclipse-intellij-idea/ ","date":"0001-01-01T00:00:00Z","permalink":"https://manios.org/1/01/01/","title":""}]